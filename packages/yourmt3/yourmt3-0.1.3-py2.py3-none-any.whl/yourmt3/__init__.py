from typing import Callable, Literal
import torch
import torchaudio
from yourmt3.utils.audio import slice_padded_array
from yourmt3.utils.midi import note_event2midi
from yourmt3.utils.note2event import mix_notes, note2note_event
from yourmt3.utils.event2note import merge_zipped_note_events_and_ties_to_notes
from yourmt3.utils.note_event_dataclasses import Event, Note
from yourmt3.utils.task_manager import TaskManager
from yourmt3.utils.utils import str2bool
from yourmt3.model.ymt3 import YourMT3
from yourmt3.model.init_train import update_config
from yourmt3.config.config import shared_cfg as def_shared_cfg
from argparse import ArgumentParser
from copy import deepcopy

if torch.__version__ >= "1.13":
    torch.set_float32_matmul_precision("high")

def filter_instrument_consistency(pred_notes,confidence_threshold:float, primary_instrument=None,allow=True):
    if not allow or not pred_notes:
        return pred_notes
    
    # Count instrument occurrences to find dominant instrument
    instrument_counts = {}
    total_notes = len(pred_notes)
    
    for note in pred_notes:
        program = getattr(note, 'program', 0)
        instrument_counts[program] = instrument_counts.get(program, 0) + 1
    
    # Determine primary instrument
    if primary_instrument is None:
        primary_instrument = max(instrument_counts, key=instrument_counts.get)
    
    primary_count = instrument_counts.get(primary_instrument, 0)
    primary_ratio = primary_count / total_notes if total_notes > 0 else 0
    
    # If primary instrument is dominant enough, filter out other instruments
    if primary_ratio >= confidence_threshold:
        filtered_notes = []
        for note in pred_notes:
            note_program = getattr(note, 'program', 0)
            if note_program == primary_instrument:
                filtered_notes.append(note)
            else:
                # Convert note to primary instrument
                note_copy = note.__class__(
                    start=note.start,
                    end=note.end, 
                    pitch=note.pitch,
                    velocity=note.velocity,
                    program=primary_instrument
                )
                filtered_notes.append(note_copy)
        return filtered_notes
    
    return pred_notes

def model_name2conf(model_name,precision):
    args = ['-pr', precision]
    if "YPTF" in model_name:
        args = args + ['-enc', 'perceiver-tf', '-ac', 'spec','-hop', '300', '-atc', '1']
    if "Multi" in model_name:
        args = args + ['-tk', 'mc13_full_plus_256', '-dec', 'multi-t5','-nl', '26']
    if "MoE" in model_name:
        args = args + ['-sqr', '1', '-ff', 'moe','-wf', '4', '-nmoe', '8', '-kmoe', '2', '-act', 'silu', '-epe', 'rope','-rp', '1']

    parser = ArgumentParser()
    parser.add_argument('-ac', '--audio-codec', type=str, default=None, help='audio codec (default=None). {"spec", "melspec"}. If None, default value defined in config.py will be used.')
    parser.add_argument('-hop', '--hop-length', type=int, default=None, help='hop length in frames (default=None). {128, 300} 128 for MT3, 300 for PerceiverTFIf None, default value defined in config.py will be used.')
    parser.add_argument('-nmel', '--n-mels', type=int, default=None, help='number of mel bins (default=None). If None, default value defined in config.py will be used.')
    parser.add_argument('-if', '--input-frames', type=int, default=None, help='number of audio frames for input segment (default=None). If None, default value defined in config.py will be used.')
    # Model configurations
    parser.add_argument('-sqr', '--sca-use-query-residual', type=str2bool, default=None, help='sca use query residual flag. Default follows config.py')
    parser.add_argument('-enc', '--encoder-type', type=str, default=None, help="Encoder type. 't5' or 'perceiver-tf' or 'conformer'. Default is 't5', following config.py.")
    parser.add_argument('-dec', '--decoder-type', type=str, default=None, help="Decoder type. 't5' or 'multi-t5'. Default is 't5', following config.py.")
    parser.add_argument('-preenc', '--pre-encoder-type', type=str, default='default', help="Pre-encoder type. None or 'conv' or 'default'. By default, t5_enc:None, perceiver_tf_enc:conv, conformer:None")
    parser.add_argument('-predec', '--pre-decoder-type', type=str, default='default', help="Pre-decoder type. {None, 'linear', 'conv1', 'mlp', 'group_linear'} or 'default'. Default is {'t5': None, 'perceiver-tf': 'linear', 'conformer': None}.")
    parser.add_argument('-cout', '--conv-out-channels', type=int, default=None, help='Number of filters for pre-encoder conv layer. Default follows "model_cfg" of config.py.')
    parser.add_argument('-tenc', '--task-cond-encoder', type=str2bool, default=True, help='task conditional encoder (default=True). True or False')
    parser.add_argument('-tdec', '--task-cond-decoder', type=str2bool, default=True, help='task conditional decoder (default=True). True or False')
    parser.add_argument('-df', '--d-feat', type=int, default=None, help='Audio feature will be projected to this dimension for Q,K,V of T5 or K,V of Perceiver (default=None). If None, default value defined in config.py will be used.')
    parser.add_argument('-pt', '--pretrained', type=str2bool, default=False, help='pretrained T5(default=False). True or False')
    parser.add_argument('-b', '--base-name', type=str, default="google/t5-v1_1-small", help='base model name (default="google/t5-v1_1-small")')
    parser.add_argument('-epe', '--encoder-position-encoding-type', type=str, default='default', help="Positional encoding type of encoder. By default, pre-defined PE for T5 or Perceiver-TF encoder in config.py. For T5: {'sinusoidal', 'trainable'}, conformer: {'rotary', 'trainable'}, Perceiver-TF: {'trainable', 'rope', 'alibi', 'alibit', 'None', '0', 'none', 'tkd', 'td', 'tk', 'kdt'}.")
    parser.add_argument('-dpe', '--decoder-position-encoding-type', type=str, default='default', help="Positional encoding type of decoder. By default, pre-defined PE for T5 in config.py. {'sinusoidal', 'trainable'}.")
    parser.add_argument('-twe', '--tie-word-embedding', type=str2bool, default=None, help='tie word embedding (default=None). If None, default value defined in config.py will be used.')
    parser.add_argument('-el', '--event-length', type=int, default=None, help='event length (default=None). If None, default value defined in model cfg of config.py will be used.')
    # Perceiver-TF configurations
    parser.add_argument('-dl', '--d-latent', type=int, default=None, help='Latent dimension of Perceiver. On T5, this will be ignored (default=None). If None, default value defined in config.py will be used.')
    parser.add_argument('-nl', '--num-latents', type=int, default=None, help='Number of latents of Perceiver. On T5, this will be ignored (default=None). If None, default value defined in config.py will be used.')
    parser.add_argument('-dpm', '--perceiver-tf-d-model', type=int, default=None, help='Perceiver-TF d_model (default=None). If None, default value defined in config.py will be used.')
    parser.add_argument('-npb', '--num-perceiver-tf-blocks', type=int, default=None, help='Number of blocks of Perceiver-TF. On T5, this will be ignored (default=None). If None, default value defined in config.py.')
    parser.add_argument('-npl', '--num-perceiver-tf-local-transformers-per-block', type=int, default=None, help='Number of local layers per block of Perceiver-TF. On T5, this will be ignored (default=None). If None, default value defined in config.py will be used.')
    parser.add_argument('-npt', '--num-perceiver-tf-temporal-transformers-per-block', type=int, default=None, help='Number of temporal layers per block of Perceiver-TF. On T5, this will be ignored (default=None). If None, default value defined in config.py will be used.')
    parser.add_argument('-atc', '--attention-to-channel', type=str2bool, default=None, help='Attention to channel flag of Perceiver-TF. On T5, this will be ignored (default=None). If None, default value defined in config.py will be used.')
    parser.add_argument('-ln', '--layer-norm-type', type=str, default=None, help='Layer normalization type (default=None). {"layer_norm", "rms_norm"}. If None, default value defined in config.py will be used.')
    parser.add_argument('-ff', '--ff-layer-type', type=str, default=None, help='Feed forward layer type (default=None). {"mlp", "moe", "gmlp"}. If None, default value defined in config.py will be used.')
    parser.add_argument('-wf', '--ff-widening-factor', type=int, default=None, help='Feed forward layer widening factor for MLP/MoE/gMLP (default=None). If None, default value defined in config.py will be used.')
    parser.add_argument('-nmoe', '--moe-num-experts', type=int, default=None, help='Number of experts for MoE (default=None). If None, default value defined in config.py will be used.')
    parser.add_argument('-kmoe', '--moe-topk', type=int, default=None, help='Top-k for MoE (default=None). If None, default value defined in config.py will be used.')
    parser.add_argument('-act', '--hidden-act', type=str, default=None, help='Hidden activation function (default=None). {"gelu", "silu", "relu", "tanh"}. If None, default value defined in config.py will be used.')
    parser.add_argument('-rt', '--rotary-type', type=str, default=None, help='Rotary embedding type expressed in three letters. e.g. ppl: "pixel" for SCA and latents, "lang" for temporal transformer. If None, use config.')
    parser.add_argument('-rk', '--rope-apply-to-keys', type=str2bool, default=None, help='Apply rope to keys (default=None). If None, use config.')
    parser.add_argument('-rp', '--rope-partial-pe', type=str2bool, default=None, help='Whether to apply RoPE to partial positions (default=None). If None, use config.')
    # Decoder configurations
    parser.add_argument('-dff', '--decoder-ff-layer-type', type=str, default=None, help='Feed forward layer type of decoder (default=None). {"mlp", "moe", "gmlp"}. If None, default value defined in config.py will be used.')
    parser.add_argument('-dwf', '--decoder-ff-widening-factor', type=int, default=None, help='Feed forward layer widening factor for decoder MLP/MoE/gMLP (default=None). If None, default value defined in config.py will be used.')
    # Task and Evaluation configurations
    parser.add_argument('-tk', '--task', type=str, default='mt3_full_plus', help='tokenizer type (default=mt3_full_plus). See config/task.py for more options.')
    parser.add_argument('-epv', '--eval-program-vocab', type=str, default=None, help='evaluation vocabulary (default=None). If None, default vocabulary of the data preset will be used.')
    parser.add_argument('-edv', '--eval-drum-vocab', type=str, default=None, help='evaluation vocabulary for drum (default=None). If None, default vocabulary of the data preset will be used.')
    parser.add_argument('-etk', '--eval-subtask-key', type=str, default='default', help='evaluation subtask key (default=default). See config/task.py for more options.')
    parser.add_argument('-t', '--onset-tolerance', type=float, default=0.05, help='onset tolerance (default=0.05).')
    parser.add_argument('-os', '--test-octave-shift', type=str2bool, default=False, help='test optimal octave shift (default=False). True or False')
    parser.add_argument('-w', '--write-model-output', type=str2bool, default=True, help='write model test output to file (default=False). True or False')
    # Trainer configurations
    parser.add_argument('-pr','--precision', type=str, default="bf16-mixed", help='precision (default="bf16-mixed") {32, 16, bf16, bf16-mixed}')
    parser.add_argument('-st', '--strategy', type=str, default='auto', help='strategy (default=auto). auto or deepspeed or ddp')
    parser.add_argument('-n', '--num-nodes', type=int, default=1, help='number of nodes (default=1)')
    parser.add_argument('-g', '--num-gpus', type=str, default='auto', help='number of gpus (default="auto")')
    parser.add_argument('-wb', '--wandb-mode', type=str, default="disabled", help='wandb mode for logging (default=None). "disabled" or "online" or "offline". If None, default value defined in config.py will be used.')
    # Debug
    parser.add_argument('-debug', '--debug-mode', type=str2bool, default=False, help='debug mode (default=False). True or False')
    parser.add_argument('-tps', '--test-pitch-shift', type=int, default=None, help='use pitch shift when testing. debug-purpose only. (default=None). semitone in int.')
    args = parser.parse_args(args)
    # yapf: enable
    args.epochs = None
    return args


class YMT3:
    def __init__(self,model_path,model_name:Literal["YMT3+", "YPTF+Single", "YPTF+Multi", "YPTF+MoE+Multi"],precision:Literal["32", "bf16-mixed", "16"] = "32",device = "cpu"):
        args = model_name2conf(model_name,precision)
        shared_cfg, audio_cfg, model_cfg = update_config(args, deepcopy(def_shared_cfg), stage='test')
        self.model = YourMT3(audio_cfg,model_cfg,shared_cfg,optimizer=None,task_manager=TaskManager(task_name=args.task,max_shift_steps=int(shared_cfg["TOKENIZER"]["max_shift_steps"]),debug_mode=args.debug_mode),eval_subtask_key=args.eval_subtask_key,write_output_dir=".")
        self.model.load_state_dict(torch.load(model_path),strict=False)
        self.model.to(device)
        self.model.eval()

    def create_instrument_task_tokens(self, n_segments,instrument):
        if instrument != "default":
            task_token_ids = [self.model.task_manager.tokenizer.codec.encode_event(event) for event in [Event("program",100),Event("program",101)]] # Singing And chorus
            task_tokens = torch.zeros((n_segments, 1, len(task_token_ids)), dtype=torch.long, device=self.model.device)
            for i in range(n_segments):
                task_tokens[i, 0, :] = torch.tensor(task_token_ids, dtype=torch.long)
            return task_tokens

    def predict(self,filepath,batch_size=8,confidence_threshold:float=0.7,instrument:Literal["singing-only","default"]="default",callback:Callable[[int,int],None]=None,output_path="output.mid"):
        audio, sr = torchaudio.load(filepath)
        audio = torch.mean(audio, dim=0).unsqueeze(0)
        audio = torchaudio.functional.resample(audio, sr, self.model.audio_cfg['sample_rate'])
        audio_segments = slice_padded_array(audio, self.model.audio_cfg['input_frames'], self.model.audio_cfg['input_frames'])
        audio_segments = torch.from_numpy(audio_segments.astype('float32')).to(self.model.device).unsqueeze(1) # (n_seg, 1, seg_sz)
        n_items = audio_segments.shape[0]
        pred_token_arr, _ = self.model.inference_file(bsz=batch_size, audio_segments=audio_segments,callback=callback,task_token_array=self.create_instrument_task_tokens(n_items,instrument))
        num_channels = self.model.task_manager.num_decoding_channels
        start_secs_file = [self.model.audio_cfg['input_frames'] * i / self.model.audio_cfg['sample_rate'] for i in range(n_items)]
        pred_notes_in_file = []
        for ch in range(num_channels):
            pred_token_arr_ch = [arr[:, ch, :] for arr in pred_token_arr]  # (B, L)
            zipped_note_events_and_tie, _, _ = self.model.task_manager.detokenize_list_batches(pred_token_arr_ch, start_secs_file, return_events=True)
            pred_notes_ch, _ = merge_zipped_note_events_and_ties_to_notes(zipped_note_events_and_tie)
            pred_notes_in_file.append(pred_notes_ch)
        note_event2midi(note2note_event([note if note.is_drum else Note(is_drum=note.is_drum,program=self.model.midi_output_inverse_vocab.get(note.program, [note.program])[0],onset=note.onset,offset=note.offset,pitch=note.pitch,velocity=note.velocity) for note in filter_instrument_consistency(mix_notes(pred_notes_in_file),confidence_threshold,allow=bool(instrument !="default"))], return_activity=False), output_path, output_inverse_vocab=self.model.midi_output_inverse_vocab)
        return output_path

