# AIæ¨¡å‹æœåŠ¡æŠ½è±¡å±‚

AIæ¨¡å‹æœåŠ¡æŠ½è±¡å±‚æ˜¯ Midscene Python çš„ç»Ÿä¸€ AI æ¥å£ï¼Œæ”¯æŒå¤šç§ AI æä¾›å•†ï¼Œè®©å¼€å‘è€…å¯ä»¥è½»æ¾åˆ‡æ¢å’Œé…ç½®ä¸åŒçš„ AI æ¨¡å‹ã€‚

## ğŸ¯ è®¾è®¡ç†å¿µ

### ç»Ÿä¸€æŠ½è±¡æ¥å£
ä¸åŒçš„ AI æä¾›å•†æœ‰ç€ä¸åŒçš„ API æ¥å£å’Œè°ƒç”¨æ–¹å¼ï¼ŒAIModelService æä¾›ç»Ÿä¸€çš„æŠ½è±¡å±‚ï¼š

```python
# æ— è®ºä½¿ç”¨å“ªä¸ªæä¾›å•†ï¼Œè°ƒç”¨æ–¹å¼éƒ½ç›¸åŒ
result = await ai_service.call_ai(
    messages=messages,
    response_schema=schema,
    model_config=config
)
```

### å¯æ’æ‹”çš„æä¾›å•†æ¶æ„
æ–°çš„ AI æä¾›å•†å¯ä»¥é€šè¿‡å®ç° AIProvider æ¥å£è½»æ¾é›†æˆï¼š

```python
class CustomAIProvider(AIProvider):
    async def call(self, messages, config, **kwargs):
        # å®ç°è‡ªå®šä¹‰æä¾›å•†é€»è¾‘
        pass

# æ³¨å†Œè‡ªå®šä¹‰æä¾›å•†
ai_service.register_provider("custom", CustomAIProvider())
```

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ ¸å¿ƒç»„ä»¶

```mermaid
graph TB
    A[AIModelService] --> B[Provider Registry]
    A --> C[Config Manager]
    A --> D[Response Parser]
    
    B --> E[OpenAI Provider]
    B --> F[Anthropic Provider]
    B --> G[Qwen Provider]
    B --> H[Gemini Provider]
    B --> I[Custom Provider]
    
    E --> J[OpenAI API]
    F --> K[Claude API]
    G --> L[DashScope API]
    H --> M[Gemini API]
    
    subgraph "æœåŠ¡å±‚"
        A
        B
        C
        D
    end
    
    subgraph "æä¾›å•†å±‚"
        E
        F
        G
        H
        I
    end
    
    subgraph "å¤–éƒ¨ API"
        J
        K
        L
        M
    end
```

### ç±»ç»“æ„è®¾è®¡

```python
class AIModelService:
    """Unified AI model service interface"""
    
    def __init__(self):
        self.providers: Dict[str, AIProvider] = {}
        self._register_providers()
    
    async def call_ai(
        self,
        messages: List[Dict[str, Any]], 
        response_schema: Optional[Type[BaseModel]] = None,
        model_config: Optional[AIModelConfig] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """ç»Ÿä¸€çš„ AI è°ƒç”¨æ¥å£"""
```

## âš™ï¸ é…ç½®ç®¡ç†

### AIModelConfig é…ç½®ç±»

```python
class AIModelConfig(BaseModel):
    """AI model configuration"""
    provider: str                    # æä¾›å•†åç§°
    model: str                      # æ¨¡å‹åç§°
    api_key: str                    # API å¯†é’¥
    base_url: Optional[str] = None  # è‡ªå®šä¹‰ API åœ°å€
    max_tokens: int = 4000          # æœ€å¤§ token æ•°
    temperature: float = 0.1        # éšæœºæ€§æ§åˆ¶
    timeout: int = 60               # è¯·æ±‚è¶…æ—¶
```

### é…ç½®æ–¹å¼

#### 1. ç¯å¢ƒå˜é‡é…ç½®
```bash
# .env æ–‡ä»¶
MIDSCENE_AI_PROVIDER=openai
MIDSCENE_AI_MODEL=gpt-4-vision-preview
MIDSCENE_AI_API_KEY=your_api_key_here
MIDSCENE_AI_BASE_URL=https://api.openai.com/v1
```

#### 2. ä»£ç é…ç½®
```python
from midscene.core.ai_model import AIModelConfig, AIModelService

# åˆ›å»ºé…ç½®
config = AIModelConfig(
    provider="openai",
    model="gpt-4-vision-preview", 
    api_key="your_api_key",
    temperature=0.1,
    max_tokens=2000
)

# åˆ›å»ºæœåŠ¡å®ä¾‹
ai_service = AIModelService()

# ä½¿ç”¨é…ç½®è°ƒç”¨
result = await ai_service.call_ai(
    messages=messages,
    model_config=config
)
```

#### 3. å¤šé…ç½®ç®¡ç†
```python
# ä¸ºä¸åŒä»»åŠ¡é…ç½®ä¸åŒçš„æ¨¡å‹
configs = {
    "locate": AIModelConfig(
        provider="openai",
        model="gpt-4-vision-preview",
        temperature=0.0,  # å®šä½éœ€è¦ç¡®å®šæ€§
        max_tokens=500
    ),
    "extract": AIModelConfig(
        provider="claude", 
        model="claude-3-sonnet-20240229",
        temperature=0.2,  # æå–å…è®¸åˆ›é€ æ€§
        max_tokens=2000
    ),
    "assert": AIModelConfig(
        provider="qwen",
        model="qwen-vl-max",
        temperature=0.1,
        max_tokens=1000
    )
}

# æ ¹æ®ä»»åŠ¡é€‰æ‹©é…ç½®
result = await ai_service.call_ai(
    messages=messages,
    model_config=configs["locate"]
)
```

## ğŸ¤– æ”¯æŒçš„AIæä¾›å•†

### 1. OpenAI
```python
# GPT-4V é…ç½®
openai_config = AIModelConfig(
    provider="openai",
    model="gpt-4-vision-preview",  # æˆ– "gpt-4o"
    api_key="sk-...",
    base_url="https://api.openai.com/v1",  # å¯é€‰
    temperature=0.1
)
```

**æ”¯æŒçš„æ¨¡å‹**:
- `gpt-4-vision-preview`: GPT-4 è§†è§‰æ¨¡å‹
- `gpt-4o`: æœ€æ–°çš„ GPT-4 ä¼˜åŒ–æ¨¡å‹
- `gpt-4-turbo`: GPT-4 Turbo æ¨¡å‹

### 2. Anthropic (Claude)
```python
# Claude é…ç½®
claude_config = AIModelConfig(
    provider="anthropic",
    model="claude-3-sonnet-20240229",
    api_key="sk-ant-...",
    max_tokens=4000
)
```

**æ”¯æŒçš„æ¨¡å‹**:
- `claude-3-opus-20240229`: æœ€å¼ºèƒ½åŠ›æ¨¡å‹
- `claude-3-sonnet-20240229`: å¹³è¡¡æ€§èƒ½æ¨¡å‹
- `claude-3-haiku-20240307`: å¿«é€Ÿå“åº”æ¨¡å‹

### 3. é€šä¹‰åƒé—® (Qwen)
```python
# Qwen é…ç½®
qwen_config = AIModelConfig(
    provider="qwen",
    model="qwen-vl-max",
    api_key="sk-...",  # DashScope API Key
    temperature=0.1
)
```

**æ”¯æŒçš„æ¨¡å‹**:
- `qwen-vl-max`: é€šä¹‰åƒé—®è§†è§‰ç†è§£æ¨¡å‹
- `qwen-vl-plus`: å¢å¼ºç‰ˆè§†è§‰æ¨¡å‹
- `qwen2.5-vl`: æœ€æ–°ç‰ˆæœ¬æ¨¡å‹

### 4. Google Gemini
```python
# Gemini é…ç½®
gemini_config = AIModelConfig(
    provider="gemini",
    model="gemini-1.5-pro-vision",
    api_key="AIza...",
    temperature=0.2
)
```

**æ”¯æŒçš„æ¨¡å‹**:
- `gemini-1.5-pro-vision`: Gemini 1.5 Pro è§†è§‰æ¨¡å‹
- `gemini-pro-vision`: Gemini Pro è§†è§‰æ¨¡å‹

## ğŸ”Œ æä¾›å•†æ¥å£

### AIProvider æŠ½è±¡åŸºç±»

```python
class AIProvider(ABC):
    """Abstract base class for AI service providers"""
    
    @abstractmethod
    async def call(
        self,
        messages: List[Dict[str, Any]],
        config: AIModelConfig,
        response_schema: Optional[Type[BaseModel]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """Call AI service"""
        pass
```

### è‡ªå®šä¹‰æä¾›å•†å®ç°

```python
class CustomAIProvider(AIProvider):
    """è‡ªå®šä¹‰ AI æä¾›å•†ç¤ºä¾‹"""
    
    async def call(
        self,
        messages: List[Dict[str, Any]],
        config: AIModelConfig,
        response_schema: Optional[Type[BaseModel]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        
        # 1. æ„å»ºè¯·æ±‚
        request_data = self._build_request(messages, config)
        
        # 2. è°ƒç”¨ API
        async with httpx.AsyncClient() as client:
            response = await client.post(
                config.base_url,
                headers={"Authorization": f"Bearer {config.api_key}"},
                json=request_data,
                timeout=config.timeout
            )
            response.raise_for_status()
        
        # 3. è§£æå“åº”
        result = response.json()
        content = self._extract_content(result)
        
        # 4. å¤„ç†ç»“æ„åŒ–å“åº”
        if response_schema:
            parsed_content = parse_json_response(content)
            validated_content = response_schema(**parsed_content)
            content = validated_content.dict()
        
        # 5. è¿”å›ç»Ÿä¸€æ ¼å¼
        return {
            "content": content,
            "usage": self._extract_usage(result)
        }
    
    def _build_request(self, messages, config):
        """æ„å»º API è¯·æ±‚"""
        return {
            "model": config.model,
            "messages": messages,
            "max_tokens": config.max_tokens,
            "temperature": config.temperature
        }
    
    def _extract_content(self, response):
        """æå–å“åº”å†…å®¹"""
        return response["choices"][0]["message"]["content"]
    
    def _extract_usage(self, response):
        """æå–ä½¿ç”¨ç»Ÿè®¡"""
        usage = response.get("usage", {})
        return {
            "prompt_tokens": usage.get("prompt_tokens", 0),
            "completion_tokens": usage.get("completion_tokens", 0),
            "total_tokens": usage.get("total_tokens", 0)
        }

# æ³¨å†Œè‡ªå®šä¹‰æä¾›å•†
ai_service = AIModelService()
ai_service.register_provider("custom", CustomAIProvider())
```

## ğŸ“ æ¶ˆæ¯æ ¼å¼

### ç»Ÿä¸€æ¶ˆæ¯æ ¼å¼
```python
messages = [
    {
        "role": "system",
        "content": "ä½ æ˜¯ä¸€ä¸ªUIè‡ªåŠ¨åŒ–åŠ©æ‰‹..."
    },
    {
        "role": "user",
        "content": [
            {
                "type": "text", 
                "text": "è¯·åœ¨é¡µé¢ä¸­æ‰¾åˆ°ç™»å½•æŒ‰é’®"
            },
            {
                "type": "image_url",
                "image_url": {
                    "url": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAA..."
                }
            }
        ]
    }
]
```

### å¤šæ¨¡æ€æ¶ˆæ¯æ”¯æŒ
```python
# æ–‡æœ¬ + å›¾åƒ
multimodal_message = {
    "role": "user",
    "content": [
        {"type": "text", "text": "åˆ†æè¿™ä¸ªé¡µé¢çš„å¸ƒå±€"},
        {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{image_b64}"}}
    ]
}

# çº¯æ–‡æœ¬
text_message = {
    "role": "user", 
    "content": "è§£é‡Šè‡ªåŠ¨åŒ–æµ‹è¯•çš„æ¦‚å¿µ"
}

# ç³»ç»Ÿæç¤º
system_message = {
    "role": "system",
    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„UIè‡ªåŠ¨åŒ–ä¸“å®¶"
}
```

## ğŸ”„ å“åº”å¤„ç†

### ç»“æ„åŒ–å“åº”
```python
from pydantic import BaseModel

class LocateResponse(BaseModel):
    elements: List[Dict[str, Any]]
    reasoning: str
    confidence: float

# è¯·æ±‚ç»“æ„åŒ–å“åº”
result = await ai_service.call_ai(
    messages=messages,
    response_schema=LocateResponse,
    model_config=config
)

# è‡ªåŠ¨éªŒè¯å’Œè§£æ
locate_data = result["content"]  # å·²ç»æ˜¯ LocateResponse å­—å…¸æ ¼å¼
```

### é”™è¯¯å¤„ç†
```python
try:
    result = await ai_service.call_ai(messages, config)
except ValueError as e:
    # é…ç½®é”™è¯¯
    logger.error(f"Configuration error: {e}")
except httpx.TimeoutException as e:
    # è¯·æ±‚è¶…æ—¶
    logger.error(f"Request timeout: {e}")
except httpx.HTTPStatusError as e:
    # HTTP é”™è¯¯
    logger.error(f"HTTP error: {e.response.status_code}")
except Exception as e:
    # å…¶ä»–é”™è¯¯
    logger.error(f"Unexpected error: {e}")
```

## ğŸ“Š ä½¿ç”¨ç»Ÿè®¡

### AIUsageInfo ç±»
```python
class AIUsageInfo(BaseModel):
    """AI ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯"""
    prompt_tokens: int       # è¾“å…¥ token æ•°
    completion_tokens: int   # è¾“å‡º token æ•°  
    total_tokens: int        # æ€» token æ•°
    cost: Optional[float]    # è´¹ç”¨ï¼ˆå¦‚æœå¯è®¡ç®—ï¼‰
```

### ä½¿ç”¨ç»Ÿè®¡æ”¶é›†
```python
# è°ƒç”¨åè·å–ä½¿ç”¨ç»Ÿè®¡
result = await ai_service.call_ai(messages, config)
usage = result.get("usage", {})

print(f"è¾“å…¥ tokens: {usage['prompt_tokens']}")
print(f"è¾“å‡º tokens: {usage['completion_tokens']}")
print(f"æ€»è®¡ tokens: {usage['total_tokens']}")

# ç´¯ç§¯ç»Ÿè®¡
class UsageTracker:
    def __init__(self):
        self.total_tokens = 0
        self.total_calls = 0
        self.total_cost = 0.0
    
    def record_usage(self, usage: Dict[str, Any]):
        self.total_tokens += usage.get("total_tokens", 0)
        self.total_calls += 1
        self.total_cost += usage.get("cost", 0.0)

tracker = UsageTracker()
```

## ğŸš€ æ€§èƒ½ä¼˜åŒ–

### è¿æ¥æ± é…ç½®
```python
class OptimizedAIProvider(AIProvider):
    def __init__(self):
        # é…ç½®è¿æ¥æ± 
        self.client = httpx.AsyncClient(
            limits=httpx.Limits(
                max_keepalive_connections=10,
                max_connections=20
            ),
            timeout=httpx.Timeout(60.0)
        )
    
    async def call(self, messages, config, **kwargs):
        # å¤ç”¨è¿æ¥
        response = await self.client.post(...)
        return response.json()
```

### è¯·æ±‚é‡è¯•
```python
import asyncio
from tenacity import retry, stop_after_attempt, wait_exponential

class ReliableAIProvider(AIProvider):
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def call(self, messages, config, **kwargs):
        # å¸¦é‡è¯•çš„ API è°ƒç”¨
        return await self._make_request(messages, config)
```

### æ‰¹é‡è¯·æ±‚
```python
async def batch_call_ai(
    ai_service: AIModelService,
    requests: List[Dict[str, Any]],
    config: AIModelConfig,
    concurrency: int = 3
) -> List[Dict[str, Any]]:
    """æ‰¹é‡è°ƒç”¨ AI æœåŠ¡"""
    
    semaphore = asyncio.Semaphore(concurrency)
    
    async def single_call(request):
        async with semaphore:
            return await ai_service.call_ai(
                messages=request["messages"],
                response_schema=request.get("schema"),
                model_config=config
            )
    
    tasks = [single_call(req) for req in requests]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    return results
```

## ğŸ”§ è°ƒè¯•å’Œç›‘æ§

### è¯·æ±‚æ—¥å¿—
```python
import logging

# å¯ç”¨è¯¦ç»†æ—¥å¿—
logging.getLogger("httpx").setLevel(logging.DEBUG)

# è‡ªå®šä¹‰æ—¥å¿—ä¸­é—´ä»¶
class LoggingAIProvider(AIProvider):
    async def call(self, messages, config, **kwargs):
        logger.info(f"Calling {config.provider} with model {config.model}")
        logger.debug(f"Messages: {messages}")
        
        start_time = time.time()
        try:
            result = await self._make_request(messages, config)
            duration = time.time() - start_time
            logger.info(f"Request completed in {duration:.2f}s")
            return result
        except Exception as e:
            duration = time.time() - start_time
            logger.error(f"Request failed after {duration:.2f}s: {e}")
            raise
```

### æ€§èƒ½ç›‘æ§
```python
class PerformanceMonitor:
    def __init__(self):
        self.metrics = {}
    
    def record_call(self, provider: str, duration: float, tokens: int, success: bool):
        if provider not in self.metrics:
            self.metrics[provider] = {
                "total_calls": 0,
                "total_duration": 0,
                "total_tokens": 0,
                "success_count": 0
            }
        
        metrics = self.metrics[provider]
        metrics["total_calls"] += 1
        metrics["total_duration"] += duration
        metrics["total_tokens"] += tokens
        if success:
            metrics["success_count"] += 1
    
    def get_stats(self, provider: str) -> Dict[str, float]:
        metrics = self.metrics.get(provider, {})
        total_calls = metrics.get("total_calls", 0)
        
        if total_calls == 0:
            return {}
        
        return {
            "avg_duration": metrics["total_duration"] / total_calls,
            "avg_tokens": metrics["total_tokens"] / total_calls,
            "success_rate": metrics["success_count"] / total_calls,
            "total_calls": total_calls
        }

# ä½¿ç”¨ç›‘æ§
monitor = PerformanceMonitor()
ai_service.set_monitor(monitor)
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. æ¨¡å‹é€‰æ‹©ç­–ç•¥
```python
def choose_model(task_type: str, complexity: str) -> AIModelConfig:
    """æ ¹æ®ä»»åŠ¡ç±»å‹å’Œå¤æ‚åº¦é€‰æ‹©åˆé€‚çš„æ¨¡å‹"""
    
    if task_type == "locate":
        # å®šä½ä»»åŠ¡éœ€è¦é«˜ç²¾åº¦
        return AIModelConfig(
            provider="openai",
            model="gpt-4-vision-preview",
            temperature=0.0
        )
    elif task_type == "extract" and complexity == "high":
        # å¤æ‚æå–ä½¿ç”¨æœ€å¼ºæ¨¡å‹
        return AIModelConfig(
            provider="anthropic", 
            model="claude-3-opus-20240229",
            temperature=0.1
        )
    else:
        # ä¸€èˆ¬ä»»åŠ¡ä½¿ç”¨å¹³è¡¡æ¨¡å‹
        return AIModelConfig(
            provider="qwen",
            model="qwen-vl-max",
            temperature=0.1
        )
```

### 2. é”™è¯¯é‡è¯•ç­–ç•¥
```python
async def robust_ai_call(
    ai_service: AIModelService,
    messages: List[Dict],
    config: AIModelConfig,
    max_retries: int = 3
) -> Dict[str, Any]:
    """å¸¦é‡è¯•çš„ AI è°ƒç”¨"""
    
    for attempt in range(max_retries):
        try:
            return await ai_service.call_ai(messages, config)
        except httpx.TimeoutException:
            if attempt < max_retries - 1:
                await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                continue
            raise
        except httpx.HTTPStatusError as e:
            if e.response.status_code >= 500 and attempt < max_retries - 1:
                await asyncio.sleep(2 ** attempt)
                continue
            raise
```

### 3. æˆæœ¬æ§åˆ¶
```python
class CostController:
    def __init__(self, daily_limit: float = 10.0):
        self.daily_limit = daily_limit
        self.daily_usage = 0.0
        self.last_reset = datetime.now().date()
    
    def check_limit(self, estimated_cost: float) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¶…å‡ºæ¯æ—¥é™é¢"""
        today = datetime.now().date()
        if today != self.last_reset:
            self.daily_usage = 0.0
            self.last_reset = today
        
        return (self.daily_usage + estimated_cost) <= self.daily_limit
    
    def record_usage(self, cost: float):
        """è®°å½•ä½¿ç”¨è´¹ç”¨"""
        self.daily_usage += cost

# åœ¨è°ƒç”¨å‰æ£€æŸ¥
cost_controller = CostController(daily_limit=20.0)
if cost_controller.check_limit(estimated_cost):
    result = await ai_service.call_ai(messages, config)
    cost_controller.record_usage(actual_cost)
```

## ğŸ”— ç›¸å…³æ–‡æ¡£

- **é…ç½®æŒ‡å—**: [AIæ¨¡å‹é…ç½®](../AIæ¨¡å‹é…ç½®/é…ç½®æ–¹æ³•.md)
- **æä¾›å•†é…ç½®**: [æ”¯æŒçš„AIæä¾›å•†](../AIæ¨¡å‹é…ç½®/æ”¯æŒçš„AIæä¾›å•†/README.md)
- **é›†æˆä½¿ç”¨**: [Agent æ ¸å¿ƒæ§åˆ¶å™¨](Agentæ ¸å¿ƒæ§åˆ¶å™¨.md)
- **é«˜çº§ç‰¹æ€§**: [ç¼“å­˜ç­–ç•¥](../AIæ¨¡å‹é…ç½®/é«˜çº§é€‰é¡¹/ç¼“å­˜ç­–ç•¥.md)

---

AIæ¨¡å‹æœåŠ¡æŠ½è±¡å±‚ä¸º Midscene Python æä¾›äº†å¼ºå¤§è€Œçµæ´»çš„ AI é›†æˆèƒ½åŠ›ã€‚é€šè¿‡ç»Ÿä¸€çš„æ¥å£ï¼Œä½ å¯ä»¥è½»æ¾åˆ‡æ¢ä¸åŒçš„ AI æä¾›å•†ï¼Œå¹¶æ ¹æ®å…·ä½“éœ€æ±‚ä¼˜åŒ–æ¨¡å‹é…ç½®ï¼