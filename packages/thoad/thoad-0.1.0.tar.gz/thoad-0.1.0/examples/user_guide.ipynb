{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f14b7494",
   "metadata": {},
   "source": [
    "# **THOAD USER GUIDE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1f5e44",
   "metadata": {},
   "source": [
    "### **What is thoad**\n",
    "\n",
    "thoad (Torch High Order Auto-Differentiation) package contains a full-Python implementation of a reverse-mode auto-differentiation engine for PyTorch.\n",
    "It is developed in Python 3.12 and uses PyTorch (2.4+) as its only dependency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ec4f05",
   "metadata": {},
   "source": [
    "## **1. Computing Arbitrary Order Derivatives**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84f19f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd026f9",
   "metadata": {},
   "source": [
    "### **1.1 Setting up dependencies**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89897ad2",
   "metadata": {},
   "source": [
    "Import **PyTorch** and **thoad**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f433e239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3bb4b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thoad import backward, Controller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b54d5d",
   "metadata": {},
   "source": [
    "> In as much thoad relies heavily on torch einsum operator, enabeling `opt_einsum` backend can benefit performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4ec2edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt_einsum backend enabled\n"
     ]
    }
   ],
   "source": [
    "import torch.backends.opt_einsum as opt_einsum\n",
    "\n",
    "if opt_einsum.is_available():\n",
    "    opt_einsum.enabled = True\n",
    "    opt_einsum.strategy = \"greedy\"\n",
    "    print(\"opt_einsum backend enabled\")\n",
    "else:\n",
    "    print(\"opt_einsum backend is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e50c046",
   "metadata": {},
   "source": [
    "### **1.2 Calling thoad auto-differentiation via the backward function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feee0845",
   "metadata": {},
   "source": [
    "Define the dynamic computational graph as is normally done in PyTorch. Just define a series of traceable tensors and feed them into a composition of operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73c0ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1: torch.Tensor = torch.rand(size=(2,3), requires_grad=True, device=device)\n",
    "X2: torch.Tensor = torch.rand(size=(3,4), requires_grad=True, device=device)\n",
    "\n",
    "def forward(X1: torch.Tensor, X2: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.sigmoid(input=(X1 @ X2))\n",
    "\n",
    "Y: torch.Tensor = forward(X1=X1, X2=X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48f6af2",
   "metadata": {},
   "source": [
    "Call the `thoad.backward` function to compute the partial derivatives of order *o* w.r.t. inputs $\\normalsize \\left( \\text{i.e. } \\;\\; \\frac{\\normalsize \\delta^{\\otimes o} \\; Y}{\\normalsize \\bigotimes_{i=1}^{o} \\delta X_i} \\right)$. This will aggregate 2 new attributes to the input tensors, `hgrad`and `hdata`.\n",
    "\n",
    "\n",
    "`thoad.backward` takes the following argumntes.\n",
    "\n",
    "* `tensor: torch.Tensor`: Output tensor (must have `requires_grad=True` and a `grad_fn`); root to differentiate from.\n",
    "* `order: int`: Positive derivative order $o \\ge 1$.\n",
    "* `gradient: Optional[torch.Tensor] = None`: (Optional) Upstream seed with same shape as `tensor`; for $o=1$ performs a VJP; if `None`, uses the identity seed.\n",
    "* `crossings: bool = False`: (Optional) If `True`, computes mixed partials across different terminal tensors (cross-terminal derivatives).\n",
    "* `groups: Optional[Iterable[Iterable[torch.Tensor]]] = None`: (Optional) Terminal groups that allow mixed partials only **within** each group; mutually exclusive with `crossings=True`.\n",
    "* `keep_batch: bool = False`: (Optional) Controls whether mutually independent dimensions (detected via **Indeps**) are explicitly regularized. If `True`, unified batch-independent dimensions remain unified in the resulting `hgrad`/`hdata` shapes.\n",
    "* `keep_schwarz: bool = False`: (Optional) Controls whether variable-order symmetries (detected via **VPerm**) are resolved or preserved. If `True`, derivatives that are symmetric under Schwarz theorem remain stored as permuted references rather than expanded tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f070e5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<thoad.user.interface.Controller at 0x19058e23bc0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute auto-differentiation from Y tensor\n",
    "o: int = 2\n",
    "backward(tensor=Y, order=o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea568ce",
   "metadata": {},
   "source": [
    "Access the newly included attributes to get the derivatives w.r.t. input tensors.\n",
    "\n",
    "- `Tensor.hgrad` contains a tuple of lenght *o* - 1 with the derivative tensors from order 1 to *o*\n",
    "- `Tensor.hdata` contains a tuple of lenght *o* - 1 with nested tuples of lenght 3 containing matadata (**shapes**, **indeps**, **variable permutations**) about its correstponding tensor.\n",
    "\n",
    "hgrad tensors have the following shape:\n",
    "\n",
    "  - (output_flattened_size, *(shape_input_tensor_0), ..., *(shape_input_tensor_o-1))\n",
    "\n",
    "If thoad.backward is called with default configuration hdata information has no practical value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3dea823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.functional.hessian only support autodifferentiation from scalar tensors\n",
    "fwd_sum: Callable[[torch.Tensor, torch.Tensor], torch.Tensor]\n",
    "fwd_sum = lambda X1, X2: torch.sum(forward(X1=X1, X2=X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40d6300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain input gradients\n",
    "grad_X1: torch.Tensor = X1.hgrad[0]\n",
    "grad_X1X1: torch.Tensor = X1.hgrad[1]\n",
    "grad_X2: torch.Tensor = X2.hgrad[0]\n",
    "grad_X2X2: torch.Tensor = X2.hgrad[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e05b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check X1 derivatives\n",
    "assert grad_X1.shape == (Y.numel(), *X1.shape)\n",
    "assert grad_X1X1.shape == (Y.numel(), *X1.shape, *X1.shape)\n",
    "assert torch.allclose(\n",
    "    grad_X1.flatten(),\n",
    "    torch.autograd.functional.jacobian(func=forward, inputs=(X1, X2))[0].flatten(),\n",
    ")\n",
    "assert torch.allclose(\n",
    "    grad_X1X1.sum(0).flatten(),\n",
    "    torch.autograd.functional.hessian(func=fwd_sum, inputs=(X1, X2))[0][0].flatten(),\n",
    ")\n",
    "\n",
    "# check X2 derivatives\n",
    "assert grad_X2.shape == (Y.numel(), *X2.shape)\n",
    "assert grad_X2X2.shape == (Y.numel(), *X2.shape, *X2.shape)\n",
    "assert torch.allclose(\n",
    "    grad_X2.flatten(),\n",
    "    torch.autograd.functional.jacobian(func=forward, inputs=(X1, X2))[1].flatten(),\n",
    ")\n",
    "assert torch.allclose(\n",
    "    grad_X2X2.sum(0).flatten(),\n",
    "    torch.autograd.functional.hessian(func=fwd_sum, inputs=(X1, X2))[1][1].flatten(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53a930a",
   "metadata": {},
   "source": [
    "To obtain the cross derivatives:\n",
    "1. First use the *crossings* argument of `backward` to indicate thoad to compute them\n",
    "2. Then collect the `Controller` returned by backward and use its `Controller.fetch_grad` method indicating the desired variables\n",
    "\n",
    "> Note. setting *crossings=True* will have a significant impact on performance.\n",
    "\n",
    "> Note. thoad overrides, not accumulates, the gradients of repeated AD calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe13b576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute auto-differentiation from Y tensor\n",
    "o: int = 2\n",
    "ctrl: Controller = backward(tensor=Y, order=o, crossings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2b522db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain input gradients\n",
    "grad_X1X2: torch.Tensor\n",
    "grad_X1X2, _ = ctrl.fetch_hgrad(variables=(X1, X2))\n",
    "grad_X2X1: torch.Tensor\n",
    "grad_X2X1, _ = ctrl.fetch_hgrad(variables=(X2, X1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c619309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(\n",
    "    grad_X1X2.sum(0).flatten(),\n",
    "    torch.autograd.functional.hessian(func=fwd_sum, inputs=(X1, X2))[0][1].flatten(),\n",
    ")\n",
    "assert torch.allclose(\n",
    "    grad_X2X1.sum(0).flatten(),\n",
    "    torch.autograd.functional.hessian(func=fwd_sum, inputs=(X1, X2))[1][0].flatten(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005d83dd",
   "metadata": {},
   "source": [
    "Alternatively to *crossings*, **thoad** also provides the *groups* argument, which lets the user explicitly specify among which subsets of Tensors it must compute cross derivatives. Cross derivatives among tensors which do not appear in a common specified group will not be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cbd0c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1: torch.Tensor = torch.rand(size=(2,3), requires_grad=True, device=device)\n",
    "X2: torch.Tensor = torch.rand(size=(3,4), requires_grad=True, device=device)\n",
    "X3: torch.Tensor = torch.rand(size=(2,4), requires_grad=True, device=device)\n",
    "\n",
    "def forward(X1: torch.Tensor, X2: torch.Tensor, X3: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.sigmoid(input=(X1 @ X2 + X3))\n",
    "\n",
    "Y: torch.Tensor = forward(X1=X1, X2=X2, X3=X3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7eb220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute auto-differentiation from Y tensor\n",
    "o: int = 2\n",
    "ctrl: Controller = backward(tensor=Y, order=o, groups=({X1, X2}, {X1, X3}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "094a73ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'No gradient saved for given key'\n",
      "'No gradient saved for given key'\n"
     ]
    }
   ],
   "source": [
    "ctrl.fetch_hgrad(variables=(X1, X2))\n",
    "ctrl.fetch_hgrad(variables=(X2, X1))\n",
    "ctrl.fetch_hgrad(variables=(X1, X3))\n",
    "ctrl.fetch_hgrad(variables=(X3, X1))\n",
    "\n",
    "for pair in [(X2, X3), (X3, X2)]:\n",
    "    try:\n",
    "        ctrl.fetch_hgrad(variables=pair)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9addcb36",
   "metadata": {},
   "source": [
    "### **1.3 Calling thoad auto-differentiation via the Controller interface**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc34fe44",
   "metadata": {},
   "source": [
    "Define the **PyTorch** graph the same way as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36ab8710",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1: torch.Tensor = torch.rand(size=(2,3), requires_grad=True, device=device)\n",
    "X2: torch.Tensor = torch.rand(size=(3,4), requires_grad=True, device=device)\n",
    "\n",
    "def forward(X1: torch.Tensor, X2: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.sigmoid(input=(X1 @ X2))\n",
    "\n",
    "Y: torch.Tensor = forward(X1=X1, X2=X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bea8faa",
   "metadata": {},
   "source": [
    "Instantiate the Controller interface passing as argument the output tensor and call its backward method equivalently (but without the tensor argument)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "694bf4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "o: int = 2\n",
    "ctrl: Controller = Controller(tensor=Y)\n",
    "ctrl.backward(order=o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c3d6dd",
   "metadata": {},
   "source": [
    "Finally obtain the derivatives using the `Controller.fetch_hgrad` method as before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f11b5f5",
   "metadata": {},
   "source": [
    "### **1.4 Using derivative metadata**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3671162e",
   "metadata": {},
   "source": [
    "#### **About Tensor Metadata (shapes, indeps, variable permutations)**\n",
    "\n",
    "**Shapes = Tuple[Shape, ...] = Tuple[Tuple[int, ...], ...]**  \n",
    "Indicate the form of each of the variables with respect to which the derivative tensor is differentiated.\n",
    "```\n",
    "ctrl = thoad.backward(tensor=Y, order=3)  \n",
    "hgrad, hdata = ctrl.fetch_hgrad(variables=(X1, X2, X1))  \n",
    "assert hgrad.shape == (hgrad.shape[0], *hdata[0])  \n",
    "```  \n",
    "\n",
    "**Indeps = Tuple[Indep, ...] = Tuple[Tuple[Union[None, int], ...], ...]**  \n",
    "Mutually independent dimensions are those that have all crossed terms null, thus, only their nd-diagonal contains meaningful elements. THOAD back-propagation leverages this by unifying each group of mutually independent dimensions into a single one.  \n",
    "If thoad.backward is executed with `keep_batch=True` mutually independent dimensions unified will be kept unified.\n",
    "- (output_flattened_size, *(independent_dimensions), *(non_independent_shape_input_tensor_0), ..., *(non_independent_shape_input_tensor_o-1))  \n",
    "\n",
    "Each Indep will share length with **independent_dimensions**, and will indicate which index in corresponding differentiation is unified into that independent_dimensions position. Unification of no dimension into position will be marked as None.\n",
    "*Example:*\n",
    "- *shapes=( (2,4),(3,5),(2,4) ) + indeps=( (None,None),(None,None),(None,None) ) -> hgrad.shape=(Y.numel(), 1,1, 2,4, 3,5, 2,4)*\n",
    "- *shapes=( (2,4),(3,5),(2,4) ) + indeps=( (0,None),(None,1),(0,None) ) -> hgrad.shape=(Y.numel(), 2,5, 4, 3, 4)*\n",
    "\n",
    "**VPerm = Tuple[int, ...]**  \n",
    "In subgraphs of operators satisfying scharz theorem, derivatives w.r.t. permutations of the same variable set are symmetric. thoad back-propagation leverages this symmetries by avoiding their computation when a symmetric one has already been computed. Instead it expresses it as that already symetric computed derivative wrapped by a permutation.  \n",
    "If thoad.backward is executed with `keep_schwarz=True` derivatives expressed as a permuted others will be kept expressed like that. This permuation can be found in VPerm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a5bf70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1: torch.Tensor = torch.rand(size=(2,3), requires_grad=True, device=device)\n",
    "X2: torch.Tensor = torch.rand(size=(3,4), requires_grad=True, device=device)\n",
    "\n",
    "def forward(X1: torch.Tensor, X2: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.sigmoid(input=(X1 @ X2))\n",
    "\n",
    "Y: torch.Tensor = forward(X1=X1, X2=X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4944fd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "o: int = 2\n",
    "ctrl: Controller = backward(\n",
    "    tensor=Y,\n",
    "    order=o,\n",
    "    crossings=True,\n",
    "    keep_batch=True,\n",
    "    keep_schwarz=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54367517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thoad.typing import Shape, Indep, VPerm\n",
    "\n",
    "grad_X1X2: torch.Tensor\n",
    "metadata_X1X2: Tuple[Tuple[Shape, ...], Tuple[Indep, ...], VPerm]\n",
    "grad_X1X2, metadata_X1X2 = ctrl.fetch_hgrad(variables=(X1, X2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8098bfd",
   "metadata": {},
   "source": [
    "## **2. Other Utils**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1f1321",
   "metadata": {},
   "source": [
    "### **2.1 Checking operator compatibility**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b445df",
   "metadata": {},
   "source": [
    "By date of 2025 **PyTorch** has more than 2000 operators. **thoad** does not support all of them, but it provides 2 different tools to let users check whether a given **PyTorch** graph is fully composed by compatible operators.\n",
    "\n",
    "- the `Controller.compatible` attribute\n",
    "- the `Controller.display_graph` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9549be",
   "metadata": {},
   "source": [
    "The `Controller.compatible` attribute simply returns a boolean value indicating if all the operators of the subgraph pending from the tensor passed in `Controller` instantiation are compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6669997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X: torch.Tensor = torch.rand(size=(1,), requires_grad=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26b16b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "Y: torch.Tensor = torch.sigmoid(input=X)\n",
    "ctrl: Controller = Controller(tensor=Y)\n",
    "print(ctrl.compatible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c528a91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "Y: torch.Tensor = torch.special.ndtr(input=X)\n",
    "ctrl: Controller = Controller(tensor=Y)\n",
    "print(ctrl.compatible)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231b4e64",
   "metadata": {},
   "source": [
    "The `Controller.display_graph` method displays a diagram of the full subgraph pending from the tensor passed in `Controller` instantiation, aggregating a *not supported* flag next to not supported **PyTorch** `grad_fn` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a18befb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1: torch.Tensor = torch.rand(size=(1,), requires_grad=True, device=device)\n",
    "X2: torch.Tensor = torch.rand(size=(1,), requires_grad=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8abee22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┬\u001b[31m·<ErfBackward0 object at 0x000001906F116200> (not supported)\u001b[0m\n",
      "└─·<DotBackward0 object at 0x000001906F116020>\n",
      "  ├─·<AccumulateGrad object at 0x000001906F116320>\n",
      "  └─·<AccumulateGrad object at 0x000001906F115FF0>\n"
     ]
    }
   ],
   "source": [
    "Y: torch.Tensor = torch.special.erf(input=(X1 @ X2))\n",
    "ctrl: Controller = Controller(tensor=Y)\n",
    "ctrl.display_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e136253",
   "metadata": {},
   "source": [
    "### **2.2 Saving intermediate gradients** (experimental feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7598f036",
   "metadata": {},
   "source": [
    "**thoad**'s auto-differentiation only saves derivatives of graph leaf tensors. To activate the retention of the derivative of some combination of intermediate tensors use the `Controller.require_grad_` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b54dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X: torch.Tensor = torch.rand(size=(1,), requires_grad=True, device=device)\n",
    "Y: torch.Tensor = torch.sigmoid(input=X)\n",
    "Z: torch.Tensor = torch.softmax(input=Y, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9153391",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl: Controller = Controller(tensor=Z)\n",
    "ctrl.require_grad_(variables=(Y,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96c39124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.4211e-14]], grad_fn=<SumBackward1>), (((1,),), ((None,),), (0,)))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute auto-differentiation\n",
    "o: int = 2\n",
    "ctrl.backward(order=o)\n",
    "\n",
    "# check that Y derivatives are indeed saved\n",
    "ctrl.fetch_hgrad(variables=(Y,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ced2cf",
   "metadata": {},
   "source": [
    "### **2.3 Removing dynamic tensor attributes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41137561",
   "metadata": {},
   "source": [
    "As seen before, running the auto-differentiation process will aggregate 2 derivative-related attributes. This dynamic aggregation of attributes can be an undesirable behaviour to some users. Conveniently, calling the `Controller.clear` method will remove these attributes from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7e80fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl.clear()\n",
    "\n",
    "assert \"hgrad\" not in dir(X)\n",
    "assert \"hdata\" not in dir(X)\n",
    "assert \"hgrad\" not in dir(Y)\n",
    "assert \"hdata\" not in dir(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4ffe20",
   "metadata": {},
   "source": [
    "### **2.4 Accessing high order backward functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fb6fce",
   "metadata": {},
   "source": [
    "Throughout the auto-differentiation execution **thoad** provides the supported operators their own implementation of ExtendedAutogradFunction(s), equivalent to corresponding torch.autograd.Function(s), but capable of computing internal derivatives of arbitrary order. This is done using an index that maps them one to one, and is exposed to the user through the readeable / writeable `Controller.index` attibute. This attribute can be used to override or aggregate new ExtendedAutogradFunction(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bae58377",
   "metadata": {},
   "outputs": [],
   "source": [
    "X: torch.Tensor = torch.rand(size=(1,), requires_grad=True, device=device)\n",
    "Y: torch.Tensor = torch.sigmoid(input=X)\n",
    "ctrl: Controller = Controller(tensor=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c8a345c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'SigmoidBackward0'>  ->  <class 'thoad.differentiation.internals.mathematic.sigmoid.SigmoidXBackward0'>\n"
     ]
    }
   ],
   "source": [
    "from thoad.typing import AutogradFunction\n",
    "from thoad.differentiation import ExtendedAutogradFunction\n",
    "\n",
    "xbackwards: dict[type[AutogradFunction], type[ExtendedAutogradFunction]] = ctrl.index\n",
    "grad_fn: Optional[AutogradFunction] = Y.grad_fn\n",
    "assert grad_fn is not None\n",
    "print(type(Y.grad_fn), \" -> \", xbackwards[type(grad_fn)])\n",
    "ctrl.index = xbackwards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9895a531",
   "metadata": {},
   "source": [
    "### **2.5 Modifying config settings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1e6e0d",
   "metadata": {},
   "source": [
    "**thoad** executes the auto-differentiation process following 3 settings with a default configuration. The value of this settings can be modified dynamically throughout the program. The settings are:\n",
    "\n",
    "- `config.DEBUG`: Controls the activation of a series of internal checks to catch bugs early. (defauts to *False*)\n",
    "- `config.BATCH_OPTIMIZATION`: Controls the activation of batch dimensions unification (subsets of dimensions with pair to pair null non-diagonal elements). (defauts to *True*)\n",
    "- `config.SCHWARZ_OPTIMIZATION`: Controls the activation of the usage of variable symetries to avoid repeated derivarive computations. (defauts to *True*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32a57d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import thoad.config as config\n",
    "\n",
    "config.DEBUG = False\n",
    "config.BATCH_OPTIMIZATION = True\n",
    "config.SCHWARZ_OPTIMIZATION = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61953e9c",
   "metadata": {},
   "source": [
    "## **3. Registering Hooks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889a8bc9",
   "metadata": {},
   "source": [
    "Pursuing to some degree the flexibility of **PyTorch**'s autograd, **thoad** also allows the registration of hooks for the auto-differentiation process execution. However, due to internal differences in design, **thoad**'s hooks are not attached to backward functions, but to combinations of internal variables (i.e. graph nodes, i.e tensors). To register a backward hook use the `Controller.register_backward_hook` method.\n",
    "\n",
    "Hook functions must expect 2 arguments:\n",
    "1. `grad_data`: *Tuple[torch.Tensor, Tuple[Shape, ...], Tuple[Indep, ...], VPerm]*  \n",
    "   All external derivative data. Returned grad_data must keep shapes as they are.\n",
    "2. `context`: *dict[AutogradFunction, set[torch.Tensor]]*  \n",
    "   Dictionary mapping all hook backward functions (function pointing to any of hook's key tensors) with their pointed tensors. *AutogradFunction* generally contains operator context info saved in forward pass. *set[torch.Tensor]* purpose is to help the user to identify each backward function in case of repeated operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f13ce69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X: torch.Tensor = torch.rand(size=(1,), requires_grad=True, device=device)\n",
    "Y: torch.Tensor = torch.sigmoid(input=X)\n",
    "Z: torch.Tensor = torch.softmax(input=Y, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc0f8ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook(\n",
    "    grad_data: Tuple[\n",
    "        Optional[torch.Tensor],\n",
    "        Optional[Tuple[Shape, ...]],\n",
    "        Optional[Tuple[Indep, ...]],\n",
    "        Optional[VPerm]\n",
    "    ],\n",
    "    context: dict[AutogradFunction, set[torch.Tensor]],\n",
    "    ) -> Tuple[\n",
    "        Optional[torch.Tensor],\n",
    "        Optional[Tuple[Shape, ...]],\n",
    "        Optional[Tuple[Indep, ...]],\n",
    "        Optional[VPerm]\n",
    "    ]:\n",
    "    return grad_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "542703cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "o: int = 2\n",
    "ctrl: Controller = Controller(tensor=Z)\n",
    "ctrl.register_backward_hook(variables=(Y, Y), hook=hook)\n",
    "ctrl.backward(order=o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe98576a",
   "metadata": {},
   "source": [
    "## **4. Registering New High Order Backward Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0b21bd",
   "metadata": {},
   "source": [
    "### **4.1 Registering new high‑order backward functions**\n",
    "\n",
    "thoad allows to extend the engine with custom higher‑order rules for PyTorch ops by registering an *extended* class mapped from a concrete `grad_fn` type.\n",
    "\n",
    "**How to access the index.**\n",
    "- The *Function Transcoder* maintains a dictionary `index: dict[type[AutogradFunction], type[ExtendedAutogradFunction]]`.\n",
    "- You can read/override it from a `Controller` via the property `controller.index`.\n",
    "- If your op is missing, add a mapping from the concrete `grad_fn` type to your extended class.\n",
    "\n",
    "**Two families you can inherit from.**\n",
    "1. `ContractiveFunction` — you *produce internal derivatives* of the operator with respect to an **indexed** output (`out_id`) and a tuple of **indexed** inputs (`inp_ids`). This is the most general path.\n",
    "2. `DirectFunction` — you *directly transform* a provided derivative into the external derivative layout. Only valid for operators whose internal derivatives are **first‑order only** (no internal higher‑order accumulation).\n",
    "\n",
    "In short: use **Contractive** when you must *compute* internal derivatives per `(out_id, inp_ids)`; use **Direct** when you only need to *relabel/route* already available 1st‑order internals into the external differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9be44d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered ops: 72\n",
      "Custom mapping installed.\n"
     ]
    }
   ],
   "source": [
    "from typing import Type\n",
    "from thoad.typing import AutogradFunction\n",
    "from thoad.differentiation import ExtendedAutogradFunction\n",
    "\n",
    "T: torch.Tensor = torch.rand(3, requires_grad=True)\n",
    "G: torch.Tensor = (T + 1).sum()\n",
    "ctrl = Controller(G)\n",
    "\n",
    "# Read current map\n",
    "func_index: dict[Type[AutogradFunction], Type[ExtendedAutogradFunction]] = ctrl.index\n",
    "\n",
    "# Add/override a mapping\n",
    "ctrl.index = func_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74537e81",
   "metadata": {},
   "source": [
    "### **4.2 ContractiveFunction**\n",
    "\n",
    "`ContractiveFunction` computes **internal derivatives** indexed by `(out_id, inp_ids)` and returns them with an **Einstein‑style notation** the engine knows how to contract.\n",
    "\n",
    "**You must provide:**\n",
    "- `check_shape(out_id, inp_id, shape, indep, crossed) -> (Shape, Indep)`\n",
    "  - Validate/normalize the internal *external* derivative slot shape and independence mask; set `self._shape` as needed.\n",
    "- `_extract_context()`\n",
    "  - Pull raw data from the wrapped `grad_fn` (e.g., saved tensors/scalars).\n",
    "- `_process_context()`\n",
    "  - Turn raw context into ready‑to‑use buffers/constants.\n",
    "- `compute_internal(out_id: int, inp_id: tuple[int, ...]) -> IDData`\n",
    "  - Produce the internal derivative **tensor** and its **notation**.\n",
    "  - Convention: optionally factor logic by defining private helpers per case, e.g. `_compute_internal_0_1_2()`.\n",
    "\n",
    "**Returned data:**\n",
    "- `IDData = tuple[Tensor, Notation]`\n",
    "- `Notation` encodes (a) external indices; (b) internal indices per variable; and (c) meta row with `([shape_dims], [batch/schwarz flags])`.\n",
    "\n",
    "**Notes.**\n",
    "- `ContractiveFunction` supports multi‑output ops (`out_id`) and multi‑input selections (`inp_ids`), exactly as many entries as the *internal derivative order* you are producing.\n",
    "- Set class attribute `schwarz: bool` if you want symmetric accumulation across internal slots (when applicable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76385402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thoad.typing import IDData, Notation\n",
    "from thoad.differentiation import ContractiveFunction\n",
    "\n",
    "class MyOpXBackward(ContractiveFunction):\n",
    "    \n",
    "    schwarz: bool = True\n",
    "\n",
    "    def check_shape(\n",
    "        self,\n",
    "        out_id: int,\n",
    "        inp_id: int,\n",
    "        shape: Shape,\n",
    "        indep: Indep,\n",
    "        crossed: bool,\n",
    "        ) -> Tuple[Shape, Indep]:\n",
    "        self._shape = shape\n",
    "        return (shape, indep)\n",
    "\n",
    "    def _extract_context(self) -> None:\n",
    "        self._context = {}\n",
    "        self._process_context()\n",
    "\n",
    "    def _process_context(self) -> None:\n",
    "        assert self._context is not None\n",
    "        self._processed_context = {}\n",
    "\n",
    "    def _compute_internal_0_0(self) -> IDData:\n",
    "        assert self._shape is not None\n",
    "        derivative: torch.Tensor = torch.ones(\n",
    "            self._shape,\n",
    "            dtype=self._dtype,\n",
    "            device=self._device,\n",
    "        )\n",
    "        notation: Notation = [\n",
    "            (tuple(range(len(self._shape))), tuple(range(len(self._shape)))),\n",
    "            (tuple(range(len(self._shape))),),\n",
    "            (tuple(self._shape), tuple(False for _ in self._shape)),\n",
    "        ]\n",
    "        return (derivative, notation)\n",
    "\n",
    "    def compute_internal(self, out_id: int, inp_id: Tuple[int, ...]) -> IDData:\n",
    "        if (out_id, tuple(inp_id)) == (0, (0,)):\n",
    "            return self._compute_internal_0_0()\n",
    "        raise NotImplementedError((out_id, tuple(inp_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1885b4",
   "metadata": {},
   "source": [
    "### **4.3 DirectFunction**\n",
    "\n",
    "`DirectFunction` **does not** compute new internal tensors. Instead, it **routes** existing (first‑order) internals into the external derivative layout, hence it only works when the underlying op has **1st‑order‑only** internal derivatives.\n",
    "\n",
    "**You must provide:**\n",
    "- Maintain `self._indeps: list[Indep]` of length = number of *unique* inputs that are differentiable for the op; fill it in `check_shape`.\n",
    "- `check_shape(out_id, inp_id, shape, indep, crossed)`\n",
    "  - Validate/normalize the external slot and cache `self._shape` and the appropriate entry in `self._indeps`.\n",
    "- `_extract_context()` and `_process_context()`\n",
    "  - As in the contractive case; prepare anything you need for reshaping/validation.\n",
    "- `transform(derivative, shapes, indeps, out_id, inp_id) -> EDData`\n",
    "  - Receive a derivative **already computed elsewhere** plus shape/independence metadata for each external slot.\n",
    "  - Validate alignments (see `_check_transform`) and return the re‑packed `(derivative, shapes, indeps)` (and optionally a variable permutation if your engine expects it).\n",
    "\n",
    "**Constraints and alignment.**\n",
    "- `out_id[i]` is `None` iff `inp_id[i]` is `None`.\n",
    "- When `inp_id[i] = j`, the `indeps[i]` must equal `self._indeps[j]` (same independence pattern)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1a08be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thoad.typing import StaticEDData\n",
    "from thoad.differentiation import DirectFunction\n",
    "\n",
    "class MyPointwiseXBackward(DirectFunction):\n",
    "    schwarz: bool = True\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            grad_fn: AutogradFunction,\n",
    "            order: int,\n",
    "            dtype: torch.dtype,\n",
    "            device: torch.device,\n",
    "        ) -> None:\n",
    "        super().__init__(\n",
    "            grad_fn=grad_fn,\n",
    "            order=order,\n",
    "            dtype=dtype,\n",
    "            device=device\n",
    "        )\n",
    "        self._indeps = [None]\n",
    "\n",
    "    def check_shape(\n",
    "            self,\n",
    "            out_id: int,\n",
    "            inp_id: int,\n",
    "            shape: Shape,\n",
    "            indep: Indep,\n",
    "            crossed: bool,\n",
    "        ) -> Tuple[Shape, Indep]:\n",
    "        self._shape = shape\n",
    "        self._indeps[0] = indep\n",
    "        return (shape, indep)\n",
    "\n",
    "    def _extract_context(self) -> None:\n",
    "        self._context = {}\n",
    "        self._process_context()\n",
    "\n",
    "    def _process_context(self) -> None:\n",
    "        assert self._context is not None\n",
    "        self._processed_context = {}\n",
    "\n",
    "    def _transform_0_0(\n",
    "            self,\n",
    "            derivative: torch.Tensor,\n",
    "            shapes: Tuple[Shape, ...],\n",
    "            indeps: Tuple[Indep, ...],\n",
    "            variables: Tuple[int, ...],\n",
    "        ) -> StaticEDData:\n",
    "        # TODO: Transform derivative differentiations\n",
    "        return (derivative, shapes, indeps)\n",
    "\n",
    "    def transform(\n",
    "            self,\n",
    "            derivative: torch.Tensor,\n",
    "            shapes: Tuple[Shape, ...],\n",
    "            indeps: Tuple[Indep, ...],\n",
    "            out_id: Tuple[Union[None, int], ...],\n",
    "            inp_id: Tuple[Union[None, int], ...],\n",
    "        ) -> StaticEDData:\n",
    "        if bool(getattr(config, \"DEBUG\", False)):\n",
    "            self._check_transform(\n",
    "                derivative=derivative,\n",
    "                shapes=shapes,\n",
    "                indeps=indeps,\n",
    "                out_id=out_id,\n",
    "                inp_id=inp_id,\n",
    "            )\n",
    "        assert all(oo in (None, 0) for oo in out_id)\n",
    "        assert all(ii in (None, 0) for ii in inp_id)\n",
    "        variables = tuple(i for i, ii in enumerate(inp_id) if ii == 0)\n",
    "        return self._transform_0_0(\n",
    "            derivative=derivative,\n",
    "            shapes=shapes,\n",
    "            indeps=indeps,\n",
    "            variables=variables,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thoad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
