{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6af656a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import psutil\n",
    "from typing import Tuple, Union\n",
    "from timeit import timeit\n",
    "from warnings import warn\n",
    "\n",
    "# PyTorch dependencies\n",
    "import torch\n",
    "import torch.backends.opt_einsum as opt_einsum\n",
    "from torch import Tensor\n",
    "\n",
    "# Internal dependencies\n",
    "from thoad import config\n",
    "from thoad import backward, Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499ed816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# control size of tensors\n",
    "TENSOR_SCALE: Union[int, float] = 1\n",
    "REPEAT_SCALE: Union[int, float] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "041bffd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system           Windows 11 10.0.26100\n"
     ]
    }
   ],
   "source": [
    "sys: platform.uname_result = platform.uname()\n",
    "print(f\"system           {sys.system} {sys.release} {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cb928f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device     cpu -> AMD64 Family 23 Model 160 Stepping 0, AuthenticAMD\n",
      "physical cores   4\n",
      "logical cores    8\n"
     ]
    }
   ],
   "source": [
    "dev: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if dev.type == 'cuda':\n",
    "    idx = dev.index if dev.index is not None else 0\n",
    "    props: \"_CudaDeviceProperties\" = torch.cuda.get_device_properties(idx)\n",
    "    name: str = props.name\n",
    "    total_mem_gb: float = props.total_memory / (1024**3)\n",
    "    print(f\"using device     {dev} -> {name}\")\n",
    "    print(f\"device memory    {total_mem_gb:.1f} GB)\")\n",
    "else:\n",
    "    cpu_name: str = platform.processor() or \"CPU\"\n",
    "    print(f\"using device     {dev} -> {cpu_name}\")\n",
    "    print(f\"physical cores   {psutil.cpu_count(logical=False)}\")\n",
    "    print(f\"logical cores    {psutil.cpu_count(logical=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b03156c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\migue\\AppData\\Local\\Temp\\ipykernel_18292\\3768752012.py:6: UserWarning: opt_einsum backend is not available. For better performance, install and enable opt_einsum.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "if opt_einsum.is_available():\n",
    "    opt_einsum.enabled = True\n",
    "    opt_einsum.strategy = \"greedy\"\n",
    "    print(\"opt_einsum backend enabled\")\n",
    "else:\n",
    "    warn(\n",
    "        \"opt_einsum backend is not available. \"\n",
    "        \"For better performance, install and enable opt_einsum.\",\n",
    "        UserWarning\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c50a6d",
   "metadata": {},
   "source": [
    "definition of MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f784376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foward_pass(X: Tensor, *params) -> Tensor:\n",
    "    T: Tensor = X\n",
    "    for i, P in enumerate(params):\n",
    "        last_step: bool = i == (len(params) - 1)\n",
    "        T = T @ P\n",
    "        T = torch.softmax(T, dim=1) if last_step else torch.relu(T)\n",
    "    return T.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edac14b",
   "metadata": {},
   "source": [
    "definition of helper function to meassure differentiation times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74e7e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_differentiation(\n",
    "        reps: int,\n",
    "        param_grad: bool,\n",
    "        keep_batch: bool,\n",
    "        keep_schwarz: bool,\n",
    "        order: int,\n",
    "        X: Tensor,\n",
    "        *params,\n",
    "    ) -> float:\n",
    "    X.requires_grad_(True)\n",
    "    params: list[Tensor] = [P.requires_grad_(param_grad) for P in params]\n",
    "    def _foward_and_backward() -> None:\n",
    "        T: Tensor = foward_pass(X, *params)\n",
    "        ctrl: Controller = backward(\n",
    "            tensor=T,\n",
    "            order=order,\n",
    "            crossings=param_grad,\n",
    "            keep_batch=keep_batch,\n",
    "            keep_schwarz=keep_schwarz,\n",
    "        )\n",
    "        ctrl.clear()\n",
    "        return None\n",
    "    time: float = timeit(\n",
    "        lambda: _foward_and_backward(),\n",
    "        number=reps,\n",
    "    )\n",
    "    return time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0765e6bd",
   "metadata": {},
   "source": [
    "## **Benchmark optimizations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3c3c93",
   "metadata": {},
   "source": [
    "computational cost w.r.t. **batch size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ff9de65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ORDER 1\n",
      "batch size: 010 -> baseline: 0.0302  optimized: 0.0226\n",
      "batch size: 020 -> baseline: 0.0204  optimized: 0.0182\n",
      "batch size: 030 -> baseline: 0.0167  optimized: 0.0198\n",
      "batch size: 040 -> baseline: 0.0167  optimized: 0.0191\n",
      "batch size: 050 -> baseline: 0.0185  optimized: 0.0205\n",
      "batch size: 060 -> baseline: 0.0183  optimized: 0.0168\n",
      "batch size: 070 -> baseline: 0.0185  optimized: 0.0182\n",
      "batch size: 080 -> baseline: 0.0197  optimized: 0.0191\n",
      "\n",
      "ORDER 2\n",
      "batch size: 010 -> baseline: 0.0636  optimized: 0.0593\n",
      "batch size: 020 -> baseline: 0.0591  optimized: 0.0556\n",
      "batch size: 030 -> baseline: 0.0596  optimized: 0.0629\n",
      "batch size: 040 -> baseline: 0.0529  optimized: 0.0582\n",
      "batch size: 050 -> baseline: 0.0544  optimized: 0.0552\n",
      "batch size: 060 -> baseline: 0.0658  optimized: 0.0574\n",
      "batch size: 070 -> baseline: 0.0744  optimized: 0.0649\n",
      "batch size: 080 -> baseline: 0.0700  optimized: 0.0618\n",
      "\n",
      "ORDER 3\n",
      "batch size: 010 -> baseline: 0.2732  optimized: 0.2962\n",
      "batch size: 020 -> baseline: 0.3937  optimized: 0.3205\n",
      "batch size: 030 -> baseline: 0.2666  optimized: 0.2754\n",
      "batch size: 040 -> baseline: 0.2775  optimized: 0.2646\n",
      "batch size: 050 -> baseline: 0.2849  optimized: 0.2660\n",
      "batch size: 060 -> baseline: 0.2781  optimized: 0.2804\n",
      "batch size: 070 -> baseline: 0.3090  optimized: 0.2689\n",
      "batch size: 080 -> baseline: 0.3621  optimized: 0.2646\n"
     ]
    }
   ],
   "source": [
    "for o in [1, 2, 3]:\n",
    "    print(f\"\\nORDER {o}\")\n",
    "    for batch_size in [10, 20, 30, 40, 50, 60, 70, 80]:\n",
    "        param_size: int = int(10 * (1 / o) * TENSOR_SCALE)\n",
    "        x_shape: Tuple[int, int] = (batch_size, param_size)\n",
    "        p_shape: Tuple[int, int] = (param_size, param_size)\n",
    "\n",
    "        X: Tensor = torch.rand(size=x_shape, device=dev)\n",
    "        params: list[Tensor] = [torch.rand(size=p_shape, device=dev) for _ in range(3)]\n",
    "\n",
    "        config.SCHWARZ_OPTIMIZATION = False\n",
    "        reps: int = int(200 * (1/batch_size) * REPEAT_SCALE)\n",
    "        \n",
    "        config.BATCH_OPTIMIZATION = False\n",
    "        regular_time: float = time_differentiation(\n",
    "            reps, True, True, False, o, X, *params\n",
    "        )\n",
    "        config.BATCH_OPTIMIZATION = True\n",
    "        optimized_time: float = time_differentiation(\n",
    "            reps, True, True, False, o, X, *params\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"batch size: {batch_size:03d} -> \"\n",
    "            f\"baseline: {regular_time / reps:.4f}  \"\n",
    "            f\"optimized: {optimized_time / reps:.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ecc39d",
   "metadata": {},
   "source": [
    "computational cost w.r.t. **depth ~ number of terminal nodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d34b4843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ORDER 1\n",
      "graph depth: 02 -> baseline: 0.0185  optimized: 0.0146\n",
      "graph depth: 03 -> baseline: 0.0212  optimized: 0.0189\n",
      "graph depth: 04 -> baseline: 0.0254  optimized: 0.0286\n",
      "graph depth: 05 -> baseline: 0.0315  optimized: 0.0298\n",
      "graph depth: 06 -> baseline: 0.0386  optimized: 0.0417\n",
      "\n",
      "ORDER 2\n",
      "graph depth: 02 -> baseline: 0.0329  optimized: 0.0297\n",
      "graph depth: 03 -> baseline: 0.0919  optimized: 0.0509\n",
      "graph depth: 04 -> baseline: 0.0987  optimized: 0.0801\n",
      "graph depth: 05 -> baseline: 0.1432  optimized: 0.0914\n",
      "graph depth: 06 -> baseline: 0.1742  optimized: 0.1564\n",
      "\n",
      "ORDER 3\n",
      "graph depth: 02 -> baseline: 0.1404  optimized: 0.0668\n",
      "graph depth: 03 -> baseline: 0.2618  optimized: 0.1318\n",
      "graph depth: 04 -> baseline: 0.5782  optimized: 0.2635\n",
      "graph depth: 05 -> baseline: 1.0093  optimized: 0.4209\n",
      "graph depth: 06 -> baseline: 1.8741  optimized: 0.7664\n"
     ]
    }
   ],
   "source": [
    "for o in [1, 2, 3]:\n",
    "    print(f\"\\nORDER {o}\")\n",
    "    for depth in range(2, 7):\n",
    "        batch_size: int = 10\n",
    "        param_size: int = int(10 * (1 / o) * TENSOR_SCALE)\n",
    "        x_shape: Tuple[int, int] = (batch_size, param_size)\n",
    "        p_shape: Tuple[int, int] = (param_size, param_size)\n",
    "\n",
    "        X: Tensor = torch.rand(size=x_shape, device=dev)\n",
    "        params: list[Tensor] = [torch.rand(size=p_shape, device=dev) for _ in range(depth)]\n",
    "\n",
    "        config.BATCH_OPTIMIZATION = False\n",
    "        reps: int = int(200 * (1/depth) * REPEAT_SCALE)\n",
    "\n",
    "        config.SCHWARZ_OPTIMIZATION = False\n",
    "        baseline_time: float = time_differentiation(\n",
    "            reps, True, False, True, o, X, *params\n",
    "        )\n",
    "        config.SCHWARZ_OPTIMIZATION = True\n",
    "        optimized_time: float = time_differentiation(\n",
    "            reps, True, False, True, o, X, *params\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"graph depth: {depth:02d} -> \"\n",
    "            f\"baseline: {baseline_time / reps:.4f}  \"\n",
    "            f\"optimized: {optimized_time / reps:.4f}\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thoad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
