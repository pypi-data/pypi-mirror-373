{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a812862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import psutil\n",
    "from typing import Tuple, Union\n",
    "from timeit import timeit\n",
    "from warnings import warn\n",
    "\n",
    "# PyTorch dependencies\n",
    "import torch\n",
    "import torch.backends.opt_einsum as opt_einsum\n",
    "from torch import Tensor\n",
    "\n",
    "# Internal dependencies\n",
    "from thoad import backward, Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d2bb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# control size of tensors\n",
    "TENSOR_SCALE: Union[int, float] = 1\n",
    "REPEAT_SCALE: Union[int, float] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e5433d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system           Windows 11 10.0.26100\n"
     ]
    }
   ],
   "source": [
    "sys: platform.uname_result = platform.uname()\n",
    "print(f\"system           {sys.system} {sys.release} {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31b7adf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device     cpu -> AMD64 Family 23 Model 160 Stepping 0, AuthenticAMD\n",
      "physical cores   4\n",
      "logical cores    8\n"
     ]
    }
   ],
   "source": [
    "dev: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if dev.type == 'cuda':\n",
    "    idx = dev.index if dev.index is not None else 0\n",
    "    props: \"_CudaDeviceProperties\" = torch.cuda.get_device_properties(idx)\n",
    "    name: str = props.name\n",
    "    total_mem_gb: float = props.total_memory / (1024**3)\n",
    "    print(f\"using device     {dev} -> {name}\")\n",
    "    print(f\"device memory    {total_mem_gb:.1f} GB)\")\n",
    "else:\n",
    "    cpu_name: str = platform.processor() or \"CPU\"\n",
    "    print(f\"using device     {dev} -> {cpu_name}\")\n",
    "    print(f\"physical cores   {psutil.cpu_count(logical=False)}\")\n",
    "    print(f\"logical cores    {psutil.cpu_count(logical=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd37689e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt_einsum backend enabled\n"
     ]
    }
   ],
   "source": [
    "if opt_einsum.is_available():\n",
    "    opt_einsum.enabled = True\n",
    "    opt_einsum.strategy = \"greedy\"\n",
    "    print(\"opt_einsum backend enabled\")\n",
    "else:\n",
    "    warn(\n",
    "        \"opt_einsum backend is not available. \"\n",
    "        \"For better performance, install and enable opt_einsum.\",\n",
    "        UserWarning\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86cfdcd",
   "metadata": {},
   "source": [
    "definition of MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca234ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foward_pass(X: Tensor, *params) -> Tensor:\n",
    "    T: Tensor = X\n",
    "    for i, P in enumerate(params):\n",
    "        last_step: bool = i == (len(params) - 1)\n",
    "        T = T @ P\n",
    "        T = torch.softmax(T, dim=1) if last_step else torch.relu(T)\n",
    "    return T.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb282a20",
   "metadata": {},
   "source": [
    "## **Benchmark jacobians on full MLP**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef73331",
   "metadata": {},
   "source": [
    "definition of helper functions to meassure jacobian times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7507b5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_autograd_jacobian(param_grad: bool, reps: int, X: Tensor, *params) -> float:\n",
    "    def _fixed_forward_pass(X) -> Tensor:\n",
    "        return foward_pass(X, *params)\n",
    "    def _foward_and_backward() -> None:\n",
    "        if param_grad:\n",
    "            torch.autograd.functional.jacobian(func=foward_pass, inputs=(X, *params))\n",
    "        else:\n",
    "            torch.autograd.functional.jacobian(func=_fixed_forward_pass, inputs=X)\n",
    "        return None\n",
    "    time: float = timeit(\n",
    "        lambda: _foward_and_backward(),\n",
    "        number=reps,\n",
    "    )\n",
    "    return time\n",
    "\n",
    "def time_thoad_jacobian(param_grad: bool, reps: int, X: Tensor, *params) -> float:\n",
    "    X.requires_grad_(True)\n",
    "    params: list[Tensor] = [P.requires_grad_(param_grad) for P in params]\n",
    "    def _foward_and_backward() -> None:\n",
    "        T: Tensor = foward_pass(X, *params)\n",
    "        ctrl: Controller = backward(tensor=T, order=1, crossings=param_grad, keep_batch=True)\n",
    "        ctrl.clear()\n",
    "        return None\n",
    "    time: float = timeit(\n",
    "        lambda: _foward_and_backward(),\n",
    "        number=reps,\n",
    "    )\n",
    "    return time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c89dad1",
   "metadata": {},
   "source": [
    "**jacobian** computational cost w.r.t. **batch size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5ce3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 010 -> autograd: 0.0007  thoad: 0.0140\n",
      "batch size: 020 -> autograd: 0.0003  thoad: 0.0081\n",
      "batch size: 030 -> autograd: 0.0007  thoad: 0.0095\n",
      "batch size: 040 -> autograd: 0.0004  thoad: 0.0117\n",
      "batch size: 050 -> autograd: 0.0007  thoad: 0.0104\n",
      "batch size: 060 -> autograd: 0.0005  thoad: 0.0101\n",
      "batch size: 070 -> autograd: 0.0005  thoad: 0.0109\n",
      "batch size: 080 -> autograd: 0.0006  thoad: 0.0106\n"
     ]
    }
   ],
   "source": [
    "for batch_size in [10, 20, 30, 40, 50, 60, 70, 80]:\n",
    "    param_size: int = int(10 * TENSOR_SCALE)\n",
    "    x_shape: Tuple[int, int] = (batch_size, param_size)\n",
    "    p_shape: Tuple[int, int] = (param_size, param_size)\n",
    "\n",
    "    X: Tensor = torch.rand(size=x_shape, device=dev)\n",
    "    params: list[Tensor] = [torch.rand(size=p_shape, device=dev) for _ in range(3)]\n",
    "\n",
    "    reps: int = int(200 * (1/batch_size) * REPEAT_SCALE)\n",
    "    autograd_time: float = time_autograd_jacobian(False, reps, X, *params)\n",
    "    thoad_time: float = time_thoad_jacobian(False, reps, X, *params)\n",
    "\n",
    "    print(\n",
    "        f\"batch size: {batch_size:03d} -> \"\n",
    "        f\"autograd: {autograd_time / reps:.4f}  thoad: {thoad_time / reps:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e390c23f",
   "metadata": {},
   "source": [
    "**jacobian** computational cost w.r.t. **param size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96c6bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param size: 010 -> autograd: 0.0008  thoad: 0.0109\n",
      "param size: 020 -> autograd: 0.0005  thoad: 0.0122\n",
      "param size: 030 -> autograd: 0.0006  thoad: 0.0096\n",
      "param size: 040 -> autograd: 0.0003  thoad: 0.0091\n",
      "param size: 050 -> autograd: 0.0005  thoad: 0.0090\n",
      "param size: 060 -> autograd: 0.0005  thoad: 0.0096\n",
      "param size: 070 -> autograd: 0.0006  thoad: 0.0098\n",
      "param size: 080 -> autograd: 0.0006  thoad: 0.0136\n"
     ]
    }
   ],
   "source": [
    "for param_size in [10, 20, 30, 40, 50, 60, 70, 80]:\n",
    "    batch_size: int = int(10 * TENSOR_SCALE)\n",
    "    x_shape: Tuple[int, int] = (batch_size, param_size)\n",
    "    p_shape: Tuple[int, int] = (param_size, param_size)\n",
    "\n",
    "    X: Tensor = torch.rand(size=x_shape, device=dev)\n",
    "    params: list[Tensor] = [torch.rand(size=p_shape, device=dev) for _ in range(3)]\n",
    "\n",
    "    reps: int = int(200 * (1/param_size) * REPEAT_SCALE)\n",
    "    autograd_time: float = time_autograd_jacobian(False, reps, X, *params)\n",
    "    thoad_time: float = time_thoad_jacobian(False, reps, X, *params)\n",
    "\n",
    "    print(\n",
    "        f\"param size: {param_size:03d} -> \"\n",
    "        f\"autograd: {autograd_time / reps:.4f}  thoad: {thoad_time / reps:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5632e9",
   "metadata": {},
   "source": [
    "**jacobian** computational cost w.r.t. **graph depth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73553f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph depth: 02 -> autograd: 0.0011  thoad: 0.0100\n",
      "graph depth: 03 -> autograd: 0.0005  thoad: 0.0095\n",
      "graph depth: 04 -> autograd: 0.0005  thoad: 0.0116\n",
      "graph depth: 05 -> autograd: 0.0005  thoad: 0.0115\n",
      "graph depth: 06 -> autograd: 0.0005  thoad: 0.0132\n",
      "graph depth: 07 -> autograd: 0.0008  thoad: 0.0162\n",
      "graph depth: 08 -> autograd: 0.0010  thoad: 0.0179\n",
      "graph depth: 09 -> autograd: 0.0007  thoad: 0.0176\n",
      "graph depth: 10 -> autograd: 0.0008  thoad: 0.0190\n",
      "graph depth: 11 -> autograd: 0.0015  thoad: 0.0231\n",
      "graph depth: 12 -> autograd: 0.0009  thoad: 0.0232\n",
      "graph depth: 13 -> autograd: 0.0015  thoad: 0.0278\n",
      "graph depth: 14 -> autograd: 0.0011  thoad: 0.0288\n",
      "graph depth: 15 -> autograd: 0.0010  thoad: 0.0339\n",
      "graph depth: 16 -> autograd: 0.0011  thoad: 0.0344\n",
      "graph depth: 17 -> autograd: 0.0014  thoad: 0.0470\n",
      "graph depth: 18 -> autograd: 0.0019  thoad: 0.0540\n",
      "graph depth: 19 -> autograd: 0.0015  thoad: 0.0502\n",
      "graph depth: 20 -> autograd: 0.0015  thoad: 0.0540\n"
     ]
    }
   ],
   "source": [
    "for depth in range(2, 21):\n",
    "    batch_size: int = 60\n",
    "    param_size: int = int(20 * TENSOR_SCALE)\n",
    "    x_shape: Tuple[int, int] = (batch_size, param_size)\n",
    "    p_shape: Tuple[int, int] = (param_size, param_size)\n",
    "\n",
    "    X: Tensor = torch.rand(size=x_shape, device=dev)\n",
    "    params: list[Tensor] = [torch.rand(size=p_shape, device=dev) for _ in range(depth)]\n",
    "\n",
    "    reps: int = int(20 * (1/depth) * REPEAT_SCALE)\n",
    "    autograd_time: float = time_autograd_jacobian(False, reps, X, *params)\n",
    "    thoad_time: float = time_thoad_jacobian(False, reps, X, *params)\n",
    "\n",
    "    print(\n",
    "        f\"graph depth: {depth:02d} -> \"\n",
    "        f\"autograd: {autograd_time / reps:.4f}  thoad: {thoad_time / reps:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7506fa",
   "metadata": {},
   "source": [
    "**jacobian** computational cost w.r.t. **batch size** (param gradients included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd98b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 05 -> autograd: 0.0006  thoad: 0.0152\n",
      "batch size: 10 -> autograd: 0.0005  thoad: 0.0144\n",
      "batch size: 15 -> autograd: 0.0005  thoad: 0.0147\n",
      "batch size: 20 -> autograd: 0.0005  thoad: 0.0160\n",
      "batch size: 25 -> autograd: 0.0007  thoad: 0.0174\n",
      "batch size: 30 -> autograd: 0.0007  thoad: 0.0166\n",
      "batch size: 35 -> autograd: 0.0005  thoad: 0.0158\n",
      "batch size: 40 -> autograd: 0.0006  thoad: 0.0176\n",
      "batch size: 45 -> autograd: 0.0008  thoad: 0.0160\n",
      "batch size: 50 -> autograd: 0.0008  thoad: 0.0161\n"
     ]
    }
   ],
   "source": [
    "for batch_size in [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]:\n",
    "    param_size: int = int(10 * TENSOR_SCALE)\n",
    "    x_shape: Tuple[int, int] = (batch_size, param_size)\n",
    "    p_shape: Tuple[int, int] = (param_size, param_size)\n",
    "\n",
    "    X: Tensor = torch.rand(size=x_shape, device=dev)\n",
    "    params: list[Tensor] = [torch.rand(size=p_shape, device=dev) for _ in range(3)]\n",
    "\n",
    "    reps: int = int(200 * (1/batch_size) * REPEAT_SCALE)\n",
    "    autograd_time: float = time_autograd_jacobian(True, reps, X, *params)\n",
    "    thoad_time: float = time_thoad_jacobian(True, reps, X, *params)\n",
    "\n",
    "    print(\n",
    "        f\"batch size: {batch_size:02d} -> \"\n",
    "        f\"autograd: {autograd_time / reps:.4f}  thoad: {thoad_time / reps:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ca7b55",
   "metadata": {},
   "source": [
    "**jacobian** computational cost w.r.t. **param size** (param gradients included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5a14bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param size: 05 -> autograd: 0.0007  thoad: 0.0163\n",
      "param size: 10 -> autograd: 0.0005  thoad: 0.0175\n",
      "param size: 15 -> autograd: 0.0005  thoad: 0.0167\n",
      "param size: 20 -> autograd: 0.0006  thoad: 0.0174\n",
      "param size: 25 -> autograd: 0.0005  thoad: 0.0180\n",
      "param size: 30 -> autograd: 0.0011  thoad: 0.0217\n",
      "param size: 35 -> autograd: 0.0011  thoad: 0.0201\n",
      "param size: 40 -> autograd: 0.0005  thoad: 0.0195\n",
      "param size: 45 -> autograd: 0.0006  thoad: 0.0173\n",
      "param size: 50 -> autograd: 0.0008  thoad: 0.0167\n"
     ]
    }
   ],
   "source": [
    "for param_size in [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]:\n",
    "    batch_size: int = int(10 * TENSOR_SCALE)\n",
    "    x_shape: Tuple[int, int] = (batch_size, param_size)\n",
    "    p_shape: Tuple[int, int] = (param_size, param_size)\n",
    "\n",
    "    X: Tensor = torch.rand(size=x_shape, device=dev)\n",
    "    params: list[Tensor] = [torch.rand(size=p_shape, device=dev) for _ in range(3)]\n",
    "\n",
    "    reps: int = int(200 * (1/param_size) * REPEAT_SCALE)\n",
    "    autograd_time: float = time_autograd_jacobian(True, reps, X, *params)\n",
    "    thoad_time: float = time_thoad_jacobian(True, reps, X, *params)\n",
    "\n",
    "    print(\n",
    "        f\"param size: {param_size:02d} -> \"\n",
    "        f\"autograd: {autograd_time / reps:.4f}  thoad: {thoad_time / reps:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e317ec3",
   "metadata": {},
   "source": [
    "**jacobian** computational cost w.r.t. **graph depth** (param gradients included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c69b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph depth: 02 -> autograd: 0.0012  thoad: 0.0139\n",
      "graph depth: 03 -> autograd: 0.0005  thoad: 0.0174\n",
      "graph depth: 04 -> autograd: 0.0007  thoad: 0.0213\n",
      "graph depth: 05 -> autograd: 0.0006  thoad: 0.0237\n",
      "graph depth: 06 -> autograd: 0.0012  thoad: 0.0345\n"
     ]
    }
   ],
   "source": [
    "for depth in range(2, 7):\n",
    "    batch_size: int = 10\n",
    "    param_size: int = int(10 * TENSOR_SCALE)\n",
    "    x_shape: Tuple[int, int] = (batch_size, param_size)\n",
    "    p_shape: Tuple[int, int] = (param_size, param_size)\n",
    "\n",
    "    X: Tensor = torch.rand(size=x_shape, device=dev)\n",
    "    params: list[Tensor] = [torch.rand(size=p_shape, device=dev) for _ in range(depth)]\n",
    "\n",
    "    reps: int = int(200 * (1/depth) * REPEAT_SCALE)\n",
    "    autograd_time: float = time_autograd_jacobian(True, reps, X, *params)\n",
    "    thoad_time: float = time_thoad_jacobian(True, reps, X, *params)\n",
    "\n",
    "    print(\n",
    "        f\"graph depth: {depth:02d} -> \"\n",
    "        f\"autograd: {autograd_time / reps:.4f}  thoad: {thoad_time / reps:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852e55bf",
   "metadata": {},
   "source": [
    "## **Benchmark hessians on full MLP**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2cce7a",
   "metadata": {},
   "source": [
    "definition of helper functions to meassure hessian times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f343184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_autograd_hessian(param_grad: bool, reps: int, X: Tensor, *params) -> float:\n",
    "    def _fixed_forward_pass(X) -> Tensor:\n",
    "        return foward_pass(X, *params)\n",
    "    def _foward_and_backward() -> None:\n",
    "        if param_grad:\n",
    "            torch.autograd.functional.hessian(func=foward_pass, inputs=(X, *params))\n",
    "        else:\n",
    "            torch.autograd.functional.hessian(func=_fixed_forward_pass, inputs=X)\n",
    "        return None\n",
    "    time: float = timeit(\n",
    "        lambda: _foward_and_backward(),\n",
    "        number=reps,\n",
    "    )\n",
    "    return time\n",
    "\n",
    "def time_thoad_hessian(param_grad: bool, reps: int, X: Tensor, *params) -> float:\n",
    "    X.requires_grad_(True)\n",
    "    params: list[Tensor] = [P.requires_grad_(param_grad) for P in params]\n",
    "    def _foward_and_backward() -> None:\n",
    "        T: Tensor = foward_pass(X, *params)\n",
    "        ctrl: Controller = backward(tensor=T, order=2, crossings=param_grad, keep_batch=True)\n",
    "        ctrl.clear()\n",
    "        return None\n",
    "    time: float = timeit(\n",
    "        lambda: _foward_and_backward(),\n",
    "        number=reps,\n",
    "    )\n",
    "    return time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af2ecec",
   "metadata": {},
   "source": [
    "**hessian** computational cost w.r.t. **batch size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74561cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 010 -> autograd: 0.0322  thoad: 0.0179\n",
      "batch size: 020 -> autograd: 0.0774  thoad: 0.0170\n",
      "batch size: 030 -> autograd: 0.1125  thoad: 0.0174\n",
      "batch size: 040 -> autograd: 0.1673  thoad: 0.0196\n",
      "batch size: 050 -> autograd: 0.1987  thoad: 0.0188\n",
      "batch size: 060 -> autograd: 0.2464  thoad: 0.0178\n",
      "batch size: 070 -> autograd: 0.3157  thoad: 0.0175\n",
      "batch size: 080 -> autograd: 0.3139  thoad: 0.0221\n"
     ]
    }
   ],
   "source": [
    "for batch_size in [10, 20, 30, 40, 50, 60, 70, 80]:\n",
    "    param_size: int = int(10 * TENSOR_SCALE)\n",
    "    x_shape: Tuple[int, int] = (batch_size, param_size)\n",
    "    p_shape: Tuple[int, int] = (param_size, param_size)\n",
    "\n",
    "    X: Tensor = torch.rand(size=x_shape, device=dev)\n",
    "    params: list[Tensor] = [torch.rand(size=p_shape, device=dev) for _ in range(3)]\n",
    "\n",
    "    reps: int = int(200 * (1/batch_size) * REPEAT_SCALE)\n",
    "    autograd_time: float = time_autograd_hessian(False, reps, X, *params)\n",
    "    thoad_time: float = time_thoad_hessian(False, reps, X, *params)\n",
    "\n",
    "    print(\n",
    "        f\"batch size: {batch_size:03d} -> \"\n",
    "        f\"autograd: {autograd_time / reps:.4f}  thoad: {thoad_time / reps:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab00182",
   "metadata": {},
   "source": [
    "**hessian** computational cost w.r.t. **param size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd659c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param size: 10 -> autograd: 0.0325  thoad: 0.0218\n",
      "param size: 20 -> autograd: 0.0819  thoad: 0.0182\n",
      "param size: 30 -> autograd: 0.1147  thoad: 0.0186\n",
      "param size: 40 -> autograd: 0.1766  thoad: 0.0236\n",
      "param size: 50 -> autograd: 0.2110  thoad: 0.0253\n",
      "param size: 60 -> autograd: 0.2792  thoad: 0.0323\n",
      "param size: 70 -> autograd: 0.2708  thoad: 0.0348\n",
      "param size: 80 -> autograd: 0.3115  thoad: 0.0454\n"
     ]
    }
   ],
   "source": [
    "for param_size in [10, 20, 30, 40, 50, 60, 70, 80]:\n",
    "    batch_size: int = int(10 * TENSOR_SCALE)\n",
    "    x_shape: Tuple[int, int] = (batch_size, param_size)\n",
    "    p_shape: Tuple[int, int] = (param_size, param_size)\n",
    "\n",
    "    X: Tensor = torch.rand(size=x_shape, device=dev)\n",
    "    params: list[Tensor] = [torch.rand(size=p_shape, device=dev) for _ in range(3)]\n",
    "\n",
    "    reps: int = int(200 * (1/param_size) * REPEAT_SCALE)\n",
    "    autograd_time: float = time_autograd_hessian(False, reps, X, *params)\n",
    "    thoad_time: float = time_thoad_hessian(False, reps, X, *params)\n",
    "\n",
    "    print(\n",
    "        f\"param size: {param_size:02d} -> \"\n",
    "        f\"autograd: {autograd_time / reps:.4f}  thoad: {thoad_time / reps:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541c6a79",
   "metadata": {},
   "source": [
    "**hessian** computational cost w.r.t. **graph depth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30647e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph depth: 02 -> autograd: 0.0754  thoad: 0.0139\n",
      "graph depth: 03 -> autograd: 0.0794  thoad: 0.0335\n",
      "graph depth: 04 -> autograd: 0.0941  thoad: 0.0253\n",
      "graph depth: 05 -> autograd: 0.1173  thoad: 0.0385\n",
      "graph depth: 06 -> autograd: 0.1258  thoad: 0.0344\n",
      "graph depth: 07 -> autograd: 0.1352  thoad: 0.0338\n",
      "graph depth: 08 -> autograd: 0.1468  thoad: 0.0407\n",
      "graph depth: 09 -> autograd: 0.1652  thoad: 0.0453\n",
      "graph depth: 10 -> autograd: 0.1897  thoad: 0.0622\n"
     ]
    }
   ],
   "source": [
    "for depth in range(2, 11):\n",
    "    input_size: int = 20\n",
    "    param_size: int = int(20 * TENSOR_SCALE)\n",
    "    x_shape: Tuple[int, int] = (batch_size, param_size)\n",
    "    p_shape: Tuple[int, int] = (param_size, param_size)\n",
    "\n",
    "    X: Tensor = torch.rand(size=x_shape, device=dev)\n",
    "    params: list[Tensor] = [torch.rand(size=p_shape, device=dev) for _ in range(depth)]\n",
    "\n",
    "    reps: int = int(20 * (1/depth) * REPEAT_SCALE)\n",
    "    autograd_time: float = time_autograd_hessian(False, reps, X, *params)\n",
    "    thoad_time: float = time_thoad_hessian(False, reps, X, *params)\n",
    "\n",
    "    print(\n",
    "        f\"graph depth: {depth:02d} -> \"\n",
    "        f\"autograd: {autograd_time / reps:.4f}  thoad: {thoad_time / reps:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed1e513",
   "metadata": {},
   "source": [
    "**hessian** computational cost w.r.t. **batch size** (param gradients included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af5e6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 05 -> autograd: 0.1456  thoad: 0.0527\n",
      "batch size: 10 -> autograd: 0.1735  thoad: 0.0516\n",
      "batch size: 15 -> autograd: 0.1929  thoad: 0.0550\n",
      "batch size: 20 -> autograd: 0.2536  thoad: 0.0519\n",
      "batch size: 25 -> autograd: 0.2782  thoad: 0.0517\n",
      "batch size: 30 -> autograd: 0.3074  thoad: 0.0572\n",
      "batch size: 35 -> autograd: 0.3403  thoad: 0.0564\n",
      "batch size: 40 -> autograd: 0.4598  thoad: 0.0594\n",
      "batch size: 45 -> autograd: 0.4883  thoad: 0.0590\n",
      "batch size: 50 -> autograd: 0.4837  thoad: 0.0551\n"
     ]
    }
   ],
   "source": [
    "for batch_size in [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]:\n",
    "    param_size: int = int(10 * TENSOR_SCALE)\n",
    "    x_shape: Tuple[int, int] = (batch_size, param_size)\n",
    "    p_shape: Tuple[int, int] = (param_size, param_size)\n",
    "\n",
    "    X: Tensor = torch.rand(size=x_shape, device=dev)\n",
    "    params: list[Tensor] = [torch.rand(size=p_shape, device=dev) for _ in range(3)]\n",
    "\n",
    "    reps: int = int(20 * (1/batch_size) * REPEAT_SCALE)\n",
    "    autograd_time: float = time_autograd_hessian(True, reps, X, *params)\n",
    "    thoad_time: float = time_thoad_hessian(True, reps, X, *params)\n",
    "\n",
    "    print(\n",
    "        f\"batch size: {batch_size:02d} -> \"\n",
    "        f\"autograd: {autograd_time / reps:.4f}  thoad: {thoad_time / reps:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106b2c61",
   "metadata": {},
   "source": [
    "**hessian** computational cost w.r.t. **param size** (param gradients included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61de2213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param size: 05 -> autograd: 0.0545  thoad: 0.0586\n",
      "param size: 10 -> autograd: 0.1825  thoad: 0.0529\n",
      "param size: 15 -> autograd: 0.4287  thoad: 0.0532\n",
      "param size: 20 -> autograd: 0.6604  thoad: 0.0658\n",
      "param size: 25 -> autograd: 0.9822  thoad: 0.0618\n",
      "param size: 30 -> autograd: 1.2951  thoad: 0.1134\n",
      "param size: 35 -> autograd: 2.0411  thoad: 0.1073\n",
      "param size: 40 -> autograd: 3.6852  thoad: 0.1747\n",
      "param size: 45 -> autograd: 4.2953  thoad: 0.2358\n",
      "param size: 50 -> autograd: 4.9012  thoad: 0.3049\n"
     ]
    }
   ],
   "source": [
    "for param_size in [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]:\n",
    "    batch_size: int = int(10 * TENSOR_SCALE)\n",
    "    x_shape: Tuple[int, int] = (batch_size, param_size)\n",
    "    p_shape: Tuple[int, int] = (param_size, param_size)\n",
    "\n",
    "    X: Tensor = torch.rand(size=x_shape, device=dev)\n",
    "    params: list[Tensor] = [torch.rand(size=p_shape, device=dev) for _ in range(3)]\n",
    "\n",
    "    reps: int = int(200 * (1/param_size) * REPEAT_SCALE)\n",
    "    autograd_time: float = time_autograd_hessian(True, reps, X, *params)\n",
    "    thoad_time: float = time_thoad_hessian(True, reps, X, *params)\n",
    "\n",
    "    print(\n",
    "        f\"param size: {param_size:02d} -> \"\n",
    "        f\"autograd: {autograd_time / reps:.4f}  thoad: {thoad_time / reps:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1302794c",
   "metadata": {},
   "source": [
    "**hessian** computational cost w.r.t. **graph depth** (param gradients included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813bf7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph depth: 02 -> autograd: 0.1375  thoad: 0.0324\n",
      "graph depth: 03 -> autograd: 0.1757  thoad: 0.0628\n",
      "graph depth: 04 -> autograd: 0.2567  thoad: 0.0781\n",
      "graph depth: 05 -> autograd: 0.3848  thoad: 0.1105\n",
      "graph depth: 06 -> autograd: 0.5297  thoad: 0.1862\n",
      "graph depth: 07 -> autograd: 0.5988  thoad: 0.2180\n",
      "graph depth: 08 -> autograd: 0.7029  thoad: 0.2517\n"
     ]
    }
   ],
   "source": [
    "for depth in [2, 3, 4, 5, 6, 7, 8]:\n",
    "    batch_size: int = 10\n",
    "    param_size: int = int(10 * TENSOR_SCALE)\n",
    "    x_shape: Tuple[int, int] = (batch_size, param_size)\n",
    "    p_shape: Tuple[int, int] = (param_size, param_size)\n",
    "\n",
    "    X: Tensor = torch.rand(size=x_shape, device=dev)\n",
    "    params: list[Tensor] = [torch.rand(size=p_shape, device=dev) for _ in range(depth)]\n",
    "\n",
    "    reps: int = int(20 * (1/depth) * REPEAT_SCALE)\n",
    "    autograd_time: float = time_autograd_hessian(True, reps, X, *params)\n",
    "    thoad_time: float = time_thoad_hessian(True, reps, X, *params)\n",
    "\n",
    "    print(\n",
    "        f\"graph depth: {depth:02d} -> \"\n",
    "        f\"autograd: {autograd_time / reps:.4f}  thoad: {thoad_time / reps:.4f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
