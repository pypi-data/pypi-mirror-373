Metadata-Version: 2.4
Name: fastdatasets-llm
Version: 0.1.3
Summary: Generate high-quality LLM training datasets from documents with distillation and augmentation.
Author: FastDatasets Authors
License: Apache-2.0
Project-URL: Homepage, https://github.com/ZhuLinsen/FastDatasets
Project-URL: Repository, https://github.com/ZhuLinsen/FastDatasets
Project-URL: Issues, https://github.com/ZhuLinsen/FastDatasets/issues
Project-URL: Demo, https://huggingface.co/spaces/mumu157/FastDatasets
Keywords: llm,dataset,alpaca,sharegpt,distillation,sft
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3 :: Only
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Operating System :: OS Independent
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: pydantic<3,>=2.7.0
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: loguru>=0.7.0
Requires-Dist: openai>=1.0.0
Requires-Dist: rich>=13.0.0
Requires-Dist: tqdm>=4.65.0
Requires-Dist: httpx>=0.24.0
Provides-Extra: web
Requires-Dist: fastapi>=0.111; extra == "web"
Requires-Dist: uvicorn>=0.30; extra == "web"
Requires-Dist: gradio==4.44.0; extra == "web"
Provides-Extra: doc
Requires-Dist: textract-py3>=2.1.1; extra == "doc"
Provides-Extra: all
Requires-Dist: fastdatasets[web]; extra == "all"
Requires-Dist: fastdatasets[doc]; extra == "all"
Dynamic: license-file

# FastDatasets

Generate high-quality LLM training datasets from documents. Distillation, augmentation, multi-format export.

## Install

```bash
pip install fastdatasets
# Optional extras:
# pip install 'fastdatasets[web]'   # Web UI / API
# pip install 'fastdatasets[doc]'   # Better doc parsing (textract)
# pip install 'fastdatasets[all]'   # Everything
```

## Configure LLM

Use environment variables or pass parameters directly (function args override env):

```bash
export LLM_API_KEY="sk-..."
export LLM_API_BASE="https://api.example.com/v1"
export LLM_MODEL="your-model"
```

## Quick Start (Python)

```python
from fastdatasets import generate_dataset_to_dir

dataset = generate_dataset_to_dir(
  inputs=["./docs", "./data/sample.txt"],
  output_dir="./output",
  formats=["alpaca", "sharegpt"],
  file_format="jsonl",
  chunk_size=1000,
  chunk_overlap=200,
  enable_cot=False,
  max_llm_concurrency=5,
  # api_key="sk-...", api_base="https://api.example.com/v1", model_name="your-model",
)
print(len(dataset))
```

## CLI

```bash
# Core usage
fastdatasets generate ./data -o ./output -f alpaca,sharegpt --file-format jsonl

# Override LLM just for this command
LLM_API_KEY=sk-xxx LLM_API_BASE=https://api.example.com/v1 LLM_MODEL=your-model \
  fastdatasets generate ./docs -o ./out
```

## Optional Features
- Web/API: `pip install 'fastdatasets[web]'` then run your web/app code
- Better doc parsing (PDF/DOCX): `pip install 'fastdatasets[doc]'`

## Links
- Source: https://github.com/ZhuLinsen/FastDatasets
- Demo (Spaces): https://huggingface.co/spaces/mumu157/FastDatasets
- Issues: https://github.com/ZhuLinsen/FastDatasets/issues
