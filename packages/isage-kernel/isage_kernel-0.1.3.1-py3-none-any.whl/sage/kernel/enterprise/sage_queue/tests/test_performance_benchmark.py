#!/usr/bin/env python3
"""
SAGE Memory-Mapped Queue æ€§èƒ½åŸºå‡†æµ‹è¯•
Performance benchmark test for SAGE high-performance memory-mapped queue
"""

import os
import sys
import time
import threading
import multiprocessing
import statistics
import json
from typing import List, Dict, Any, Tuple
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor

# æ·»åŠ ä¸Šçº§ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from sage.extensions.sage_queue.python.sage_queue import SageQueue, SageQueueRef, destroy_queue
    print("âœ“ æˆåŠŸå¯¼å…¥ SageQueue")
except ImportError as e:
    print(f"âœ— å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·å…ˆè¿è¡Œ ../build.sh ç¼–è¯‘Cåº“")
    sys.exit(1)


class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    def __init__(self, name: str):
        self.name = name
        self.start_time = time.time()
        self.end_time = None
        self.metrics = {}
        self.success = False
    
    def finish(self, success: bool = True, **metrics):
        self.end_time = time.time()
        self.success = success
        self.metrics.update(metrics)
    
    @property
    def duration(self):
        if self.end_time:
            return self.end_time - self.start_time
        return time.time() - self.start_time
    
    def to_dict(self):
        return {
            'name': self.name,
            'duration': self.duration,
            'success': self.success,
            'metrics': self.metrics
        }


def benchmark_throughput() -> BenchmarkResult:
    """ååé‡åŸºå‡†æµ‹è¯•"""
    result = BenchmarkResult("ååé‡æµ‹è¯•")
    
    try:
        queue_name = f"bench_throughput_{int(time.time())}"
        destroy_queue(queue_name)
        
        # æµ‹è¯•ä¸åŒæ¶ˆæ¯å¤§å°çš„ååé‡
        message_sizes = [64, 256, 1024, 4096]  # bytes
        buffer_size = 1024 * 1024  # 1MB buffer
        
        throughput_results = {}
        
        for msg_size in message_sizes:
            print(f"  æµ‹è¯• {msg_size} å­—èŠ‚æ¶ˆæ¯...")
            
            queue = SageQueue(queue_name + f"_{msg_size}", maxsize=buffer_size)
            
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            test_data = 'x' * msg_size
            message = {"size": msg_size, "data": test_data}
            
            # å†™å…¥æµ‹è¯•
            num_messages = min(10000, buffer_size // (msg_size + 100))  # é¿å…æº¢å‡º
            
            write_start = time.time()
            written = 0
            
            for i in range(num_messages):
                try:
                    message["id"] = i
                    queue.put_nowait(message)
                    written += 1
                except:
                    break
            
            write_time = time.time() - write_start
            
            # è¯»å–æµ‹è¯•
            read_start = time.time()
            read = 0
            
            while read < written:
                try:
                    queue.get_nowait()
                    read += 1
                except:
                    break
            
            read_time = time.time() - read_start
            
            # è®¡ç®—æŒ‡æ ‡
            write_throughput = written / write_time if write_time > 0 else 0
            read_throughput = read / read_time if read_time > 0 else 0
            write_bandwidth = (written * msg_size) / (1024 * 1024) / write_time if write_time > 0 else 0
            read_bandwidth = (read * msg_size) / (1024 * 1024) / read_time if read_time > 0 else 0
            
            throughput_results[msg_size] = {
                'written': written,
                'read': read,
                'write_msg_per_sec': write_throughput,
                'read_msg_per_sec': read_throughput,
                'write_mb_per_sec': write_bandwidth,
                'read_mb_per_sec': read_bandwidth
            }
            
            print(f"    å†™å…¥: {written} æ¶ˆæ¯, {write_throughput:.0f} msg/s, {write_bandwidth:.1f} MB/s")
            print(f"    è¯»å–: {read} æ¶ˆæ¯, {read_throughput:.0f} msg/s, {read_bandwidth:.1f} MB/s")
            
            queue.close()
            destroy_queue(queue_name + f"_{msg_size}")
        
        result.finish(True, throughput=throughput_results)
        
    except Exception as e:
        result.finish(False, error=str(e))
    
    return result


def benchmark_latency() -> BenchmarkResult:
    """å»¶è¿ŸåŸºå‡†æµ‹è¯•"""
    result = BenchmarkResult("å»¶è¿Ÿæµ‹è¯•")
    
    try:
        queue_name = f"bench_latency_{int(time.time())}"
        destroy_queue(queue_name)
        
        queue = SageQueue(queue_name)
        
        # æµ‹è¯•å•æ¬¡æ“ä½œå»¶è¿Ÿ
        num_samples = 1000
        message = {"id": 0, "data": "latency_test_message"}
        
        write_latencies = []
        read_latencies = []
        roundtrip_latencies = []
        
        print(f"  é‡‡é›† {num_samples} ä¸ªå»¶è¿Ÿæ ·æœ¬...")
        
        for i in range(num_samples):
            message["id"] = i
            
            # æµ‹è¯•å†™å…¥å»¶è¿Ÿ
            start = time.time()
            queue.put(message)
            write_lat = time.time() - start
            write_latencies.append(write_lat * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
            
            # æµ‹è¯•è¯»å–å»¶è¿Ÿ
            start = time.time()
            queue.get()
            read_lat = time.time() - start
            read_latencies.append(read_lat * 1000)
            
            # å¾€è¿”å»¶è¿Ÿ
            roundtrip_latencies.append((write_lat + read_lat) * 1000)
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        def calc_stats(data):
            return {
                'mean': statistics.mean(data),
                'median': statistics.median(data),
                'p95': sorted(data)[int(0.95 * len(data))],
                'p99': sorted(data)[int(0.99 * len(data))],
                'min': min(data),
                'max': max(data)
            }
        
        write_stats = calc_stats(write_latencies)
        read_stats = calc_stats(read_latencies)
        roundtrip_stats = calc_stats(roundtrip_latencies)
        
        print(f"  å†™å…¥å»¶è¿Ÿ (ms): å¹³å‡={write_stats['mean']:.3f}, P95={write_stats['p95']:.3f}")
        print(f"  è¯»å–å»¶è¿Ÿ (ms): å¹³å‡={read_stats['mean']:.3f}, P95={read_stats['p95']:.3f}")
        print(f"  å¾€è¿”å»¶è¿Ÿ (ms): å¹³å‡={roundtrip_stats['mean']:.3f}, P95={roundtrip_stats['p99']:.3f}")
        
        queue.close()
        destroy_queue(queue_name)
        
        result.finish(True, 
                     write_latency=write_stats,
                     read_latency=read_stats, 
                     roundtrip_latency=roundtrip_stats)
        
    except Exception as e:
        result.finish(False, error=str(e))
    
    return result


def benchmark_concurrent_access() -> BenchmarkResult:
    """å¹¶å‘è®¿é—®åŸºå‡†æµ‹è¯•"""
    result = BenchmarkResult("å¹¶å‘è®¿é—®æµ‹è¯•")
    
    try:
        queue_name = f"bench_concurrent_{int(time.time())}"
        destroy_queue(queue_name)
        
        queue = SageQueue(queue_name)  # 256KB buffer
        
        # æµ‹è¯•ä¸åŒçº¿ç¨‹æ•°çš„å¹¶å‘æ€§èƒ½
        thread_counts = [1, 2, 4, 8]
        messages_per_thread = 100
        
        concurrent_results = {}
        
        for num_threads in thread_counts:
            print(f"  æµ‹è¯• {num_threads} çº¿ç¨‹å¹¶å‘...")
            
            results_data = {'completed_ops': 0, 'total_time': 0, 'errors': 0}
            results_lock = threading.Lock()
            
            def worker(thread_id: int):
                try:
                    start_time = time.time()
                    
                    # æ¯ä¸ªçº¿ç¨‹æ‰§è¡Œputå’Œgetæ“ä½œ
                    for i in range(messages_per_thread):
                        message = {
                            'thread_id': thread_id,
                            'message_id': i,
                            'data': f'thread_{thread_id}_msg_{i}'
                        }
                        
                        # Put
                        queue.put(message, timeout=10.0)
                        
                        # Get
                        queue.get(timeout=10.0)
                    
                    end_time = time.time()
                    
                    with results_lock:
                        results_data['completed_ops'] += messages_per_thread * 2  # put + get
                        results_data['total_time'] = max(results_data['total_time'], end_time - start_time)
                        
                except Exception as e:
                    with results_lock:
                        results_data['errors'] += 1
            
            # è¿è¡Œå¹¶å‘æµ‹è¯•
            threads = []
            for i in range(num_threads):
                t = threading.Thread(target=worker, args=(i,))
                threads.append(t)
            
            overall_start = time.time()
            
            for t in threads:
                t.start()
            
            for t in threads:
                t.join(timeout=30.0)
            
            overall_end = time.time()
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            total_ops = results_data['completed_ops']
            wall_time = overall_end - overall_start
            ops_per_second = total_ops / wall_time if wall_time > 0 else 0
            
            concurrent_results[num_threads] = {
                'total_operations': total_ops,
                'wall_time': wall_time,
                'ops_per_second': ops_per_second,
                'errors': results_data['errors']
            }
            
            print(f"    æ“ä½œæ•°: {total_ops}, ç”¨æ—¶: {wall_time:.3f}s, æ€§èƒ½: {ops_per_second:.0f} ops/s")
        
        queue.close()
        destroy_queue(queue_name)
        
        result.finish(True, concurrent_performance=concurrent_results)
        
    except Exception as e:
        result.finish(False, error=str(e))
    
    return result


def benchmark_memory_efficiency() -> BenchmarkResult:
    """å†…å­˜æ•ˆç‡åŸºå‡†æµ‹è¯•"""
    result = BenchmarkResult("å†…å­˜æ•ˆç‡æµ‹è¯•")
    
    try:
        queue_name = f"bench_memory_{int(time.time())}"
        destroy_queue(queue_name)
        
        # æµ‹è¯•ä¸åŒç¼“å†²åŒºå¤§å°çš„å†…å­˜åˆ©ç”¨ç‡
        buffer_sizes = [4096, 16384, 65536, 262144]  # 4KB to 256KB
        message_size = 128  # bytes per message
        
        memory_results = {}
        
        for buffer_size in buffer_sizes:
            print(f"  æµ‹è¯• {buffer_size} å­—èŠ‚ç¼“å†²åŒº...")
            
            test_queue_name = f"{queue_name}_{buffer_size}"
            queue = SageQueue(test_queue_name, maxsize=buffer_size)
            
            test_message = {"data": "x" * message_size}
            
            # å¡«æ»¡ç¼“å†²åŒº
            messages_written = 0
            while True:
                try:
                    test_message["id"] = messages_written
                    queue.put_nowait(test_message)
                    messages_written += 1
                    
                    if messages_written > buffer_size // 50:  # å®‰å…¨é™åˆ¶
                        break
                        
                except:
                    break
            
            stats = queue.get_stats()
            
            # è®¡ç®—æ•ˆç‡æŒ‡æ ‡
            theoretical_max = buffer_size // (message_size + 20)  # ä¼°è®¡å¼€é”€
            utilization = stats['utilization']
            efficiency = messages_written / theoretical_max if theoretical_max > 0 else 0
            
            memory_results[buffer_size] = {
                'messages_stored': messages_written,
                'buffer_utilization': utilization,
                'storage_efficiency': efficiency,
                'bytes_per_message': buffer_size / messages_written if messages_written > 0 else 0
            }
            
            print(f"    å­˜å‚¨æ¶ˆæ¯: {messages_written}, åˆ©ç”¨ç‡: {utilization:.1%}, æ•ˆç‡: {efficiency:.1%}")
            
            queue.close()
            destroy_queue(test_queue_name)
        
        result.finish(True, memory_efficiency=memory_results)
        
    except Exception as e:
        result.finish(False, error=str(e))
    
    return result


def benchmark_multiprocess_worker(queue_name: str, worker_id: int, num_operations: int) -> Dict[str, Any]:
    """å¤šè¿›ç¨‹åŸºå‡†æµ‹è¯•å·¥ä½œè¿›ç¨‹"""
    try:
        queue = SageQueue(queue_name)
        
        start_time = time.time()
        completed = 0
        
        for i in range(num_operations):
            message = {
                'worker_id': worker_id,
                'operation_id': i,
                'timestamp': time.time(),
                'data': f'process_{worker_id}_op_{i}'
            }
            
            # Put message
            queue.put(message, timeout=15.0)
            
            # Get message (might not be our own)
            retrieved = queue.get(timeout=15.0)
            
            completed += 2  # put + get
        
        end_time = time.time()
        queue.close()
        
        return {
            'worker_id': worker_id,
            'completed_operations': completed,
            'duration': end_time - start_time,
            'ops_per_second': completed / (end_time - start_time) if end_time > start_time else 0,
            'success': True
        }
        
    except Exception as e:
        return {
            'worker_id': worker_id,
            'error': str(e),
            'success': False
        }


def benchmark_multiprocess() -> BenchmarkResult:
    """å¤šè¿›ç¨‹åŸºå‡†æµ‹è¯•"""
    result = BenchmarkResult("å¤šè¿›ç¨‹æµ‹è¯•")
    
    try:
        queue_name = f"bench_multiproc_{int(time.time())}"
        destroy_queue(queue_name)
        
        # åˆ›å»ºä¸»é˜Ÿåˆ—
        main_queue = SageQueue(queue_name)  # 512KB buffer
        main_queue.close()  # å…³é—­ä¸»é˜Ÿåˆ—ï¼Œè®©å­è¿›ç¨‹ä½¿ç”¨
        
        # æµ‹è¯•å‚æ•°
        num_processes = 4
        operations_per_process = 100
        
        print(f"  å¯åŠ¨ {num_processes} ä¸ªè¿›ç¨‹ï¼Œæ¯ä¸ªæ‰§è¡Œ {operations_per_process} å¯¹æ“ä½œ...")
        
        # ä½¿ç”¨ProcessPoolExecutoræ‰§è¡Œ
        with ProcessPoolExecutor(max_workers=num_processes) as executor:
            futures = []
            for worker_id in range(num_processes):
                future = executor.submit(
                    benchmark_multiprocess_worker,
                    queue_name,
                    worker_id,
                    operations_per_process
                )
                futures.append(future)
            
            # æ”¶é›†ç»“æœ
            worker_results = []
            for future in futures:
                try:
                    worker_result = future.result(timeout=60)
                    worker_results.append(worker_result)
                except Exception as e:
                    worker_results.append({
                        'error': str(e),
                        'success': False
                    })
        
        # åˆ†æç»“æœ
        successful_workers = [r for r in worker_results if r.get('success', False)]
        total_operations = sum(r['completed_operations'] for r in successful_workers)
        avg_ops_per_sec = statistics.mean([r['ops_per_second'] for r in successful_workers]) if successful_workers else 0
        
        print(f"    æˆåŠŸè¿›ç¨‹: {len(successful_workers)}/{num_processes}")
        print(f"    æ€»æ“ä½œæ•°: {total_operations}")
        print(f"    å¹³å‡æ€§èƒ½: {avg_ops_per_sec:.0f} ops/s per process")
        
        destroy_queue(queue_name)
        
        multiprocess_stats = {
            'num_processes': num_processes,
            'successful_processes': len(successful_workers),
            'total_operations': total_operations,
            'avg_ops_per_second': avg_ops_per_sec,
            'worker_results': successful_workers
        }
        
        result.finish(True, multiprocess_performance=multiprocess_stats)
        
    except Exception as e:
        result.finish(False, error=str(e))
        try:
            destroy_queue(queue_name)
        except:
            pass
    
    return result


def run_all_benchmarks():
    """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
    print("SAGE Memory-Mapped Queue æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 60)
    print()
    
    # åŸºå‡†æµ‹è¯•å‡½æ•°åˆ—è¡¨
    benchmarks = [
        benchmark_throughput,
        benchmark_latency,
        benchmark_memory_efficiency,
        benchmark_concurrent_access,
        benchmark_multiprocess,
    ]
    
    results = []
    
    for benchmark_func in benchmarks:
        print(f"è¿è¡Œ {benchmark_func.__doc__ or benchmark_func.__name__}...")
        try:
            benchmark_result = benchmark_func()
            results.append(benchmark_result)
            
            if benchmark_result.success:
                print(f"âœ“ {benchmark_result.name} å®Œæˆ ({benchmark_result.duration:.2f}s)")
            else:
                print(f"âœ— {benchmark_result.name} å¤±è´¥: {benchmark_result.metrics.get('error', 'Unknown error')}")
            
            print()
            
        except Exception as e:
            print(f"âœ— {benchmark_func.__name__} æ‰§è¡Œå¼‚å¸¸: {e}")
            print()
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    print("=" * 60)
    print("åŸºå‡†æµ‹è¯•ç»“æœæ±‡æ€»:")
    print("-" * 60)
    
    successful_tests = sum(1 for r in results if r.success)
    total_tests = len(results)
    
    print(f"æˆåŠŸæµ‹è¯•: {successful_tests}/{total_tests}")
    print(f"æ€»è€—æ—¶: {sum(r.duration for r in results):.1f}ç§’")
    print("-" * 60)
    
    for result in results:
        status = "âœ“" if result.success else "âœ—"
        print(f"{status} {result.name}: {result.duration:.2f}s")
    
    # ä¿å­˜è¯¦ç»†ç»“æœåˆ°æ–‡ä»¶
    timestamp = int(time.time())
    report_file = os.path.expanduser(f"~/benchmark_report_{timestamp}.json")
    
    # ç¡®å®šé¡¹ç›®æ ¹ç›®å½•çš„logs/sage_queue_testsæ–‡ä»¶å¤¹
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))
    logs_dir = os.path.join(project_root, 'logs', 'sage_queue_tests')
    os.makedirs(logs_dir, exist_ok=True)
    
    report_filepath = os.path.join(logs_dir, report_file)
    
    report_data = {
        'timestamp': timestamp,
        'test_suite': 'SAGE Memory-Mapped Queue Benchmark',
        'summary': {
            'total_tests': total_tests,
            'successful_tests': successful_tests,
            'total_duration': sum(r.duration for r in results)
        },
        'results': [r.to_dict() for r in results]
    }
    
    with open(report_filepath, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, indent=2, default=str)
    
    print(f"\nğŸ“Š è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_filepath}")
    
    return successful_tests == total_tests


if __name__ == "__main__":
    # è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•
    if hasattr(multiprocessing, 'set_start_method'):
        try:
            multiprocessing.set_start_method('spawn')
        except RuntimeError:
            pass
    
    success = run_all_benchmarks()
    sys.exit(0 if success else 1)
