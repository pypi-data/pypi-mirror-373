"""Module holding the Converter class, which converts from XBRL-XML to XBRL-CSV
taking as input the taxonomy object and the instance object
"""

from __future__ import annotations

import csv
import json
from pathlib import Path
from tempfile import TemporaryDirectory
from typing import Any, Dict, Union
from zipfile import ZipFile

import pandas as pd

from xbridge.modules import Module, Table
from xbridge.xml_instance import Instance

INDEX_FILE = Path(__file__).parent / "modules" / "index.json"
MAPPING_PATH = Path(__file__).parent / "modules"

if not INDEX_FILE.exists():
    raise ValueError(
        "Cannot find the index file for the modules. "
        "Please make sure that the index file and the "
        "JSON files with the mappings exist in the modules folder."
    )

with open(INDEX_FILE, "r", encoding="utf-8") as fl:
    index: Dict[str, str] = json.load(fl)


class Converter:
    """Converter different types of files into others, using the EBA
    :obj:`taxonomy <xbridge.taxonomy.Taxonomy>` and XBRL-instance. Each file is extracted and saved
    in a temporary directory.
    Then, these files are converted into JSON and again saved in a
    compressed folder such as ZIP or 7z.

    :obj:`Variables <xbridge.taxonomy.Variable>` are generated by combining the open keys
    column with the attributes found within the taxonomy
    :obj:`modules <xbridge.taxonomy.Module>`.
    Then, variable's dimension is extracted
    from it with the purpose to be used with the context
    coming from the ``XML_instance``,
    to create the :obj:`scenario <xbridge.xml_instance.Scenario>`.

    Finally, an inner join is done between the variables
    created and the values from the facts
    of the ``XML_instance`` :obj:`context <xbridge.xml_instance.Context>`.

    """

    def __init__(self, instance_path: Union[str, Path]) -> None:
        path_str = str(instance_path)
        self.instance: Instance = Instance(path_str)
        module_ref = self.instance.module_ref
        if not module_ref:
            raise ValueError("No module_ref found in the instance.")
        if module_ref not in index:
            raise ValueError(f"Module {module_ref} not found in the taxonomy index")

        module_path = Path(__file__).parent / "modules" / index[module_ref]
        self.module = Module.from_serialized(module_path)
        self._reported_tables: list[str] = []
        self._decimals_parameters: dict[str, int] = {}

    def convert(self, output_path: Union[str, Path], headers_as_datapoints: bool = False) -> Path:
        """Convert the ``XML Instance`` to a CSV file"""
        if not output_path:
            raise ValueError("Output path not provided")

        if isinstance(output_path, str):
            output_path = Path(output_path)
        if self.instance is None:
            raise ValueError("Instance not provided")

        if self.module is None:
            raise ValueError("Module of the instance file not found in the taxonomy")

        module_filind_codes = [table.filing_indicator_code for table in self.module.tables]

        filing_indicator_codes = (
            self.instance.filing_indicators if self.instance.filing_indicators else []
        )

        for filing_indicator in filing_indicator_codes:
            if filing_indicator.table not in module_filind_codes:
                raise ValueError(
                    f"Filing indicator {filing_indicator.table} not found in the module tables."
                )

        instance_path = self.instance.path
        if not isinstance(instance_path, str):
            instance_path = str(instance_path)
        instance_path_stem = Path(instance_path).stem

        temp_dir = TemporaryDirectory()
        temp_dir_path = Path(temp_dir.name)

        meta_inf_dir = temp_dir_path / instance_path_stem / "META-INF"
        report_dir = temp_dir_path / instance_path_stem / "reports"

        meta_inf_dir.mkdir(parents=True)
        report_dir.mkdir(parents=True)

        with open(meta_inf_dir / "reportPackage.json", "w", encoding="UTF-8") as fl:
            json.dump(
                {"documentInfo": {"documentType": "http://xbrl.org/PWD/2020-12-09/report-package"}},
                fl,
            )

        with open(report_dir / "report.json", "w", encoding="UTF-8") as fl:
            json.dump(
                {
                    "documentInfo": {
                        "documentType": "https://xbrl.org/CR/2021-02-03/xbrl-csv",
                        "extends": [self.module.url],
                    }
                },
                fl,
            )

        self._convert_filing_indicator(report_dir)
        with open(MAPPING_PATH / self.module.dim_dom_file_name, "r", encoding="utf-8") as fl:
            mapping_dict: Dict[str, str] = json.load(fl)
        self._convert_tables(report_dir, mapping_dict, headers_as_datapoints)
        self._convert_parameters(report_dir)

        file_name = instance_path_stem + ".zip"

        zip_file_path = output_path / file_name

        with ZipFile(zip_file_path, "w") as zip_fl:
            for file in meta_inf_dir.iterdir():
                zip_fl.write(file, arcname=f"{instance_path_stem}/META-INF/{file.name}")
            for file in report_dir.iterdir():
                zip_fl.write(file, arcname=f"{instance_path_stem}/reports/{file.name}")

        temp_dir.cleanup()

        return zip_file_path

    def _get_instance_df(self, table: Table) -> pd.DataFrame:
        """Returns the dataframe with the subset of instace facts applicable to the table"""
        if self.instance.instance_df is None:
            return pd.DataFrame(columns=["datapoint", "value"])
        instance_columns = set(self.instance.instance_df.columns)
        variable_columns = set(table.variable_columns or [])
        open_keys = set(table.open_keys)
        attributes = set(table.attributes)

        # If any open key is not in the instance, then the table cannot have
        # any datapoint
        if not open_keys.issubset(instance_columns):
            return pd.DataFrame(columns=["datapoint", "value"] + list(open_keys))

        # Determine the not relevant dims
        not_relevant_dims = (
            instance_columns
            - variable_columns
            - open_keys
            - attributes
            - {"value", "unit", "decimals"}
        )

        # Convert to list so Pandas won't complain
        needed_columns = list(
            variable_columns | open_keys | attributes | {"value", "decimals"} | not_relevant_dims
        )

        # Intersect with instance_columns as a list
        needed_columns = list(set(needed_columns).intersection(instance_columns))

        instance_df = self.instance.instance_df[needed_columns].copy()

        cols_to_drop = [
            col for col in ["unit"] if col not in attributes and col in instance_df.columns
        ]
        if cols_to_drop:
            instance_df.drop(columns=cols_to_drop, inplace=True)

        # Drop datapoints that have non-null values in not relevant dimensions
        # And drop the not relevant columns
        if not_relevant_dims:
            # Convert to list
            nrd_list = list(not_relevant_dims)
            # Create mask to filter out rows where not relevant dimensions have non-null values
            mask = instance_df[nrd_list].isnull().all(axis=1)
            instance_df = instance_df.loc[mask]
            instance_df.drop(columns=nrd_list, inplace=True)

        return instance_df

    def _variable_generator(self, table: Table) -> pd.DataFrame:
        """Returns the dataframe with the CSV file for the table

        :param table: The table we use.

        """
        instance_df = self._get_instance_df(table)
        if instance_df.empty or table.variable_df is None:
            return instance_df

        variable_columns = set(table.variable_columns or [])

        open_keys = set(table.open_keys)
        instance_columns = (
            set(self.instance.instance_df.columns)
            if self.instance.instance_df is not None
            else set()
        )

        # Do the intersection and drop from datapoints the columns and records
        datapoint_df = table.variable_df
        missing_cols = list(variable_columns - instance_columns)
        if "data_type" in missing_cols:
            missing_cols.remove("data_type")
        if missing_cols:
            mask = datapoint_df[missing_cols].isnull().all(axis=1)
            datapoint_df = datapoint_df.loc[mask]
            datapoint_df = datapoint_df.drop(columns=missing_cols)

        # Join the dataframes on the datapoint_columns
        merge_cols = list(variable_columns & instance_columns)
        table_df = pd.merge(datapoint_df, instance_df, on=merge_cols, how="inner")

        if "data_type" in table_df.columns and "decimals" in table_df.columns:
            decimals_table = table_df[["decimals", "data_type"]].drop_duplicates()
            for _, row in decimals_table.iterrows():
                if not row["data_type"] or not row["decimals"]:
                    continue

                data_type = row["data_type"][1:]
                decimals = row["decimals"]

                if data_type not in self._decimals_parameters:
                    self._decimals_parameters[data_type] = decimals
                else:
                    if decimals in {"INF", "#none"}:
                        self._decimals_parameters[data_type] = decimals
                    else:
                        if (
                            isinstance(self._decimals_parameters, int)
                            and self._decimals_parameters[data_type] < decimals
                        ):
                            self._decimals_parameters[data_type] = decimals

            drop_columns = merge_cols + ["data_type", "decimals"]
        else:
            drop_columns = merge_cols

        table_df.drop(columns=drop_columns, inplace=True)

        # Drop the datapoints that have null values in the open keys
        valid_open_keys = [key for key in open_keys if key in table_df.columns]
        if valid_open_keys:
            table_df.dropna(subset=valid_open_keys, inplace=True)

        if "unit" in table.attributes and "unit" in table_df.columns and self.instance.units:
            table_df["unit"] = table_df["unit"].map(self.instance.units, na_action="ignore")

        return table_df

    def _convert_tables(
        self,
        temp_dir_path: Path,
        mapping_dict: Dict[str, str],
        headers_as_datapoints: bool,
    ) -> None:
        for table in self.module.tables:
            ##Workaround:
            # To calculate the table code for abstract tables, we look whether the name
            # ends with a letter, and if so, we remove the last part of the code
            # Possible alternative: add metadata mapping abstract and concrete tables to
            # avoid doing this kind of corrections
            # Defining the output path and check if the table is reported

            if table.filing_indicator_code not in self._reported_tables:
                continue

            datapoints = self._variable_generator(table)

            if datapoints.empty:
                continue

            # if table.architecture == 'datapoints':

            # Cleaning up the dataframe and sorting it
            datapoints = datapoints.rename(columns={"value": "factValue"})
            # Workaround
            # The enumerated key dimensions need to have a prefix like the one
            # Defined by the EBA in the JSON files. We take them from the taxonomy
            # Because EBA is using exactly those for the JSON files.

            for open_key in table.open_keys:
                if open_key in datapoints.columns:
                    dim_name = mapping_dict.get(open_key)
                    # For open keys, there are no dim_names (they are not mapped)
                    if dim_name and not datapoints.empty:
                        datapoints[open_key] = dim_name + ":" + datapoints[open_key].astype(str)

            datapoints.sort_values(by=["datapoint"], ascending=True, inplace=True)
            output_path_table = temp_dir_path / (table.url or "table.csv")

            export_index = False

            if table.architecture == "headers" and not headers_as_datapoints:
                datapoint_column_df = pd.DataFrame(table.columns, columns=["code", "variable_id"])
                datapoint_column_df.rename(
                    columns={"variable_id": "datapoint", "code": "column_code"},
                    inplace=True,
                )
                open_keys_mapping = {k: f"c{v}" for k, v in table._open_keys_mapping.items()}
                datapoints.rename(columns=open_keys_mapping, inplace=True)
                datapoints = pd.merge(datapoint_column_df, datapoints, on="datapoint", how="inner")
                if not table.open_keys:
                    datapoints["index"] = 0
                    index = "index"
                else:
                    index = list(open_keys_mapping.values())  # type: ignore[assignment]
                    export_index = True
                datapoints = datapoints.pivot(
                    index=index, columns="column_code", values="factValue"
                )

            elif table.architecture == "headers" and headers_as_datapoints:
                datapoints["datapoint"] = "dp" + datapoints["datapoint"].astype(str)

            datapoints.to_csv(output_path_table, index=export_index)

    def _convert_filing_indicator(self, temp_dir_path: Path) -> None:
        # Workaround;
        # Developed for the EBA structure
        output_path_fi = temp_dir_path / "FilingIndicators.csv"
        if self.instance.filing_indicators is None:
            return
        filing_indicators = self.instance.filing_indicators

        with output_path_fi.open("w", newline="", encoding="utf-8") as fl:
            csv_writer = csv.writer(fl)
            csv_writer.writerow(["templateID", "reported"])
            for fil_ind in filing_indicators:
                value = "true" if fil_ind.value else "false"
                csv_writer.writerow([fil_ind.table, value])
                if fil_ind.value and fil_ind.table:
                    self._reported_tables.append(fil_ind.table)

    def _convert_parameters(self, temp_dir_path: Path) -> None:
        # Workaround;
        # Developed for the EBA structure
        output_path_parameters = temp_dir_path / "parameters.csv"
        parameters: Dict[str, Any] = {
            "entityID": self.instance.entity,
            "refPeriod": self.instance.period,
            "baseCurrency": self.instance.base_currency,
        }

        for data_type, decimals in self._decimals_parameters.items():
            parameters[data_type] = decimals

        with open(output_path_parameters, "w", newline="", encoding="utf-8") as fl:
            csv_writer = csv.writer(fl)
            csv_writer.writerow(["name", "value"])
            for k, v in parameters.items():
                csv_writer.writerow([k, v])
