{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e41e4018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-09-01 09:57:47 - parquetdb.utils.config[37][load_config] - Config file: C:\\Users\\lllang\\AppData\\Local\\parquetdb\\parquetdb\\config.yml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lllang\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR: c:\\Users\\lllang\\Desktop\\Current_Projects\\Crystal-Parquet-Database\n",
      "DATA_DIR: c:\\Users\\lllang\\Desktop\\Current_Projects\\Crystal-Parquet-Database\\data\n",
      "CURRENT_DIR: c:\\Users\\lllang\\Desktop\\Current_Projects\\Crystal-Parquet-Database\\examples\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "from parquetdb import ParquetDB\n",
    "\n",
    "from crystpqdb.loaders import get_loader\n",
    "\n",
    "\n",
    "CURRENT_DIR = Path(os.path.abspath(\".\"))\n",
    "ROOT_DIR = CURRENT_DIR.parent\n",
    "DATA_DIR = ROOT_DIR / \"data\"\n",
    "\n",
    "print(\"ROOT_DIR: {}\".format(ROOT_DIR))\n",
    "print(\"DATA_DIR: {}\".format(DATA_DIR))\n",
    "print(\"CURRENT_DIR: {}\".format(CURRENT_DIR))\n",
    "\n",
    "DB_DIR = DATA_DIR / \"crystpqdb\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836f5a5d",
   "metadata": {},
   "source": [
    "## Initialize or Download the database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f0f0d7",
   "metadata": {},
   "source": [
    "## Initialize from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60d433ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-09-01 09:25:38 - parquetdb.core.parquetdb[201][__init__] - Initializing ParquetDB with db_path: c:\\Users\\lllang\\Desktop\\Current_Projects\\Crystal-Parquet-Database\\data\\crystpqdb\n",
      "[INFO] 2025-09-01 09:25:38 - parquetdb.core.parquetdb[203][__init__] - verbose: 1\n"
     ]
    }
   ],
   "source": [
    "if DB_DIR.exists():\n",
    "    shutil.rmtree(DB_DIR)\n",
    "pqdb = ParquetDB(DB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec737d5",
   "metadata": {},
   "source": [
    "## Download or initialize from local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da8bf413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-09-01 09:54:33 - parquetdb.core.parquetdb[201][__init__] - Initializing ParquetDB with db_path: c:\\Users\\lllang\\Desktop\\Current_Projects\\Crystal-Parquet-Database\\data\\crystpqdb\n",
      "[INFO] 2025-09-01 09:54:33 - parquetdb.core.parquetdb[203][__init__] - verbose: 1\n"
     ]
    }
   ],
   "source": [
    "if not DB_DIR.exists():\n",
    "    print(\"Downloading the database...\")\n",
    "    from crystpqdb.download import download\n",
    "    download(DB_DIR)\n",
    "pqdb = ParquetDB(DB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765618e1",
   "metadata": {},
   "source": [
    "## Load the datasets into the database withe the loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6524549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    (\"alex\", \"3d\"),\n",
    "    (\"alex\", \"2d\"),\n",
    "    (\"alex\", \"1d\"),\n",
    "    (\"mp\", \"summary\"),\n",
    "    (\"materialscloud\", \"mc3d\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5a87df",
   "metadata": {},
   "source": [
    "### Alexandria3D (alex, 3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3ed61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = get_loader(\"alex\", \"3d\", data_dir=DATA_DIR)\n",
    "table = loader.run()\n",
    "pqdb.create(table, convert_to_fixed_shape=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07539711",
   "metadata": {},
   "source": [
    "### Alexandria2D (alex, 2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39b61d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = get_loader(\"alex\", \"2d\", data_dir=DATA_DIR)\n",
    "table = loader.run()\n",
    "pqdb.create(table, convert_to_fixed_shape=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace6c6c7",
   "metadata": {},
   "source": [
    "### Alexandria1D (alex, 1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e296899",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = get_loader(\"alex\", \"1d\", data_dir=DATA_DIR)\n",
    "table = loader.run()\n",
    "pqdb.create(table, convert_to_fixed_shape=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a28688d",
   "metadata": {},
   "source": [
    "### Materials Project (mp, summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b0634f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-09-01 09:04:29 - crystpqdb.loaders.base[147][download] - Directory c:\\Users\\lllang\\Desktop\\Current_Projects\\Crystal-Parquet-Database\\data\\materials_project\\summary\\raw already exists and is not empty\n",
      "Loading from c:\\Users\\lllang\\Desktop\\Current_Projects\\Crystal-Parquet-Database\\data\\materials_project\\summary\\raw into c:\\Users\\lllang\\Desktop\\Current_Projects\\Crystal-Parquet-Database\\data\\materials_project\\summary\\interim\\pqdb\n"
     ]
    }
   ],
   "source": [
    "loader = get_loader(\"mp\", \"summary\", data_dir=DATA_DIR)\n",
    "table = loader.run()\n",
    "pqdb.create(table, convert_to_fixed_shape=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8eed58",
   "metadata": {},
   "source": [
    "### Materials Cloud (materialscloud, mc3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e2f132e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-09-01 09:25:39 - crystpqdb.loaders.base[147][download] - Directory c:\\Users\\lllang\\Desktop\\Current_Projects\\Crystal-Parquet-Database\\data\\materialscloud\\mc3d\\raw already exists and is not empty\n"
     ]
    }
   ],
   "source": [
    "from crystpqdb.loaders import LoaderConfig\n",
    "\n",
    "config = LoaderConfig(ingest_from_scratch=False)\n",
    "loader = get_loader(\"materialscloud\", \"mc3d\", data_dir=DATA_DIR, config=config)\n",
    "table = loader.run()\n",
    "pqdb.create(table, convert_to_fixed_shape=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ba73bb",
   "metadata": {},
   "source": [
    "## Check the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f5e6a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the database:  (5465421, 87)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the database: \", (pqdb.n_rows,pqdb.n_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbc7258",
   "metadata": {},
   "source": [
    "### Field names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1e4bf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cart_coords                    list<element: list<element: double>>\n",
      "data.band_gap                  double                        \n",
      "data.band_gap_dir              double                        \n",
      "data.band_gap_ind              double                        \n",
      "data.dos_ef                    double                        \n",
      "data.e_electronic              double                        \n",
      "data.e_ionic                   double                        \n",
      "data.e_total                   double                        \n",
      "data.energy_above_hull         double                        \n",
      "data.energy_corrected          double                        \n",
      "data.energy_formation          double                        \n",
      "data.energy_phase_seperation   double                        \n",
      "data.energy_total              double                        \n",
      "data.energy_uncorrected        double                        \n",
      "data.g_reuss                   double                        \n",
      "data.g_voigt                   double                        \n",
      "data.g_vrh                     double                        \n",
      "data.is_gap_direct             double                        \n",
      "data.is_stable                 bool                          \n",
      "data.k_reuss                   double                        \n",
      "data.k_voigt                   double                        \n",
      "data.k_vrh                     double                        \n",
      "data.magnetic_ordering         string                        \n",
      "data.n                         double                        \n",
      "data.piezoelectric_modulus     double                        \n",
      "data.poisson_ratio             double                        \n",
      "data.stress                    list<element: list<element: double>>\n",
      "data.surface_energy_anisotropy double                        \n",
      "data.total_magnetization       double                        \n",
      "data.weighted_surface_energy   double                        \n",
      "data.weighted_work_function    double                        \n",
      "frac_coords                    list<element: list<element: double>>\n",
      "has_props.absorption           bool                          \n",
      "has_props.bandstructure        bool                          \n",
      "has_props.charge_density       bool                          \n",
      "has_props.chemenv              bool                          \n",
      "has_props.dielectric           bool                          \n",
      "has_props.dos                  bool                          \n",
      "has_props.elasticity           bool                          \n",
      "has_props.electronic_structure bool                          \n",
      "has_props.eos                  bool                          \n",
      "has_props.grain_boundaries     bool                          \n",
      "has_props.insertion_electrodes bool                          \n",
      "has_props.magnetism            bool                          \n",
      "has_props.materials            bool                          \n",
      "has_props.oxi_states           bool                          \n",
      "has_props.phonon               bool                          \n",
      "has_props.piezoelectric        bool                          \n",
      "has_props.provenance           bool                          \n",
      "has_props.substrates           bool                          \n",
      "has_props.surface_properties   bool                          \n",
      "has_props.thermo               bool                          \n",
      "has_props.xas                  bool                          \n",
      "id                             int64                         \n",
      "lattice.a                      double                        \n",
      "lattice.alpha                  double                        \n",
      "lattice.b                      double                        \n",
      "lattice.beta                   double                        \n",
      "lattice.c                      double                        \n",
      "lattice.gamma                  double                        \n",
      "lattice.matrix                 list<element: list<element: double>>\n",
      "lattice.pbc                    list<element: bool>           \n",
      "lattice.volume                 double                        \n",
      "source_database                string                        \n",
      "source_dataset                 string                        \n",
      "source_id                      string                        \n",
      "species                        list<element: string>         \n",
      "structure.@class               string                        \n",
      "structure.@module              string                        \n",
      "structure.charge               double                        \n",
      "structure.lattice.a            double                        \n",
      "structure.lattice.alpha        double                        \n",
      "structure.lattice.b            double                        \n",
      "structure.lattice.beta         double                        \n",
      "structure.lattice.c            double                        \n",
      "structure.lattice.gamma        double                        \n",
      "structure.lattice.matrix       list<element: list<element: double>>\n",
      "structure.lattice.pbc          list<element: bool>           \n",
      "structure.lattice.volume       double                        \n",
      "structure.sites                list<element: struct<abc: list<element: double>, label: string, properties: struct<charge: double, forces: list<element: double>, magmom: double, selective_dynamics: list<element: bool>, dummy_field: int16>, species: list<element: struct<element: string, occu: double>>, xyz: list<element: double>>>\n",
      "symmetry.angle_tolerance       double                        \n",
      "symmetry.crystal_system        string                        \n",
      "symmetry.number                int64                         \n",
      "symmetry.point_group           string                        \n",
      "symmetry.symbol                string                        \n",
      "symmetry.symprec               double                        \n",
      "symmetry.version               string                        \n"
     ]
    }
   ],
   "source": [
    "schema = pqdb.get_schema()\n",
    "for field in schema:\n",
    "    print(f\"{field.name:<30} {str(field.type):<30}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e9b857",
   "metadata": {},
   "source": [
    "## Distrbution of data across row groups and files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02568eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of row groups per file: [84, 1]\n",
      "\n",
      "================================================================================\n",
      "File sizes:\n",
      "crystpqdb_0.parquet: 6077.85 MB\n",
      "crystpqdb_1.parquet: 55.69 MB\n",
      "================================================================================\n",
      "Number of rows/size per row group per file:\n",
      "File 1: crystpqdb_0.parquet | 6077.85 MB\n",
      "  Row group 0: 65536 rows | 92.70 MB\n",
      "  Row group 1: 65536 rows | 101.62 MB\n",
      "  Row group 2: 65536 rows | 103.12 MB\n",
      "  Row group 3: 65536 rows | 98.98 MB\n",
      "  Row group 4: 65536 rows | 99.54 MB\n",
      "  Row group 5: 65536 rows | 85.15 MB\n",
      "  Row group 6: 65536 rows | 107.19 MB\n",
      "  Row group 7: 65536 rows | 94.16 MB\n",
      "  Row group 8: 65536 rows | 91.69 MB\n",
      "  Row group 9: 65536 rows | 103.27 MB\n",
      "  Row group 10: 65536 rows | 103.24 MB\n",
      "  Row group 11: 65536 rows | 98.97 MB\n",
      "  Row group 12: 65536 rows | 105.41 MB\n",
      "  Row group 13: 65536 rows | 96.83 MB\n",
      "  Row group 14: 65536 rows | 98.39 MB\n",
      "  Row group 15: 65536 rows | 96.53 MB\n",
      "  Row group 16: 65536 rows | 98.54 MB\n",
      "  Row group 17: 65536 rows | 95.39 MB\n",
      "  Row group 18: 65536 rows | 101.66 MB\n",
      "  Row group 19: 65536 rows | 112.19 MB\n",
      "  Row group 20: 65536 rows | 110.58 MB\n",
      "  Row group 21: 65536 rows | 115.14 MB\n",
      "  Row group 22: 65536 rows | 122.36 MB\n",
      "  Row group 23: 65536 rows | 97.13 MB\n",
      "  Row group 24: 65536 rows | 111.71 MB\n",
      "  Row group 25: 65536 rows | 106.97 MB\n",
      "  Row group 26: 65536 rows | 112.16 MB\n",
      "  Row group 27: 65536 rows | 82.76 MB\n",
      "  Row group 28: 65536 rows | 102.14 MB\n",
      "  Row group 29: 65536 rows | 90.84 MB\n",
      "  Row group 30: 65536 rows | 125.89 MB\n",
      "  Row group 31: 65536 rows | 79.87 MB\n",
      "  Row group 32: 65536 rows | 91.23 MB\n",
      "  Row group 33: 65536 rows | 99.90 MB\n",
      "  Row group 34: 65536 rows | 117.16 MB\n",
      "  Row group 35: 65536 rows | 89.60 MB\n",
      "  Row group 36: 65536 rows | 95.33 MB\n",
      "  Row group 37: 65536 rows | 96.74 MB\n",
      "  Row group 38: 65536 rows | 98.59 MB\n",
      "  Row group 39: 65536 rows | 106.45 MB\n",
      "  Row group 40: 65536 rows | 93.85 MB\n",
      "  Row group 41: 65536 rows | 107.82 MB\n",
      "  Row group 42: 65536 rows | 94.76 MB\n",
      "  Row group 43: 65536 rows | 92.41 MB\n",
      "  Row group 44: 65536 rows | 99.36 MB\n",
      "  Row group 45: 65536 rows | 98.27 MB\n",
      "  Row group 46: 65536 rows | 92.17 MB\n",
      "  Row group 47: 65536 rows | 93.84 MB\n",
      "  Row group 48: 65536 rows | 98.36 MB\n",
      "  Row group 49: 65536 rows | 96.63 MB\n",
      "  Row group 50: 65536 rows | 95.35 MB\n",
      "  Row group 51: 65536 rows | 91.59 MB\n",
      "  Row group 52: 65536 rows | 91.72 MB\n",
      "  Row group 53: 65536 rows | 123.20 MB\n",
      "  Row group 54: 65536 rows | 92.01 MB\n",
      "  Row group 55: 65536 rows | 123.03 MB\n",
      "  Row group 56: 65536 rows | 131.10 MB\n",
      "  Row group 57: 65536 rows | 103.84 MB\n",
      "  Row group 58: 65536 rows | 102.00 MB\n",
      "  Row group 59: 65536 rows | 108.78 MB\n",
      "  Row group 60: 65536 rows | 102.82 MB\n",
      "  Row group 61: 65536 rows | 101.36 MB\n",
      "  Row group 62: 65536 rows | 93.81 MB\n",
      "  Row group 63: 65536 rows | 90.41 MB\n",
      "  Row group 64: 65536 rows | 93.61 MB\n",
      "  Row group 65: 65536 rows | 98.19 MB\n",
      "  Row group 66: 65536 rows | 101.86 MB\n",
      "  Row group 67: 65536 rows | 93.31 MB\n",
      "  Row group 68: 65536 rows | 97.74 MB\n",
      "  Row group 69: 65536 rows | 98.55 MB\n",
      "  Row group 70: 65536 rows | 87.29 MB\n",
      "  Row group 71: 65536 rows | 115.86 MB\n",
      "  Row group 72: 65536 rows | 95.30 MB\n",
      "  Row group 73: 65536 rows | 101.43 MB\n",
      "  Row group 74: 65536 rows | 96.55 MB\n",
      "  Row group 75: 65536 rows | 98.10 MB\n",
      "  Row group 76: 65536 rows | 78.42 MB\n",
      "  Row group 77: 55240 rows | 81.29 MB\n",
      "  Row group 78: 52824 rows | 75.70 MB\n",
      "  Row group 79: 65536 rows | 96.97 MB\n",
      "  Row group 80: 65536 rows | 196.50 MB\n",
      "  Row group 81: 65536 rows | 206.28 MB\n",
      "  Row group 82: 65536 rows | 215.72 MB\n",
      "  Row group 83: 13971 rows | 31.35 MB\n",
      "File 2: crystpqdb_1.parquet | 55.69 MB\n",
      "  Row group 0: 34970 rows | 73.29 MB\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "n_row_groups_per_file = pqdb.get_number_of_row_groups_per_file()\n",
    "print(f\"Number of row groups per file: {n_row_groups_per_file}\\n\")\n",
    "\n",
    "file_sizes = pqdb.get_file_sizes()\n",
    "print(\"=\"*80)\n",
    "print(\"File sizes:\")\n",
    "for filename, size in file_sizes.items():\n",
    "    print(f\"{filename}: {size:.2f} MB\")\n",
    "print(\"=\"*80)\n",
    "## \n",
    "\n",
    "print(\"Number of rows/size per row group per file:\")\n",
    "n_rows_per_row_group_per_file = pqdb.get_n_rows_per_row_group_per_file()\n",
    "row_group_sizes_per_file = pqdb.get_row_group_sizes_per_file()\n",
    "for i, (filename, filesize_per_row_group) in enumerate(row_group_sizes_per_file.items()):\n",
    "    print(f\"File {i+1}: {filename} | {file_sizes[filename]:.2f} MB\")\n",
    "    n_rows_per_row_group = n_rows_per_row_group_per_file[i]\n",
    "    for row_group_index, row_group_size in filesize_per_row_group.items():\n",
    "        n_rows = n_rows_per_row_group[row_group_index]\n",
    "        print(f\"  Row group {row_group_index}: {n_rows} rows | {row_group_size:.2f} MB\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b98b3",
   "metadata": {},
   "source": [
    "> Note: This has a 6GB file. This should probably be normalized to have it be distributed across 2 GB per file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6dc716",
   "metadata": {},
   "source": [
    "# Uploading to huggingface\n",
    "\n",
    "\n",
    "Has to be executed in script to gain benefit of hf-transfer. This is due to jupyter notebook running in an event loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c619f19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [01:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcrystpqdb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdownload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m upload\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# parquet_files = list(DB_DIR.glob(\"*.parquet\"))\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# print(parquet_files)\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# for file in parquet_files:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mupload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDB_DIR\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Current_Projects\\Crystal-Parquet-Database\\crystpqdb\\download.py:24\u001b[39m, in \u001b[36mupload\u001b[39m\u001b[34m(db_path)\u001b[39m\n\u001b[32m     21\u001b[39m api = HfApi()\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m db_path.glob(\u001b[33m\"\u001b[39m\u001b[33m*.parquet\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     \u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_fileobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mREPO_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mREPO_TYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\hf_api.py:1633\u001b[39m, in \u001b[36mfuture_compatible.<locals>._inner\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1630\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_as_future(fn, \u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m   1632\u001b[39m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1633\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\hf_api.py:4673\u001b[39m, in \u001b[36mHfApi.upload_file\u001b[39m\u001b[34m(self, path_or_fileobj, path_in_repo, repo_id, token, repo_type, revision, commit_message, commit_description, create_pr, parent_commit, run_as_future)\u001b[39m\n\u001b[32m   4665\u001b[39m commit_message = (\n\u001b[32m   4666\u001b[39m     commit_message \u001b[38;5;28;01mif\u001b[39;00m commit_message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUpload \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_in_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with huggingface_hub\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4667\u001b[39m )\n\u001b[32m   4668\u001b[39m operation = CommitOperationAdd(\n\u001b[32m   4669\u001b[39m     path_or_fileobj=path_or_fileobj,\n\u001b[32m   4670\u001b[39m     path_in_repo=path_in_repo,\n\u001b[32m   4671\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m4673\u001b[39m commit_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_commit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4674\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4676\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperations\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4677\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_description\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4679\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4680\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparent_commit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparent_commit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4683\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4685\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m commit_info.pr_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4686\u001b[39m     revision = quote(_parse_revision_from_pr_url(commit_info.pr_url), safe=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\hf_api.py:1633\u001b[39m, in \u001b[36mfuture_compatible.<locals>._inner\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1630\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_as_future(fn, \u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m   1632\u001b[39m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1633\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\hf_api.py:4202\u001b[39m, in \u001b[36mHfApi.create_commit\u001b[39m\u001b[34m(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit, run_as_future)\u001b[39m\n\u001b[32m   4199\u001b[39m \u001b[38;5;66;03m# If updating twice the same file or update then delete a file in a single commit\u001b[39;00m\n\u001b[32m   4200\u001b[39m _warn_on_overwriting_operations(operations)\n\u001b[32m-> \u001b[39m\u001b[32m4202\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreupload_lfs_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4204\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditions\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43munquoted_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# first-class methods take unquoted revision\u001b[39;49;00m\n\u001b[32m   4208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfree_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# do not remove `CommitOperationAdd.path_or_fileobj` on LFS files for \"normal\" users\u001b[39;49;00m\n\u001b[32m   4211\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4213\u001b[39m files_to_copy = _fetch_files_to_copy(\n\u001b[32m   4214\u001b[39m     copies=copies,\n\u001b[32m   4215\u001b[39m     repo_type=repo_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4219\u001b[39m     endpoint=\u001b[38;5;28mself\u001b[39m.endpoint,\n\u001b[32m   4220\u001b[39m )\n\u001b[32m   4221\u001b[39m \u001b[38;5;66;03m# Remove no-op operations (files that have not changed)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\hf_api.py:4494\u001b[39m, in \u001b[36mHfApi.preupload_lfs_files\u001b[39m\u001b[34m(self, repo_id, additions, token, repo_type, revision, create_pr, num_threads, free_memory, gitignore_content)\u001b[39m\n\u001b[32m   4489\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m has_buffered_io_data:\n\u001b[32m   4490\u001b[39m             logger.warning(\n\u001b[32m   4491\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUploading files as a binary IO buffer is not supported by Xet Storage. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4492\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFalling back to HTTP upload.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4493\u001b[39m             )\n\u001b[32m-> \u001b[39m\u001b[32m4494\u001b[39m     \u001b[43m_upload_lfs_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mupload_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore [arg-type]\u001b[39;00m\n\u001b[32m   4495\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m addition \u001b[38;5;129;01min\u001b[39;00m new_lfs_additions_to_upload:\n\u001b[32m   4496\u001b[39m     addition._is_uploaded = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\_commit_api.py:450\u001b[39m, in \u001b[36m_upload_lfs_files\u001b[39m\u001b[34m(additions, repo_type, repo_id, headers, endpoint, num_threads, revision)\u001b[39m\n\u001b[32m    448\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUploading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_actions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m LFS files to the Hub using `hf_transfer`.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(filtered_actions, name=\u001b[33m\"\u001b[39m\u001b[33mhuggingface_hub.lfs_upload\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m450\u001b[39m         \u001b[43m_wrapped_lfs_upload\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(filtered_actions) == \u001b[32m1\u001b[39m:\n\u001b[32m    452\u001b[39m     logger.debug(\u001b[33m\"\u001b[39m\u001b[33mUploading 1 LFS file to the Hub\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\_commit_api.py:443\u001b[39m, in \u001b[36m_upload_lfs_files.<locals>._wrapped_lfs_upload\u001b[39m\u001b[34m(batch_action)\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    442\u001b[39m     operation = oid2addop[batch_action[\u001b[33m\"\u001b[39m\u001b[33moid\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m     \u001b[43mlfs_upload\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlfs_batch_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    445\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while uploading \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation.path_in_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m to the Hub.\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\lfs.py:246\u001b[39m, in \u001b[36mlfs_upload\u001b[39m\u001b[34m(operation, lfs_batch_action, token, headers, endpoint)\u001b[39m\n\u001b[32m    242\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m    243\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    244\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMalformed response from LFS batch endpoint: `chunk_size` should be an integer. Got \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    245\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[43m_upload_multi_part\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupload_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupload_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    248\u001b[39m     _upload_single_part(operation=operation, upload_url=upload_url)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\lfs.py:344\u001b[39m, in \u001b[36m_upload_multi_part\u001b[39m\u001b[34m(operation, header, chunk_size, upload_url)\u001b[39m\n\u001b[32m    337\u001b[39m     warnings.warn(\n\u001b[32m    338\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhf_transfer is enabled but does not support uploading from bytes or BinaryIO, falling back to regular\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m upload\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m     )\n\u001b[32m    341\u001b[39m     use_hf_transfer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    343\u001b[39m response_headers = (\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     \u001b[43m_upload_parts_hf_transfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorted_parts_urls\u001b[49m\u001b[43m=\u001b[49m\u001b[43msorted_parts_urls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_hf_transfer\n\u001b[32m    346\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m _upload_parts_iteratively(operation=operation, sorted_parts_urls=sorted_parts_urls, chunk_size=chunk_size)\n\u001b[32m    347\u001b[39m )\n\u001b[32m    349\u001b[39m \u001b[38;5;66;03m# 3. Send completion request\u001b[39;00m\n\u001b[32m    350\u001b[39m completion_res = get_session().post(\n\u001b[32m    351\u001b[39m     upload_url,\n\u001b[32m    352\u001b[39m     json=_get_completion_payload(response_headers, operation.upload_info.sha256.hex()),\n\u001b[32m    353\u001b[39m     headers=LFS_HEADERS,\n\u001b[32m    354\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\lfs.py:444\u001b[39m, in \u001b[36m_upload_parts_hf_transfer\u001b[39m\u001b[34m(operation, sorted_parts_urls, chunk_size)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(\n\u001b[32m    435\u001b[39m     unit=\u001b[33m\"\u001b[39m\u001b[33mB\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    436\u001b[39m     unit_scale=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    441\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mhuggingface_hub.lfs_upload\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    442\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[32m    443\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m         output = \u001b[43mmultipart_upload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath_or_fileobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparts_urls\u001b[49m\u001b[43m=\u001b[49m\u001b[43msorted_parts_urls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m            \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_files\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparallel_failures\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m127\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# could be removed\u001b[39;49;00m\n\u001b[32m    450\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallback\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msupports_callback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    454\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    455\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while uploading using `hf_transfer`. Consider disabling HF_HUB_ENABLE_HF_TRANSFER for\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    456\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m better error handling.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    457\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\std.py:1198\u001b[39m, in \u001b[36mtqdm.update\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m   1195\u001b[39m         \u001b[38;5;28mself\u001b[39m.n = n\n\u001b[32m   1196\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m-> \u001b[39m\u001b[32m1198\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, n=\u001b[32m1\u001b[39m):\n\u001b[32m   1199\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1200\u001b[39m \u001b[33;03m    Manually update the progress bar, useful for streams\u001b[39;00m\n\u001b[32m   1201\u001b[39m \u001b[33;03m    such as reading files.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1222\u001b[39m \u001b[33;03m        True if a `display()` was triggered.\u001b[39;00m\n\u001b[32m   1223\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1224\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.disable:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from crystpqdb.download import upload\n",
    "\n",
    "\n",
    "# parquet_files = list(DB_DIR.glob(\"*.parquet\"))\n",
    "# print(parquet_files)\n",
    "# for file in parquet_files:\n",
    "upload(DB_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
