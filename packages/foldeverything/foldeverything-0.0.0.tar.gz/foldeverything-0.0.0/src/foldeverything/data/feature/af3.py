import copy
from dataclasses import replace
import math
from typing import Dict, List, Optional, Tuple
import time
import networkx as nx
import numba
import numpy as np
import numpy.typing as npt
import pydssp
import torch
from numba import types
from rdkit.Chem import Mol
from torch import Tensor, from_numpy
from torch.nn.functional import one_hot as one_hot_torch

from foldeverything.data import const
from foldeverything.data.data import (
    MSA,
    Atom,
    Input,
    MSADeletion,
    MSAResidue,
    MSASequence,
    convert_atom_name,
    convert_ccd,
    elem_from_name,
)
from foldeverything.data.mol import (
    get_amino_acids_symmetries,
    get_chain_symmetries,
    get_ligand_symmetries,
    get_symmetries,
)
from foldeverything.data.pad import pad_dim
from foldeverything.data.parse.schema import parse_ccd_residue
from foldeverything.model.modules.utils import center_random_augmentation
from rdkit import Chem
from rdkit.Chem import AllChem
####################################################################################################
# CONSTANTS
####################################################################################################

possible_fake_atoms = []
for i in range(37):
    possible_fake_atoms.append(
        (
            const.fake_element + str(i),
            [0.0, 0.0, 0.0],
            True,
            0,
            0,
        )
    )
possible_fake_atoms = np.array(possible_fake_atoms, dtype=Atom)

####################################################################################################
# HELPERS
####################################################################################################


def one_hot_bool(tensor, num_classes=-1):
    """
    Efficient one-hot encoding that returns bool tensor instead of long tensor.

    Args:
        tensor: Input tensor of indices
        num_classes: Number of classes. If -1, inferred from tensor.max() + 1

    Returns:
        Boolean tensor with one-hot encoding
    """
    if num_classes == -1:
        num_classes = int(tensor.max().item()) + 1

    # Create output tensor with bool dtype for memory efficiency
    output_shape = list(tensor.shape) + [num_classes]
    result = torch.zeros(output_shape, dtype=torch.bool, device=tensor.device)

    # Use scatter_ for efficient assignment
    # scatter_(dim, index, src) - dim is the last dimension
    result.scatter_(-1, tensor.unsqueeze(-1), True)

    return result


def sample_d(
    min_d: float,
    max_d: float,
    n_samples: int,
    random: np.random.Generator,
) -> np.ndarray:
    """Generate samples from a 1/d distribution between min_d and max_d.

    Parameters
    ----------
    min_d : float
        Minimum value of d
    max_d : float
        Maximum value of d
    n_samples : int
        Number of samples to generate
    random : numpy.random.Generator
        Random number generator

    Returns
    -------
    numpy.ndarray
        Array of samples drawn from the distribution

    Notes
    -----
    The probability density function is:
    f(d) = 1/(d * ln(max_d/min_d)) for d in [min_d, max_d]

    The inverse CDF transform is:
    d = min_d * (max_d/min_d)**u where u ~ Uniform(0,1)

    """
    # Generate n_samples uniform random numbers in [0, 1]
    u = random.random(n_samples)
    # Transform u using the inverse CDF
    return min_d * (max_d / min_d) ** u


def compute_frames_nonpolymer(
    data: Input,
    coords,
    resolved_mask,
    atom_to_token,
    frame_data: List,
    resolved_frame_data: List,
) -> Tuple[List, List]:
    """Get the frames for non-polymer tokens.

    Parameters
    ----------
    data : Input
        The input data to the model.
    frame_data : List
        The frame data.
    resolved_frame_data : List
        The resolved frame data.

    Returns
    -------
    Tuple[List, List]
        The frame data and resolved frame data.

    """
    ## assert coords.shape[0] == 1, "No support for ensembles yet"
    # print("Warning: Frames are only using first conformer! For debug.")
    # coords = coords[0]

    frame_data = np.array(frame_data)
    resolved_frame_data = np.array(resolved_frame_data)
    asym_id_token = data.tokens["asym_id"]
    asym_id_atom = data.tokens["asym_id"][atom_to_token]
    token_idx = 0
    atom_idx = 0
    for id in np.unique(data.tokens["asym_id"]):
        mask_chain_token = asym_id_token == id
        mask_chain_atom = asym_id_atom == id
        num_tokens = mask_chain_token.sum()
        num_atoms = mask_chain_atom.sum()
        if (
            data.tokens[token_idx]["mol_type"] != const.chain_type_ids["NONPOLYMER"]
            or num_atoms < 3  # noqa: PLR2004
        ):
            token_idx += num_tokens
            atom_idx += num_atoms
            continue
        dist_mat = (
            (
                coords.reshape(-1, 3)[mask_chain_atom][:, None, :]
                - coords.reshape(-1, 3)[mask_chain_atom][None, :, :]
            )
            ** 2
        ).sum(-1) ** 0.5
        resolved_pair = 1 - (
            resolved_mask[mask_chain_atom][None, :]
            * resolved_mask[mask_chain_atom][:, None]
        ).astype(np.float32)
        resolved_pair[resolved_pair == 1] = math.inf
        indices = np.argsort(dist_mat + resolved_pair, axis=1)
        frames = (
            np.concatenate(
                [
                    indices[:, 1:2],
                    indices[:, 0:1],
                    indices[:, 2:3],
                ],
                axis=1,
            )
            + atom_idx
        )
        frame_data[token_idx : token_idx + num_atoms, :] = frames
        resolved_frame_data[token_idx : token_idx + num_atoms] = resolved_mask[
            frames
        ].all(axis=1)
        token_idx += num_tokens
        atom_idx += num_atoms
    frames_expanded = coords.reshape(-1, 3)[frame_data]

    mask_collinear = compute_collinear_mask(
        frames_expanded[:, 1] - frames_expanded[:, 0],
        frames_expanded[:, 1] - frames_expanded[:, 2],
    )
    return frame_data, resolved_frame_data & mask_collinear


def compute_collinear_mask(v1, v2):
    norm1 = np.linalg.norm(v1, axis=1, keepdims=True)
    norm2 = np.linalg.norm(v2, axis=1, keepdims=True)
    v1 = v1 / (norm1 + 1e-6)
    v2 = v2 / (norm2 + 1e-6)
    mask_angle = np.abs(np.sum(v1 * v2, axis=1)) < 0.9063
    mask_overlap1 = norm1.reshape(-1) > 1e-2
    mask_overlap2 = norm2.reshape(-1) > 1e-2
    return mask_angle & mask_overlap1 & mask_overlap2


def dummy_msa(residues: np.ndarray) -> MSA:
    """Create a dummy MSA for a chain.

    Parameters
    ----------
    residues : np.ndarray
        The residues for the chain.

    Returns
    -------
    MSA
        The dummy MSA.

    """
    residues = [res["res_type"] for res in residues]
    deletions = []
    sequences = [(0, -1, 0, len(residues), 0, 0)]
    return MSA(
        residues=np.array(residues, dtype=MSAResidue),
        deletions=np.array(deletions, dtype=MSADeletion),
        sequences=np.array(sequences, dtype=MSASequence),
    )


def construct_paired_msa(  # noqa: C901, PLR0915, PLR0912
    data: Input,
    random: np.random.Generator,
    max_seqs: int,
    max_pairs: int = 8192,
    max_total: int = 16384,
    random_subset: bool = False,
) -> Tuple[Tensor, Tensor, Tensor]:
    """Pair the MSA data.

    Parameters
    ----------
    data : Input
        The input data to the model.

    Returns
    -------
    Tensor
        The MSA data.
    Tensor
        The deletion data.
    Tensor
        Mask indicating paired sequences.

    """
    # Get unique chains (ensuring monotonicity in the order)
    assert np.all(np.diff(data.tokens["asym_id"], n=1) >= 0)
    chain_ids = np.unique(data.tokens["asym_id"])

    # Get relevant MSA, and create a dummy for chains without
    msa: Dict[int, MSA] = {}
    for chain_id in chain_ids:
        # Get input sequence
        chain = data.structure.chains[chain_id]
        res_start = chain["res_idx"]
        res_end = res_start + chain["res_num"]
        residues = data.structure.residues[res_start:res_end]

        # Check if we have an MSA, and that the
        # first sequence matches the input sequence
        if chain_id in data.msa:
            # Set the MSA
            msa[chain_id] = data.msa[chain_id]

            # Run length and residue type checks
            first = data.msa[chain_id].sequences[0]
            first_start = first["res_start"]
            first_end = first["res_end"]
            msa_residues = data.msa[chain_id].residues
            first_residues = msa_residues[first_start:first_end]

            warning = "Warning: MSA does not match input sequence, creating dummy."
            if len(residues) == len(first_residues):
                # If there is a mismatch, check if it is between MET & UNK
                # If so, replace the first sequence with the input sequence.
                # Otherwise, replace with a dummy MSA for this chain.
                mismatches = residues["res_type"] != first_residues["res_type"]
                if mismatches.sum().item():
                    idx = np.where(mismatches)[0]
                    is_met = residues["res_type"][idx] == const.token_ids["MET"]
                    is_unk = residues["res_type"][idx] == const.token_ids["UNK"]
                    is_msa_unk = (
                        first_residues["res_type"][idx] == const.token_ids["UNK"]
                    )
                    if (np.all(is_met) and np.all(is_msa_unk)) or np.all(is_unk):
                        msa_residues[first_start:first_end]["res_type"] = residues[
                            "res_type"
                        ]
                    else:
                        print(
                            warning,
                            "1",
                            residues["res_type"],
                            first_residues["res_type"],
                            data.record.id,
                        )
                        msa[chain_id] = dummy_msa(residues)
            else:
                print(
                    warning,
                    "2",
                    residues["res_type"],
                    first_residues["res_type"],
                    data.record.id,
                )
                msa[chain_id] = dummy_msa(residues)
        else:
            msa[chain_id] = dummy_msa(residues)

    # Map taxonomies to (chain_id, seq_idx)
    taxonomy_map: Dict[str, List] = {}
    for chain_id, chain_msa in msa.items():
        sequences = chain_msa.sequences
        sequences = sequences[sequences["taxonomy"] != -1]
        for sequence in sequences:
            seq_idx = sequence["seq_idx"]
            taxon = sequence["taxonomy"]
            taxonomy_map.setdefault(taxon, []).append((chain_id, seq_idx))

    # Remove taxonomies with only one sequence and sort by the
    # number of chain_id present in each of the taxonomies
    taxonomy_map = {k: v for k, v in taxonomy_map.items() if len(v) > 1}
    taxonomy_map = sorted(
        taxonomy_map.items(),
        key=lambda x: len({c for c, _ in x[1]}),
        reverse=True,
    )

    # Keep track of the sequences available per chain, keeping the original
    # order of the sequences in the MSA to favor the best matching sequences
    visited = {(c, s) for c, items in taxonomy_map for s in items}
    available = {}
    for c in chain_ids:
        available[c] = [
            i for i in range(1, len(msa[c].sequences)) if (c, i) not in visited
        ]

    # Create sequence pairs
    is_paired = []
    pairing = []

    # Start with the first sequence for each chain
    is_paired.append({c: 1 for c in chain_ids})
    pairing.append({c: 0 for c in chain_ids})

    # Then add up to 8191 paired rows
    for _, pairs in taxonomy_map:
        # Group occurences by chain_id in case we have multiple
        # sequences from the same chain and same taxonomy
        chain_occurences = {}
        for chain_id, seq_idx in pairs:
            chain_occurences.setdefault(chain_id, []).append(seq_idx)

        # We create as many pairings as the maximum number of occurences
        max_occurences = max(len(v) for v in chain_occurences.values())
        for i in range(max_occurences):
            row_pairing = {}
            row_is_paired = {}

            # Add the chains present in the taxonomy
            for chain_id, seq_idxs in chain_occurences.items():
                # Roll over the sequence index to maximize diversity
                idx = i % len(seq_idxs)
                seq_idx = seq_idxs[idx]

                # Add the sequence to the pairing
                row_pairing[chain_id] = seq_idx
                row_is_paired[chain_id] = 1

            # Add any missing chains
            for chain_id in chain_ids:
                if chain_id not in row_pairing:
                    row_is_paired[chain_id] = 0
                    if available[chain_id]:
                        # Add the next available sequence
                        seq_idx = available[chain_id].pop(0)
                        row_pairing[chain_id] = seq_idx
                    else:
                        # No more sequences available, we place a gap
                        row_pairing[chain_id] = -1

            pairing.append(row_pairing)
            is_paired.append(row_is_paired)

            # Break if we have enough pairs
            if len(pairing) >= max_pairs:
                break

        # Break if we have enough pairs
        if len(pairing) >= max_pairs:
            break

    # Now add up to 16384 unpaired rows total
    max_left = max(len(v) for v in available.values())
    for _ in range(min(max_total - len(pairing), max_left)):
        row_pairing = {}
        row_is_paired = {}
        for chain_id in chain_ids:
            row_is_paired[chain_id] = 0
            if available[chain_id]:
                # Add the next available sequence
                seq_idx = available[chain_id].pop(0)
                row_pairing[chain_id] = seq_idx
            else:
                # No more sequences available, we place a gap
                row_pairing[chain_id] = -1

        pairing.append(row_pairing)
        is_paired.append(row_is_paired)

        # Break if we have enough sequences
        if len(pairing) >= max_total:
            break

    # Randomly sample a subset of the pairs
    # ensuring the first row is always present
    if random_subset:
        num_seqs = len(pairing)
        if num_seqs > max_seqs:
            indices = random.choice(
                np.arange(1, num_seqs), size=max_seqs - 1, replace=False
            )  # noqa: NPY002
            pairing = [pairing[0]] + [pairing[i] for i in indices]
            is_paired = [is_paired[0]] + [is_paired[i] for i in indices]
    else:
        # Deterministic downsample to max_seqs
        pairing = pairing[:max_seqs]
        is_paired = is_paired[:max_seqs]

    # Map (chain_id, seq_idx, res_idx) to deletion
    deletions = {}
    for chain_id, chain_msa in msa.items():
        chain_deletions = chain_msa.deletions
        for sequence in chain_msa.sequences:
            del_start = sequence["del_start"]
            del_end = sequence["del_end"]
            chain_deletions = chain_msa.deletions[del_start:del_end]
            for deletion_data in chain_deletions:
                seq_idx = sequence["seq_idx"]
                res_idx = deletion_data["res_idx"]
                deletion = deletion_data["deletion"]
                deletions[(chain_id, seq_idx, res_idx)] = deletion

    # Add all the token MSA data
    msa_data, del_data, paired_data = prepare_msa_arrays(
        data.tokens, pairing, is_paired, deletions, msa
    )
    msa_data = torch.tensor(msa_data, dtype=torch.long)
    del_data = torch.tensor(del_data, dtype=torch.float)
    paired_data = torch.tensor(paired_data, dtype=torch.float)
    return msa_data, del_data, paired_data


def prepare_msa_arrays(
    tokens,
    pairing: list[dict[int, int]],
    is_paired: list[dict[int, int]],
    deletions: dict[tuple[int, int, int], int],
    msa: dict[int, MSA],
) -> tuple[npt.NDArray[np.int64], npt.NDArray[np.int64], npt.NDArray[np.int64]]:
    """Reshape data to play nicely with numba jit."""
    token_asym_ids_arr = np.array([t["asym_id"] for t in tokens], dtype=np.int64)
    token_res_idxs_arr = np.array([t["res_idx"] for t in tokens], dtype=np.int64)

    chain_ids = sorted(msa.keys())

    # chain_ids are not necessarily contiguous (e.g. they might be 0, 24, 25).
    # This allows us to look up a chain_id by it's index in the chain_ids list.
    chain_id_to_idx = {chain_id: i for i, chain_id in enumerate(chain_ids)}
    token_asym_ids_idx_arr = np.array(
        [chain_id_to_idx[asym_id] for asym_id in token_asym_ids_arr], dtype=np.int64
    )

    pairing_arr = np.zeros((len(pairing), len(chain_ids)), dtype=np.int64)
    is_paired_arr = np.zeros((len(is_paired), len(chain_ids)), dtype=np.int64)

    for i, row_pairing in enumerate(pairing):
        for chain_id in chain_ids:
            pairing_arr[i, chain_id_to_idx[chain_id]] = row_pairing[chain_id]

    for i, row_is_paired in enumerate(is_paired):
        for chain_id in chain_ids:
            is_paired_arr[i, chain_id_to_idx[chain_id]] = row_is_paired[chain_id]

    max_seq_len = max(len(msa[chain_id].sequences) for chain_id in chain_ids)

    # we want res_start from sequences
    msa_sequences = np.full((len(chain_ids), max_seq_len), -1, dtype=np.int64)
    for chain_id in chain_ids:
        for i, seq in enumerate(msa[chain_id].sequences):
            msa_sequences[chain_id_to_idx[chain_id], i] = seq["res_start"]

    max_residues_len = max(len(msa[chain_id].residues) for chain_id in chain_ids)
    msa_residues = np.full((len(chain_ids), max_residues_len), -1, dtype=np.int64)
    for chain_id in chain_ids:
        residues = msa[chain_id].residues.astype(np.int64)
        idxs = np.arange(len(residues))
        chain_idx = chain_id_to_idx[chain_id]
        msa_residues[chain_idx, idxs] = residues

    deletions_dict = numba.typed.Dict.empty(
        key_type=numba.types.Tuple(
            [numba.types.int64, numba.types.int64, numba.types.int64]
        ),
        value_type=numba.types.int64,
    )
    deletions_dict.update(deletions)

    return _prepare_msa_arrays_inner(
        token_asym_ids_arr,
        token_res_idxs_arr,
        token_asym_ids_idx_arr,
        pairing_arr,
        is_paired_arr,
        deletions_dict,
        msa_sequences,
        msa_residues,
        const.token_ids["-"],
        const.token_ids["UNK"],
    )


deletions_dict_type = types.DictType(types.UniTuple(types.int64, 3), types.int64)


@numba.njit(
    [
        types.Tuple(
            (
                types.int64[:, ::1],  # msa_data
                types.int64[:, ::1],  # del_data
                types.int64[:, ::1],  # paired_data
            )
        )(
            types.int64[::1],  # token_asym_ids
            types.int64[::1],  # token_res_idxs
            types.int64[::1],  # token_asym_ids_idx
            types.int64[:, ::1],  # pairing
            types.int64[:, ::1],  # is_paired
            deletions_dict_type,  # deletions
            types.int64[:, ::1],  # msa_sequences
            types.int64[:, ::1],  # msa_residues
            types.int64,  # gap_token
            types.int64,  # unk_token
        )
    ],
    cache=True,
)
def _prepare_msa_arrays_inner(
    token_asym_ids: npt.NDArray[np.int64],
    token_res_idxs: npt.NDArray[np.int64],
    token_asym_ids_idx: npt.NDArray[np.int64],
    pairing: npt.NDArray[np.int64],
    is_paired: npt.NDArray[np.int64],
    deletions: dict[tuple[int, int, int], int],
    msa_sequences: npt.NDArray[np.int64],
    msa_residues: npt.NDArray[np.int64],
    gap_token: int,
    unk_token: int,
) -> tuple[npt.NDArray[np.int64], npt.NDArray[np.int64], npt.NDArray[np.int64]]:
    # If there is an index out of bounds error when indexing into any of the arrays in here, then it does not throw an error but instead just places a random seeming high value into the array.
    n_tokens = len(token_asym_ids)
    n_pairs = len(pairing)
    msa_data = np.full((n_tokens, n_pairs), gap_token, dtype=np.int64)
    paired_data = np.zeros((n_tokens, n_pairs), dtype=np.int64)
    del_data = np.zeros((n_tokens, n_pairs), dtype=np.int64)

    # Add all the token MSA data
    res_idx_contiguous = 0
    chain_id_tracker = token_asym_ids[0]
    res_idx_tracker = token_res_idxs[0]
    for token_idx in range(n_tokens):
        chain_id_idx = token_asym_ids_idx[token_idx]
        chain_id = token_asym_ids[token_idx]
        res_idx = token_res_idxs[token_idx]

        if res_idx != res_idx_tracker:
            res_idx_contiguous += 1
        res_idx_tracker = res_idx
        if chain_id != chain_id_tracker:
            res_idx_contiguous = 0
        chain_id_tracker = chain_id

        for pair_idx in range(n_pairs):
            seq_idx = pairing[pair_idx, chain_id_idx]
            paired_data[token_idx, pair_idx] = is_paired[pair_idx, chain_id_idx]

            # Add residue type
            if seq_idx != -1:
                res_start = msa_sequences[chain_id_idx, seq_idx]
                # TODO this only works for MSAs that are cropped
                seq = msa_sequences[chain_id_idx]
                res = msa_residues[chain_id_idx]
                if (
                    seq[0] == 0
                    and np.all(seq[1:] == -1)
                    and not np.all((res == -1) | (res == unk_token))
                ):
                    # if a chain has no MSA, and is not a chain of small molecules, we use the contiguous residue index
                    res_type = msa_residues[
                        chain_id_idx, res_start + res_idx_contiguous
                    ]
                else:
                    # if a chain has an MSA, or is a chain of small molecules, we use the residue index
                    res_type = msa_residues[chain_id_idx, res_start + res_idx]
                k = (chain_id, seq_idx, res_idx)
                if k in deletions:
                    del_data[token_idx, pair_idx] = deletions[k]
                msa_data[token_idx, pair_idx] = res_type

    return msa_data, del_data, paired_data


####################################################################################################
# FEATURES
####################################################################################################


def select_subset_from_mask(mask, p, random: np.random.Generator) -> np.ndarray:
    num_true = np.sum(mask)
    v = random.geometric(p) + 1
    k = min(v, num_true)

    true_indices = np.where(mask)[0]

    # Randomly select k indices from the true_indices
    selected_indices = random.choice(true_indices, size=k, replace=False)

    new_mask = np.zeros_like(mask)
    new_mask[selected_indices] = 1

    return new_mask


def get_range_bin(value: float, range_dict: Dict[Tuple[float, float], int], default=0):
    """Get the bin of a value given a range dictionary."""
    value = float(value)
    for k, idx in range_dict.items():
        if k == "other":
            continue
        low, high = k
        if low <= value < high:
            return idx
    return default


def process_token_features(  # noqa: C901, PLR0915, PLR0912
    data: Input,
    random: np.random.Generator,
    max_tokens: Optional[int] = None,
    binder_pocket_conditioned_prop: Optional[float] = 0.0,
    contact_conditioned_prop: Optional[float] = 0.0,
    binder_pocket_cutoff_min: Optional[float] = 4.0,
    binder_pocket_cutoff_max: Optional[float] = 20.0,
    binder_pocket_sampling_geometric_p: Optional[float] = 0.0,
    only_ligand_binder_pocket: Optional[bool] = False,
    only_pp_contact: Optional[bool] = False,
    maximum_bond_distance: Optional[int] = 0,
    override_method: Optional[str] = None,
    noisy_coords_std: Optional[float] = 0.0,
    disulfide_prob: Optional[float] = 1.0,
    disulfide_on: Optional[bool] = False,
) -> Dict[str, Tensor]:
    """Get the token features.

    Parameters
    ----------
    data : Input
        The input data to the model.
    max_tokens : int
        The maximum number of tokens.

    Returns
    -------
    Dict[str, Tensor]
        The token features.

    """
    # Token data
    one_hot = one_hot_torch
    token_data = data.tokens
    token_bonds = data.bonds

    # Token core features
    t0 = time.time()
    token_index = torch.arange(len(token_data), dtype=torch.long)
    residue_index = from_numpy(token_data["res_idx"]).long()
    asym_id = from_numpy(token_data["asym_id"]).long()
    entity_id = from_numpy(token_data["entity_id"]).long()
    sym_id = from_numpy(token_data["sym_id"]).long()
    mol_type = from_numpy(token_data["mol_type"]).long()
    res_type = from_numpy(token_data["res_type"]).long()
    is_standard = from_numpy(token_data["is_standard"])
    design = from_numpy(token_data["design_mask"]).long()
    res_type = one_hot(res_type, num_classes=const.num_tokens)
    modified = from_numpy(token_data["modified"]).long()
    ccd = from_numpy(token_data["ccd"]).long()
    binding_type = from_numpy(token_data["binding_type"]).long()
    structure_group = from_numpy(token_data["structure_group"]).long()
    center_coords = from_numpy(token_data["center_coords"]).float()
    target_msa_mask = from_numpy(token_data["target_msa_mask"])
    design_ss_mask = from_numpy(token_data["design_ss_mask"])
    feature_residue_index = from_numpy(token_data["feature_res_idx"]).long()
    feature_asym_id = from_numpy(token_data["feature_asym_id"]).long()
    is_anchor = from_numpy(token_data["is_anchor"]).long()
    token_to_res = from_numpy(data.token_to_res).long()

    method = (
        np.zeros(len(token_data))
        + const.method_types_ids[
            ("other" if override_method is None else override_method.lower())
        ]
    )
    default_T = const.temperature_bins_ids["other"]
    default_pH = const.ph_bins_ids["other"]
    temp_feature = np.zeros(len(token_data)) + default_T
    ph_feature = np.zeros(len(token_data)) + default_pH
    if data.record is not None:
        if (
            override_method is None
            and data.record.structure.method is not None
            and data.record.structure.method.lower() in const.method_types_ids
        ):
            method = (method * 0) + const.method_types_ids[
                data.record.structure.method.lower()
            ]

        if data.record.md is not None:
            if data.record.md.temperature is not None:
                T = data.record.md.temperature
                temp_feature = (temp_feature * 0) + get_range_bin(
                    T, const.temperature_bins_ids, default=default_T
                )
            if data.record.md.pH is not None:
                pH = data.record.md.pH
                ph_feature = (ph_feature * 0) + get_range_bin(
                    pH, const.ph_bins_ids, default=default_pH
                )
        else:
            if data.record.structure.temperature is not None:
                T = data.record.structure.temperature
                temp_feature = (temp_feature * 0) + get_range_bin(
                    T, const.temperature_bins_ids, default=default_T
                )
            if data.record.structure.pH is not None:
                pH = data.record.structure.pH
                ph_feature = (ph_feature * 0) + get_range_bin(
                    pH, const.ph_bins_ids, default=default_pH
                )
    method_feature = from_numpy(method).long()
    temp_feature = from_numpy(temp_feature).long()
    ph_feature = from_numpy(ph_feature).long()

    # Token mask features
    pad_mask = torch.ones(len(token_data), dtype=torch.float)
    resolved_mask = from_numpy(token_data["resolved_mask"]).float()
    disto_mask = from_numpy(token_data["disto_mask"]).float()

    # Token bond features
    if max_tokens is not None:
        pad_len = max_tokens - len(token_data)
        num_tokens = max_tokens if pad_len > 0 else len(token_data)
    else:
        num_tokens = len(token_data)

    tok_to_idx = {tok["token_idx"]: idx for idx, tok in enumerate(token_data)}
    bonds = torch.zeros(num_tokens, num_tokens, dtype=torch.float)
    bonds_type = torch.zeros(num_tokens, num_tokens, dtype=torch.long)
    skip_prob = random.random()
    for token_bond in token_bonds:
        token_1 = tok_to_idx[token_bond["token_1"]]
        token_2 = tok_to_idx[token_bond["token_2"]]
        # TODO: Add a check to not include head-to-tail connections for cyclic peptides here
        bond_type = token_bond["type"]
        if (
            bond_type == 1 + const.bond_type_ids["COVALENT"]
            and token_data[token_1]["res_name"] == "CYS"
            and token_data[token_2]["res_name"] == "CYS"
            and token_data[token_1]["asym_id"] == token_data[token_2]["asym_id"]
            and abs(token_1 - token_2) != token_data[token_1]["cyclic_period"]
            and (not disulfide_on or skip_prob < 1 - disulfide_prob)
        ):
            continue

        bonds[token_1, token_2] = 1
        bonds[token_2, token_1] = 1
        bond_type = token_bond["type"]
        bonds_type[token_1, token_2] = bond_type
        bonds_type[token_2, token_1] = bond_type

    if maximum_bond_distance > 1:
        G = nx.from_numpy_array(bonds.numpy())
        shortest_path = nx.floyd_warshall_numpy(G)
        shortest_path = np.where(
            shortest_path > shortest_path.shape[0],
            maximum_bond_distance + 1,
            np.minimum(shortest_path, maximum_bond_distance),
        )
        bonds = one_hot(
            torch.from_numpy(shortest_path).long(),
            num_classes=maximum_bond_distance + 2,
        )
    else:
        bonds = bonds.unsqueeze(-1)

    contact_conditioning = (
        np.zeros((len(token_data), len(token_data)))
        + const.contact_conditioning_info["UNSELECTED"]
    )
    contact_threshold = np.zeros((len(token_data), len(token_data)))
    contact_threshold = from_numpy(contact_threshold).float()

    if np.all(contact_conditioning == const.contact_conditioning_info["UNSELECTED"]):
        contact_conditioning = (
            contact_conditioning
            - const.contact_conditioning_info["UNSELECTED"]
            + const.contact_conditioning_info["UNSPECIFIED"]
        )
    contact_conditioning = from_numpy(contact_conditioning).long()
    contact_conditioning = one_hot(
        contact_conditioning, num_classes=len(const.contact_conditioning_info)
    )

    # compute cyclic polymer mask
    cyclic_ids = {}
    for idx_chain, asym_id_iter in enumerate(data.structure.chains["asym_id"]):
        for connection in data.structure.bonds:
            if (
                idx_chain == connection["chain_1"] == connection["chain_2"]
                and data.structure.chains[connection["chain_1"]]["res_num"] > 2
                and connection["res_1"]
                != connection["res_2"]  # Avoid same residue bonds!
            ):
                if (
                    data.structure.chains[connection["chain_1"]]["res_num"]
                    == (connection["res_2"] + 1)
                    and connection["res_1"] == 0
                ) or (
                    data.structure.chains[connection["chain_1"]]["res_num"]
                    == (connection["res_1"] + 1)
                    and connection["res_2"] == 0
                ):
                    cyclic_ids[asym_id_iter] = data.structure.chains[
                        connection["chain_1"]
                    ]["res_num"]
    cyclic = from_numpy(
        np.array(
            [
                (cyclic_ids[asym_id_iter] if asym_id_iter in cyclic_ids else 0)
                for asym_id_iter in token_data["asym_id"]
            ]
        )
    ).float()

    # Compute token_distance_mask
    token_distance_mask = torch.zeros((len(token_data), len(token_data)))
    if all(token_data["structure_group"] == 1):
        # equal to the elif case but more efficient
        token_distance_mask += 1
    elif any(token_data["structure_group"] > 0):
        for i, token_i in enumerate(token_data):
            for j, token_j in enumerate(token_data):
                if (
                    token_i["structure_group"] > 0
                    and token_i["structure_group"] == token_j["structure_group"]
                ):
                    token_distance_mask[i, j] = 1

    noisy_center_coords = center_coords + np.random.normal(
        0, noisy_coords_std, size=center_coords.shape
    )

    ligand_affinity_mask = mol_type == const.chain_type_ids["NONPOLYMER"]

    # Pad to max tokens if given
    if max_tokens is not None:
        pad_len = max_tokens - len(token_data)
        if pad_len > 0:
            token_index = pad_dim(token_index, 0, pad_len)
            residue_index = pad_dim(residue_index, 0, pad_len)
            asym_id = pad_dim(asym_id, 0, pad_len)
            entity_id = pad_dim(entity_id, 0, pad_len)
            sym_id = pad_dim(sym_id, 0, pad_len)
            mol_type = pad_dim(mol_type, 0, pad_len)
            res_type = pad_dim(res_type, 0, pad_len)
            is_standard = pad_dim(is_standard, 0, pad_len)
            is_anchor = pad_dim(is_anchor, 0, pad_len)
            design = pad_dim(design, 0, pad_len)
            binding_type = pad_dim(binding_type, 0, pad_len)
            structure_group = pad_dim(structure_group, 0, pad_len)
            pad_mask = pad_dim(pad_mask, 0, pad_len)
            resolved_mask = pad_dim(resolved_mask, 0, pad_len)
            disto_mask = pad_dim(disto_mask, 0, pad_len)
            contact_conditioning = pad_dim(contact_conditioning, 0, pad_len)
            contact_conditioning = pad_dim(contact_conditioning, 1, pad_len)
            contact_threshold = pad_dim(contact_threshold, 0, pad_len)
            contact_threshold = pad_dim(contact_threshold, 1, pad_len)
            method_feature = pad_dim(method_feature, 0, pad_len)
            temp_feature = pad_dim(temp_feature, 0, pad_len)
            ph_feature = pad_dim(ph_feature, 0, pad_len)
            modified = pad_dim(modified, 0, pad_len)
            ccd = pad_dim(ccd, 0, pad_len)
            cyclic = pad_dim(cyclic, 0, pad_len)
            ligand_affinity_mask = pad_dim(ligand_affinity_mask, 0, pad_len)
            center_coords = pad_dim(center_coords, 0, pad_len)
            noisy_center_coords = pad_dim(noisy_center_coords, 0, pad_len)
            token_distance_mask = pad_dim(
                pad_dim(token_distance_mask, 0, pad_len), 1, pad_len
            )
            target_msa_mask = pad_dim(target_msa_mask, 0, pad_len)
            design_ss_mask = pad_dim(design_ss_mask, 0, pad_len)
            feature_residue_index = pad_dim(feature_residue_index, 0, pad_len)
            feature_asym_id = pad_dim(feature_asym_id, 0, pad_len)
            token_to_res = pad_dim(token_to_res, 0, pad_len)
    token_features = {
        "token_index": token_index,
        "residue_index": residue_index,
        "asym_id": asym_id,
        "entity_id": entity_id,
        "sym_id": sym_id,
        "mol_type": mol_type,
        "res_type": res_type,
        "res_type_clone": res_type.clone(),
        "is_standard": is_standard,
        "design_mask": design,
        "binding_type": binding_type,
        "structure_group": structure_group,
        "token_bonds": bonds,
        "type_bonds": bonds_type,
        "token_pad_mask": pad_mask,
        "token_resolved_mask": resolved_mask,
        "token_disto_mask": disto_mask,
        "token_pair_mask": disto_mask,
        "contact_conditioning": contact_conditioning,
        "contact_threshold": contact_threshold,
        "method_feature": method_feature,
        "temp_feature": temp_feature,
        "ph_feature": ph_feature,
        "modified": modified,
        "ccd": ccd,
        "cyclic": cyclic,
        "center_coords": center_coords,
        "noisy_center_coords": noisy_center_coords,
        "token_distance_mask": token_distance_mask,
        "target_msa_mask": target_msa_mask,
        "design_ss_mask": design_ss_mask,
        "feature_residue_index": feature_residue_index,
        "feature_asym_id": feature_asym_id,
        "is_anchor": is_anchor,
        "ligand_affinity_mask": ligand_affinity_mask,
        "token_to_res": token_to_res,
    }

    return token_features


def process_atom_features(
    data: Input,
    random: np.random.Generator,
    ensemble_features: Dict,
    molecules: Dict[str, Mol],
    atoms_per_window_queries: int = 32,
    min_dist: float = 2.0,
    max_dist: float = 22.0,
    num_bins: int = 64,
    max_atoms: Optional[int] = None,
    max_tokens: Optional[int] = None,
    disto_use_ensemble: Optional[bool] = False,
    add_affinity_cropping: Optional[bool] = False,
    max_tokens_affinity: int = 384,
    max_tokens_to_atomize_affinity: int = 32,
    max_atoms_to_atomize_affinity: int = 128,
    affinity_only_atoms: bool = False,
    override_bfactor: bool = False,
    compute_frames: bool = True,
    override_coords: Optional[Tensor] = None,
    bfactor_md_correction: bool = False,
    backbone_only: bool = False,
    atom14: bool = False,
    atom14_geometric: bool = True,
    atom37: bool = False,
    design: bool = False,
    res_type_override: Optional[np.ndarray] = None,
    canonical_atoms: Optional[Dict[str, np.ndarray]] = None,
    charge_info: Optional[np.ndarray] = None,
    inverse_fold: bool = False,
) -> Dict[str, Tensor]:
    """Get the atom features.

    Parameters
    ----------
    data : Input
        The input to the model.
    max_atoms : int, optional
        The maximum number of atoms.

    Returns
    -------
    Dict[str, Tensor]
        The atom features.

    """
    if inverse_fold:
        one_hot = one_hot_bool
    else:
        one_hot = one_hot_torch

    if backbone_only or atom14 or atom37:
        assert design, "Design must be true if backbone_only"

    if res_type_override is not None:
        assert not backbone_only
        assert not atom14
        assert not atom37
        assert not design

    # Filter to tokens' atoms
    atom_data = []
    new_to_old_atomidx = []
    atom_name = []
    atom_element = []
    atom_charge = []
    atom_conformer = []
    atom_chirality = []
    ref_space_uid = []
    coord_data = []
    frame_data = []
    resolved_frame_data = []
    atom_to_token = []
    token_to_rep_atom = []  # index on cropped atom table
    r_set_to_rep_atom = []
    disto_coords_ensemble = []
    backbone_feat_index = []
    token_to_bb4_atoms = []
    backbone_mask = []
    fake_atom_mask = []
    masked_ref_atom_name_chars = []
    if add_affinity_cropping:
        atom_mol_type = []
        token_mol_type = []
        token_to_num_atoms = []

    e_offsets = data.structure.ensemble["atom_coord_idx"]
    atom_idx = 0

    # Start atom idx in full atom table for structures chosen. Up to num_ensembles points.
    ensemble_atom_starts = [
        data.structure.ensemble[idx]["atom_coord_idx"]
        for idx in ensemble_features["ensemble_ref_idxs"]
    ]

    # Set unk chirality id
    unk_chirality = const.chirality_type_ids[const.unk_chirality_type]

    chain_res_ids = {}
    res_index_to_conf_id = {}
    anchor_parent_idx = []
    anchor_idx = 0

    for token_id, token in enumerate(data.tokens):
        original_atom_idx = token["atom_idx"]
        original_atom_num = token["atom_num"]
        if res_type_override is not None:
            res_type = res_type_override[token_id]
            # Even if we are not overriding *this* token, we still need to renumber the atom
            # indices since previous tokens may have been overridden.
            token = token.copy()
            token["atom_idx"] = atom_idx
            if res_type != const.token_ids[const.unk_token["PROTEIN"]]:
                # We are overriding this token.
                res_name = const.tokens[res_type]
                atoms_override = canonical_atoms[res_name]
                token["res_type"] = res_type
                token["res_name"] = res_name
                token["ccd"] = convert_ccd(res_name)
                token["atom_num"] = len(atoms_override)
                token["center_idx"] = atom_idx + const.res_to_center_atom_id[res_name]
                token["disto_idx"] = atom_idx + const.res_to_disto_atom_id[res_name]
            else:
                # Update indices that depend on atom index, keeping the same relative offset.
                token["center_idx"] = (
                    token["center_idx"] - original_atom_idx
                ) + atom_idx
                token["disto_idx"] = (token["disto_idx"] - original_atom_idx) + atom_idx

        # Get the chain residue ids
        chain_idx, res_id = token["asym_id"], token["res_idx"]
        chain = data.structure.chains[chain_idx]
        res_type = const.tokens[token["res_type"]]

        if (chain_idx, res_id) not in chain_res_ids:
            new_idx = len(chain_res_ids)
            chain_res_ids[(chain_idx, res_id)] = new_idx
        else:
            new_idx = chain_res_ids[(chain_idx, res_id)]

        # Get the molecule and conformer
        mol = molecules[token["res_name"]]
        atom_name_to_ref = {}
        atom_name_to_ref = {a.GetProp("name"): a for a in mol.GetAtoms()}
        # Sample a random conformer

        if (chain_idx, res_id) not in res_index_to_conf_id:
            conf_ids = [int(conf.GetId()) for conf in mol.GetConformers()]
            conf_id = int(random.choice(conf_ids))
            res_index_to_conf_id[(chain_idx, res_id)] = conf_id

        conf_id = res_index_to_conf_id[(chain_idx, res_id)]
        conformer = mol.GetConformer(conf_id)

        # get atom_num
        if bool(token["design_mask"]) and backbone_only:
            if token["mol_type"] == const.chain_type_ids["PROTEIN"]:
                atom_num = 4
            elif token["mol_type"] == const.chain_type_ids["DNA"]:
                atom_num = 11
            elif token["mol_type"] == const.chain_type_ids["RNA"]:
                atom_num = 12
            else:
                atom_num = token["atom_num"]
            assert token["atom_num"] >= atom_num
        elif bool(token["design_mask"]) and (atom14 or atom37):
            if token["mol_type"] == const.chain_type_ids["PROTEIN"]:
                atom_num = 14 if atom14 else 37
            elif token["mol_type"] == const.chain_type_ids["DNA"]:
                atom_num = 22
            elif token["mol_type"] == const.chain_type_ids["RNA"]:
                atom_num = 23
            else:
                atom_num = token["atom_num"]
        else:
            atom_num = token["atom_num"]

        # make backbone mask
        is_backbone = np.zeros(atom_num)
        if token["mol_type"] == const.chain_type_ids["PROTEIN"]:
            is_backbone[:4] = 1
        elif token["mol_type"] == const.chain_type_ids["DNA"]:
            is_backbone[:11] = 1
        elif token["mol_type"] == const.chain_type_ids["RNA"]:
            is_backbone[:12] = 1
        else:
            is_backbone = np.zeros(atom_num)
        backbone_mask.append(is_backbone)

        # get token rep_atom_offset
        if design:
            rep_atom_offset = token["center_idx"] - token["atom_idx"]
        else:
            rep_atom_offset = token["disto_idx"] - token["atom_idx"]

        assert rep_atom_offset >= 0, (
            f"rep_atom_offset: {rep_atom_offset} is less than 0"
        )
        # get atoms and fake atoms
        is_protein = token["mol_type"] == const.chain_type_ids["PROTEIN"]

        if bool(token["design_mask"]) and atom14 and is_protein:
            real_atoms = data.structure.atoms[
                token["atom_idx"] : token["atom_idx"] + token["atom_num"]
            ]
            if atom14_geometric:
                fake_atoms = possible_fake_atoms[token["atom_num"] : atom_num]
                token_atoms = np.concatenate([real_atoms, fake_atoms])
                placements = np.array(const.fake_atom_placements[res_type])
                oxygen = real_atoms[const.ref_atoms[res_type].index("O")]
                token_atoms["coords"][placements == "O"] = oxygen["coords"]
                token_atoms["is_present"][placements == "O"] = oxygen["is_present"]
                token_atoms["plddt"][placements == "O"] = oxygen["plddt"]

                nitrogen = real_atoms[const.ref_atoms[res_type].index("N")]
                token_atoms["coords"][placements == "N"] = nitrogen["coords"]
                token_atoms["is_present"][placements == "N"] = nitrogen["is_present"]
                token_atoms["plddt"][placements == "N"] = nitrogen["plddt"]
            else:
                rep_atom = real_atoms[rep_atom_offset]
                fake_atoms = possible_fake_atoms[token["atom_num"] : atom_num]
                fake_atoms["coords"] = rep_atom["coords"]
                fake_atoms["is_present"] = rep_atom["is_present"]
                fake_atoms["plddt"] = rep_atom["plddt"]
                token_atoms = np.concatenate([real_atoms, fake_atoms])
            fake_atom_mask.append([0] * len(real_atoms))
            fake_atom_mask.append([1] * len(fake_atoms))
            assert len(token_atoms) == atom_num
        elif bool(token["design_mask"]) and atom37 and is_protein:
            real_atoms = data.structure.atoms[
                token["atom_idx"] : token["atom_idx"] + token["atom_num"]
            ]
            real_atoms_name_to_idx = {
                real_atoms[i][0]: i for i in range(len(real_atoms))
            }
            fake_atoms = [
                possible_fake_atoms[i]
                for i, name in enumerate(const.atom_types)
                if name in real_atoms_name_to_idx
            ]

            token_atoms = np.array(
                [
                    real_atoms[real_atoms_name_to_idx[name]]
                    if name in real_atoms_name_to_idx
                    else possible_fake_atoms[i]
                    for i, name in enumerate(const.atom_types)
                ]
            )
            fake_atom_mask.append(
                [
                    0 if name in real_atoms_name_to_idx else 1
                    for name in const.atom_types
                ]
            )
            placements = np.array(
                [
                    "." if name in real_atoms_name_to_idx else "CA"
                    for name in const.atom_types
                ]
            )

            ca = real_atoms[const.atom_types.index("CA")]
            token_atoms["coords"][placements == "CA"] = ca["coords"]
            token_atoms["is_present"][placements == "CA"] = ca["is_present"]
            token_atoms["plddt"][placements == "CA"] = ca["plddt"]
            assert len(token_atoms) == atom_num
        else:
            if (
                res_type_override is not None
                and res_type_override[token_id]
                != const.token_ids[const.unk_token["PROTEIN"]]
            ):
                real_atoms = atoms_override
            elif res_type_override is not None:
                real_atoms = data.structure.atoms[
                    original_atom_idx : original_atom_idx + original_atom_num
                ]
            else:
                real_atoms = data.structure.atoms[
                    token["atom_idx"] : token["atom_idx"] + atom_num
                ]
            token_atoms = real_atoms
            fake_atoms = []
            fake_atom_mask.append([0] * atom_num)

        # Add atom ref data
        # element, charge, conformer, chirality
        token_atom_name = np.array([convert_atom_name(a["name"]) for a in token_atoms])
        token_atoms_element = []
        token_atoms_charge = []
        token_atoms_conformer = []
        token_atoms_chirality = []
        token_anchor_parent_idx = []
        for atom in token_atoms:
            if token["anchor_parent_idx"] == 1:
                # This token is a parent token of some anchor tokens
                if (
                    atom["coords"]
                    in data.tokens["center_coords"][data.tokens["is_anchor"] == 1]
                ):
                    token_anchor_parent_idx.append(1)
                else:
                    token_anchor_parent_idx.append(0)
            else:
                token_anchor_parent_idx.append(0)
            if const.fake_element in atom["name"]:
                token_atoms_element.append(const.fake_element_id)
                token_atoms_charge.append(0)
                token_atoms_conformer.append(token_atoms_conformer[1])  # CA coord
                token_atoms_chirality.append(unk_chirality)
            elif (
                token["is_anchor"] == 1
                and token["res_name"] == const.unk_token["PROTEIN"]
            ):
                # During Inference time when we give the model anchor specifications without a known residue
                token_atoms_element.append(const.element_to_atomic_num[atom["name"][0]])
                token_atoms_charge.append(charge_info[anchor_idx])
                token_atoms_conformer.append((0, 0, 0))  # CA coord
                token_atoms_chirality.append(unk_chirality)
                anchor_idx += 1
            elif (
                token["is_anchor"] == 1
                and token["res_name"] != const.unk_token["PROTEIN"]
            ):
                # During training timewhen we add anchor tokens from a given protein and know what residue comes from
                atom_ref = atom_name_to_ref[atom["name"]]
                token_atoms_element.append(atom_ref.GetAtomicNum())
                token_atoms_charge.append(atom_ref.GetFormalCharge())
                token_atoms_conformer.append((0, 0, 0))  # CA coord
                token_atoms_chirality.append(unk_chirality)
                anchor_idx += 1
            else:
                assert atom["name"] in atom_name_to_ref, (
                    f"Atom {atom['name']} not found in {atom_name_to_ref}"
                )
                atom_ref = atom_name_to_ref[atom["name"]]
                token_atoms_element.append(atom_ref.GetAtomicNum())
                token_atoms_charge.append(atom_ref.GetFormalCharge())
                token_atoms_conformer.append(
                    (
                        conformer.GetAtomPosition(atom_ref.GetIdx()).x,
                        conformer.GetAtomPosition(atom_ref.GetIdx()).y,
                        conformer.GetAtomPosition(atom_ref.GetIdx()).z,
                    )
                )
                token_atoms_chirality.append(
                    const.chirality_type_ids.get(
                        atom_ref.GetChiralTag().name, unk_chirality
                    )
                )
        token_atoms_element = np.array(token_atoms_element)
        token_atoms_charge = np.array(token_atoms_charge)
        token_atoms_conformer = np.array(token_atoms_conformer)
        token_atoms_chirality = np.array(token_atoms_chirality)
        token_anchor_parent_idx = np.array(token_anchor_parent_idx)
        # Map atoms to token indices
        ref_space_uid.extend([new_idx] * atom_num)
        atom_to_token.extend([token_id] * atom_num)

        # Add mol_type and num_atoms for token
        if add_affinity_cropping:
            atom_mol_type.append([token["mol_type"]] * token["atom_num"])
            token_mol_type.append([token["mol_type"]])
            token_to_num_atoms.append(token["atom_num"])

        # Map token to representative atom
        token_to_rep_atom.append(atom_idx + rep_atom_offset)
        if (chain["mol_type"] != const.chain_type_ids["NONPOLYMER"]) and token[
            "resolved_mask"
        ]:
            r_set_to_rep_atom.append(atom_idx + rep_atom_offset)

        if chain["mol_type"] == const.chain_type_ids["PROTEIN"]:
            backbone_index = [
                (
                    const.protein_backbone_atom_index[atom_name] + 1
                    if atom_name in const.protein_backbone_atom_index
                    else 0
                )
                for atom_name in token_atoms["name"]
            ]
        elif (
            chain["mol_type"] == const.chain_type_ids["DNA"]
            or chain["mol_type"] == const.chain_type_ids["RNA"]
        ):
            backbone_index = [
                (
                    const.nucleic_backbone_atom_index[atom_name]
                    + 1
                    + len(const.protein_backbone_atom_index)
                    if atom_name in const.nucleic_backbone_atom_index
                    else 0
                )
                for atom_name in token_atoms["name"]
            ]
        else:
            backbone_index = [0] * token["atom_num"]
        backbone_feat_index.extend(backbone_index)

        # Get token coordinates across sampled ensembles  and apply transforms
        token_coords_list = []
        for ensemble_atom_start in ensemble_atom_starts:
            if res_type_override is not None:
                coords_set = np.zeros((atom_num, 3), dtype=np.float32)
            else:
                coords_set = data.structure.coords[
                    ensemble_atom_start + token["atom_idx"] : ensemble_atom_start
                    + token["atom_idx"]
                    + atom_num
                ]["coords"]

            # Get coordinates of fake atoms
            if bool(token["design_mask"]) and atom14 and len(fake_atoms) > 0:
                coords_set = np.concatenate(
                    [
                        coords_set[: len(real_atoms)],
                        np.zeros((len(fake_atoms), 3), dtype=coords_set.dtype),
                    ]
                )
                if atom14_geometric:
                    o_coord = coords_set[const.ref_atoms[res_type].index("O")]
                    coords_set[placements == "O"] = o_coord
                    n_coord = coords_set[const.ref_atoms[res_type].index("N")]
                    coords_set[placements == "N"] = n_coord
                else:
                    rep_atom_coord = coords_set[rep_atom_offset]
                    coords_set[len(real_atoms) :] = np.repeat(
                        rep_atom_coord[None, :], len(fake_atoms), axis=0
                    )
            elif bool(token["design_mask"]) and atom37 and len(fake_atoms) > 0:
                ca_coord = coords_set[const.ref_atoms[res_type].index("CA")]
                coords_set = np.array(
                    [
                        coords_set[const.ref_atoms[res_type].index(name)]
                        if name in const.ref_atoms[res_type]
                        else ca_coord
                        for name in const.atom_types
                    ]
                )
            token_coords_list.append(coords_set)

        token_coords = np.array(token_coords_list)
        coord_data.append(token_coords)

        # Get frame data
        res_type = const.tokens[token["res_type"]]
        res_name = str(token["res_name"])

        if (
            token["is_anchor"] == 1
            or atom_num < 3
            or res_type in ["PAD", "UNK", "-", "DN", "N"]
        ):
            idx_frame_a, idx_frame_b, idx_frame_c, idx_frame_d = 0, 0, 0, 0
            mask_frame = False
        elif (token["mol_type"] == const.chain_type_ids["PROTEIN"]) and (
            res_name in const.ref_atoms
        ):
            idx_frame_a, idx_frame_b, idx_frame_c, idx_frame_d = (
                const.ref_atoms[res_name].index("N"),
                const.ref_atoms[res_name].index("CA"),
                const.ref_atoms[res_name].index("C"),
                const.ref_atoms[res_name].index("O"),
            )
            mask_frame = (
                token_atoms["is_present"][idx_frame_a]
                and token_atoms["is_present"][idx_frame_b]
                and token_atoms["is_present"][idx_frame_c]
            )
        elif (
            token["mol_type"] == const.chain_type_ids["DNA"]
            or token["mol_type"] == const.chain_type_ids["RNA"]
        ) and (res_name in const.ref_atoms):
            idx_frame_a, idx_frame_b, idx_frame_c, idx_frame_d = (
                const.ref_atoms[res_name].index("C1'"),
                const.ref_atoms[res_name].index("C3'"),
                const.ref_atoms[res_name].index("C4'"),
                const.ref_atoms[res_name].index("P"),
            )
            mask_frame = (
                token_atoms["is_present"][idx_frame_a]
                and token_atoms["is_present"][idx_frame_b]
                and token_atoms["is_present"][idx_frame_c]
            )
        elif token["mol_type"] == const.chain_type_ids["PROTEIN"]:
            # Try to look for the atom nams in the modified residue
            is_ca = token_atoms["name"] == "CA"
            idx_frame_a = is_ca.argmax()
            ca_present = (
                token_atoms[idx_frame_a]["is_present"] if is_ca.any() else False
            )

            is_n = token_atoms["name"] == "N"
            idx_frame_b = is_n.argmax()
            n_present = token_atoms[idx_frame_b]["is_present"] if is_n.any() else False

            is_c = token_atoms["name"] == "C"
            idx_frame_c = is_c.argmax()
            c_present = token_atoms[idx_frame_c]["is_present"] if is_c.any() else False
            mask_frame = ca_present and n_present and c_present

        elif (token["mol_type"] == const.chain_type_ids["DNA"]) or (
            token["mol_type"] == const.chain_type_ids["RNA"]
        ):
            # Try to look for the atom nams in the modified residue
            is_c1 = token_atoms["name"] == "C1'"
            idx_frame_a = is_c1.argmax()
            c1_present = (
                token_atoms[idx_frame_a]["is_present"] if is_c1.any() else False
            )

            is_c3 = token_atoms["name"] == "C3'"
            idx_frame_b = is_c3.argmax()
            c3_present = (
                token_atoms[idx_frame_b]["is_present"] if is_c3.any() else False
            )

            is_c4 = token_atoms["name"] == "C4'"
            idx_frame_c = is_c4.argmax()
            c4_present = (
                token_atoms[idx_frame_c]["is_present"] if is_c4.any() else False
            )
            mask_frame = c1_present and c3_present and c4_present
        else:
            idx_frame_a, idx_frame_b, idx_frame_c, idx_frame_d = 0, 0, 0, 0
            mask_frame = False
        frame_data.append(
            [
                idx_frame_a + atom_idx,
                idx_frame_b + atom_idx,
                idx_frame_c + atom_idx,
            ]
        )
        token_to_bb4_atoms.append(
            [
                idx_frame_a + atom_idx,
                idx_frame_b + atom_idx,
                idx_frame_c + atom_idx,
                idx_frame_d + atom_idx,
            ]
        )
        resolved_frame_data.append(mask_frame)

        # Get distogram coordinates
        if res_type_override is not None:
            disto_coords_ensemble_tok = np.zeros((1, 3), dtype=np.float32)
        else:
            disto_coords_ensemble_tok = data.structure.coords[
                e_offsets + token["atom_idx"] + rep_atom_offset
            ]["coords"]  # (num ensembles, 3)
        disto_coords_ensemble.append(disto_coords_ensemble_tok)

        # produce masked_ref_atom_name_chars
        for i in range(atom_num):
            masked_ref_atom_name_chars.append(
                convert_atom_name(const.mask_element + str(i))
            )

        # Update atom data. This is technically never used again (we rely on coord_data),
        # but we update for consistency and to make sure the Atom object has valid, transformed coordinates.
        token_atoms = token_atoms.copy()
        # atom has a copy of first coords in ensemble
        token_atoms["coords"] = token_coords[0]

        atom_data.append(token_atoms)
        atom_name.append(token_atom_name)
        atom_element.append(token_atoms_element)
        atom_charge.append(token_atoms_charge)
        atom_conformer.append(token_atoms_conformer)
        atom_chirality.append(token_atoms_chirality)
        anchor_parent_idx.append(token_anchor_parent_idx)
        new_to_old_atomidx.extend(
            range(token["atom_idx"], token["atom_idx"] + len(real_atoms))
        )
        new_to_old_atomidx.extend([-1] * len(fake_atoms))
        atom_idx += len(token_atoms)

    backbone_mask = np.concatenate(backbone_mask)
    fake_atom_mask = np.concatenate(fake_atom_mask)
    disto_coords_ensemble = np.array(disto_coords_ensemble)
    new_to_old_atomidx = np.array(new_to_old_atomidx)
    # Compute ensemble distogram
    L = len(data.tokens)

    if disto_use_ensemble:
        # Use all available structures to create distogram
        idx_list = range(disto_coords_ensemble.shape[1])
    else:
        # Only use a sampled structures to create distogram
        idx_list = ensemble_features["ensemble_ref_idxs"]

    # Create distogram
    if not inverse_fold:
        disto_target = torch.zeros(L, L, len(idx_list), num_bins)
        for i, e_idx in enumerate(idx_list):
            t_center = torch.Tensor(disto_coords_ensemble[:, e_idx, :])
            t_dists = torch.cdist(t_center, t_center)
            boundaries = torch.linspace(min_dist, max_dist, num_bins - 1)
            distogram = (t_dists.unsqueeze(-1) > boundaries).sum(dim=-1).long()
            disto_target[:, :, i, :] = one_hot(distogram, num_classes=num_bins).bool()
    else:
        disto_target = None

    # Normalize distogram
    atom_data = np.concatenate(atom_data)
    atom_name = np.concatenate(atom_name)
    atom_element = np.concatenate(atom_element)
    atom_charge = np.concatenate(atom_charge)
    atom_conformer = np.concatenate(atom_conformer)
    atom_chirality = np.concatenate(atom_chirality)
    anchor_parent_idx = np.concatenate(anchor_parent_idx)
    coord_data = np.concatenate(coord_data, axis=1)
    ref_space_uid = np.array(ref_space_uid)

    # Compute features
    disto_coords_ensemble = from_numpy(disto_coords_ensemble)
    disto_coords_ensemble = disto_coords_ensemble[
        :, ensemble_features["ensemble_ref_idxs"]
    ].permute(1, 0, 2)
    backbone_feat_index = from_numpy(np.asarray(backbone_feat_index)).long()
    ref_atom_name_chars = from_numpy(atom_name).long()
    masked_ref_atom_name_chars = torch.tensor(masked_ref_atom_name_chars).long()
    ref_element = from_numpy(atom_element).long()
    ref_charge = from_numpy(atom_charge).float()
    ref_pos = from_numpy(atom_conformer).float()
    ref_space_uid = from_numpy(ref_space_uid)
    ref_chirality = from_numpy(atom_chirality).long()
    anchor_parent_idx = from_numpy(anchor_parent_idx)
    coords = from_numpy(coord_data.copy())
    resolved_mask = from_numpy(atom_data["is_present"])
    backbone_mask = from_numpy(backbone_mask)
    fake_atom_mask = from_numpy(fake_atom_mask)
    pad_mask = torch.ones(len(atom_data), dtype=torch.float)
    atom_to_token = torch.tensor(atom_to_token, dtype=torch.long)
    token_to_rep_atom = torch.tensor(token_to_rep_atom, dtype=torch.long)
    r_set_to_rep_atom = torch.tensor(r_set_to_rep_atom, dtype=torch.long)
    token_to_bb4_atoms = torch.tensor(token_to_bb4_atoms, dtype=torch.long)
    new_to_old_atomidx = torch.tensor(new_to_old_atomidx, dtype=torch.long)
    bfactor = from_numpy(atom_data["bfactor"].copy())
    plddt = from_numpy(atom_data["plddt"].copy())
    if override_bfactor:
        bfactor = bfactor * 0.0

    if bfactor_md_correction and data.record.structure.method.lower() == "md":
        # MD bfactor was computed as RMSF
        # Convert to b-factor
        bfactor = 8 * (np.pi**2) * (bfactor**2)

    if add_affinity_cropping:
        affinity_token_to_atom = torch.tensor(affinity_token_to_atom, dtype=torch.long)
        atomized_atom_mask = torch.tensor(atomized_atom_mask, dtype=torch.float)
        pad_affinity_token_mask = torch.ones(
            len(affinity_token_to_atom), dtype=torch.float
        )

    # We compute frames within ensemble
    if compute_frames:
        frames = []
        frame_resolved_mask = []
        for i in range(coord_data.shape[0]):
            frame_data_, resolved_frame_data_ = compute_frames_nonpolymer(
                data,
                coord_data[i],
                atom_data["is_present"],
                atom_to_token,
                frame_data,
                resolved_frame_data,
            )  # Compute frames for NONPOLYMER tokens
            frames.append(frame_data_.copy())
            frame_resolved_mask.append(resolved_frame_data_.copy())
        frames = from_numpy(np.stack(frames))  # (N_ENS, N_TOK, 3)
        frame_resolved_mask = from_numpy(np.stack(frame_resolved_mask))

    # Convert to one-hot
    backbone_feat_index = one_hot(
        backbone_feat_index,
        num_classes=1
        + len(const.protein_backbone_atom_index)
        + len(const.nucleic_backbone_atom_index),
    )
    ref_atom_name_chars = one_hot(ref_atom_name_chars, num_classes=64)
    masked_ref_atom_name_chars = one_hot(masked_ref_atom_name_chars, num_classes=64)
    ref_element = one_hot(ref_element, num_classes=const.num_elements)
    atom_to_token = one_hot(atom_to_token, num_classes=token_id + 1).bool()
    token_to_rep_atom = one_hot(token_to_rep_atom, num_classes=len(atom_data)).bool()
    r_set_to_rep_atom = one_hot(r_set_to_rep_atom, num_classes=len(atom_data)).bool()
    token_to_bb4_atoms = one_hot(token_to_bb4_atoms, num_classes=len(atom_data)).bool()
    if add_affinity_cropping:
        affinity_token_to_atom = one_hot(
            affinity_token_to_atom, num_classes=len(atom_data)
        )

    # Center the ground truth coordinates
    center = (coords * resolved_mask[None, :, None]).sum(dim=1)
    center = center / resolved_mask.sum().clamp(min=1)
    coords = coords - center[:, None]

    if isinstance(override_coords, Tensor):
        coords = override_coords.unsqueeze(0)

    # Apply random roto-translation to the input conformers
    for i in range(torch.max(ref_space_uid)):
        included = ref_space_uid == i
        if torch.sum(included) > 0 and torch.any(resolved_mask[included]):
            ref_pos[included] = center_random_augmentation(
                ref_pos[included][None], resolved_mask[included][None], centering=True
            )[0]

    # Compute padding and apply
    if max_atoms is None or (atom14 or atom14_geometric or atom37):
        pad_len = (
            (len(atom_data) - 1) // atoms_per_window_queries + 1
        ) * atoms_per_window_queries - len(atom_data)
    else:
        assert max_atoms % atoms_per_window_queries == 0
        pad_len = max_atoms - len(atom_data)
        if pad_len < 0:
            # make sure len(atom_data) can be divided by atoms_per_window_queries
            pad_len = (
                len(atom_data) // atoms_per_window_queries + 1
            ) * atoms_per_window_queries - len(atom_data)

    if pad_len > 0:
        pad_mask = pad_dim(pad_mask, 0, pad_len)
        ref_pos = pad_dim(ref_pos, 0, pad_len)
        resolved_mask = pad_dim(resolved_mask, 0, pad_len)
        ref_atom_name_chars = pad_dim(ref_atom_name_chars, 0, pad_len)
        masked_ref_atom_name_chars = pad_dim(masked_ref_atom_name_chars, 0, pad_len)
        backbone_mask = pad_dim(backbone_mask, 0, pad_len)
        fake_atom_mask = pad_dim(fake_atom_mask, 0, pad_len)
        ref_element = pad_dim(ref_element, 0, pad_len)
        ref_charge = pad_dim(ref_charge, 0, pad_len)
        ref_chirality = pad_dim(ref_chirality, 0, pad_len)
        anchor_parent_idx = pad_dim(anchor_parent_idx, 0, pad_len)
        backbone_feat_index = pad_dim(backbone_feat_index, 0, pad_len)
        ref_space_uid = pad_dim(ref_space_uid, 0, pad_len)
        coords = pad_dim(coords, 1, pad_len)
        atom_to_token = pad_dim(atom_to_token, 0, pad_len)
        new_to_old_atomidx = pad_dim(new_to_old_atomidx, 0, pad_len)
        token_to_rep_atom = pad_dim(token_to_rep_atom, 1, pad_len)
        r_set_to_rep_atom = pad_dim(r_set_to_rep_atom, 1, pad_len)
        bfactor = pad_dim(bfactor, 0, pad_len)
        plddt = pad_dim(plddt, 0, pad_len)
        token_to_bb4_atoms = pad_dim(token_to_bb4_atoms, 2, pad_len)

    if max_tokens is not None:
        pad_len = max_tokens - token_to_rep_atom.shape[0]
        if pad_len > 0:
            atom_to_token = pad_dim(atom_to_token, 1, pad_len)
            token_to_rep_atom = pad_dim(token_to_rep_atom, 0, pad_len)
            r_set_to_rep_atom = pad_dim(r_set_to_rep_atom, 0, pad_len)
            token_to_bb4_atoms = pad_dim(token_to_bb4_atoms, 0, pad_len)
            if not inverse_fold:
                disto_target = pad_dim(pad_dim(disto_target, 0, pad_len), 1, pad_len)
            disto_coords_ensemble = pad_dim(disto_coords_ensemble, 1, pad_len)
            new_to_old_atomidx = pad_dim(new_to_old_atomidx, 0, pad_len)

            if compute_frames:
                frames = pad_dim(frames, 1, pad_len)
                frame_resolved_mask = pad_dim(frame_resolved_mask, 1, pad_len)
    extra_mols = {k: v for k, v in molecules.items() if len(k) >= 4}
    atom_features = {
        "ref_pos": ref_pos,
        "atom_resolved_mask": resolved_mask,
        "ref_atom_name_chars": ref_atom_name_chars,
        "ref_element": ref_element,
        "ref_charge": ref_charge,
        "ref_chirality": ref_chirality,
        "anchor_parent_idx": anchor_parent_idx,
        "atom_backbone_feat": backbone_feat_index,
        "ref_space_uid": ref_space_uid,
        "coords": coords,
        "atom_pad_mask": pad_mask,
        "atom_to_token": atom_to_token,
        "new_to_old_atomidx": new_to_old_atomidx,
        "token_to_rep_atom": token_to_rep_atom,
        "r_set_to_rep_atom": r_set_to_rep_atom,
        "disto_target": disto_target,
        "disto_coords_ensemble": disto_coords_ensemble,
        "bfactor": bfactor,
        "plddt": plddt,
        "masked_ref_atom_name_chars": masked_ref_atom_name_chars,
        "backbone_mask": backbone_mask,
        "fake_atom_mask": fake_atom_mask,
        "token_to_bb4_atoms": token_to_bb4_atoms,
        "extra_mols": extra_mols,
    }

    if compute_frames:
        atom_features["frames_idx"] = frames
        atom_features["frame_resolved_mask"] = frame_resolved_mask

    if add_affinity_cropping:
        if max_atoms is not None:
            pad_len = max_atoms - affinity_token_to_atom.shape[1]
            if pad_len > 0:
                affinity_token_to_atom = pad_dim(affinity_token_to_atom, 1, pad_len)
                atomized_atom_mask = pad_dim(atomized_atom_mask, 0, pad_len)

        if max_tokens_affinity is not None:
            pad_len = max_tokens_affinity - affinity_token_to_atom.shape[0]
            if pad_len > 0:
                affinity_token_to_atom = pad_dim(affinity_token_to_atom, 0, pad_len)
                pad_affinity_token_mask = pad_dim(pad_affinity_token_mask, 0, pad_len)
        atom_features["affinity_token_to_atom"] = affinity_token_to_atom
        atom_features["affinity_token_pad_mask"] = pad_affinity_token_mask
        atom_features["atomized_atom_mask"] = atomized_atom_mask

    return atom_features


def process_msa_features(
    data: Input,
    random: np.random.Generator,
    max_seqs_batch: int,
    max_seqs: int,
    max_tokens: Optional[int] = None,
    pad_to_max_seqs: bool = False,
    msa_sampling: bool = False,
    affinity: bool = False,
) -> Dict[str, Tensor]:
    """Get the MSA features.

    Parameters
    ----------
    data : Input
        The input to the model.
    random : np.random.Generator
        The random number generator.
    max_seqs : int
        The maximum number of MSA sequences.
    max_tokens : int
        The maximum number of tokens.
    pad_to_max_seqs : bool
        Whether to pad to the maximum number of sequences.
    msa_sampling : bool
        Whether to sample the MSA.

    Returns
    -------
    Dict[str, Tensor]
        The MSA features.

    """
    # Created paired MSA
    msa, deletion, paired = construct_paired_msa(
        data=data,
        random=random,
        max_seqs=max_seqs_batch,
        random_subset=msa_sampling,
    )

    msa, deletion, paired = (
        msa.transpose(1, 0),
        deletion.transpose(1, 0),
        paired.transpose(1, 0),
    )  # (N_MSA, N_RES, N_AA)

    # Prepare features
    assert torch.all(msa >= 0) and torch.all(msa < const.num_tokens)
    msa_one_hot = torch.nn.functional.one_hot(msa, num_classes=const.num_tokens)
    msa_mask = torch.ones_like(msa)
    profile = msa_one_hot.float().mean(dim=0)
    has_deletion = deletion > 0
    deletion = np.pi / 2 * np.arctan(deletion / 3)
    deletion_mean = deletion.mean(axis=0)

    # Pad in the MSA dimension (dim=0)
    if pad_to_max_seqs:
        pad_len = max_seqs - msa.shape[0]
        if pad_len > 0:
            msa = pad_dim(msa, 0, pad_len, const.token_ids["-"])
            paired = pad_dim(paired, 0, pad_len)
            msa_mask = pad_dim(msa_mask, 0, pad_len)
            has_deletion = pad_dim(has_deletion, 0, pad_len)
            deletion = pad_dim(deletion, 0, pad_len)

    # Pad in the token dimension (dim=1)
    if max_tokens is not None:
        pad_len = max_tokens - msa.shape[1]
        if pad_len > 0:
            msa = pad_dim(msa, 1, pad_len, const.token_ids["-"])
            paired = pad_dim(paired, 1, pad_len)
            msa_mask = pad_dim(msa_mask, 1, pad_len)
            has_deletion = pad_dim(has_deletion, 1, pad_len)
            deletion = pad_dim(deletion, 1, pad_len)
            profile = pad_dim(profile, 0, pad_len)
            deletion_mean = pad_dim(deletion_mean, 0, pad_len)

    if affinity:
        return {
            "deletion_mean_affinity": deletion_mean,
            "profile_affinity": profile,
        }
    else:
        return {
            "msa": msa,
            "msa_paired": paired,
            "deletion_value": deletion,
            "has_deletion": has_deletion,
            "deletion_mean": deletion_mean,
            "profile": profile,
            "msa_mask": msa_mask,
        }


def process_template_features(
    data: Input,
    max_tokens: int,
    max_templates: int = 1,
    pad_to_max_templates: bool = False,
) -> Dict[str, Tensor]:
    """Get the template features.

    Parameters
    ----------
    data : Input
        The input to the model.
    max_tokens : int
        The maximum number of tokens.
    max_templates : int
        The maximum number of templates.
    pad_to_max_templates : bool
        Whether to pad to the maximum number of templates.

    Returns
    -------
    Dict[str, Tensor]
        The template features.

    """
    # Get feature dimensions
    one_hot = one_hot_torch
    tdim = (
        max([1] + [len(v) for v in data.templates.values()])
        if data.templates is not None
        else 1
    )
    tdim = tdim if not pad_to_max_templates else max_templates
    num_tokens = len(data.tokens) if max_tokens is None else max_tokens

    # Allocate features
    res_type = np.zeros((tdim, num_tokens), dtype=np.int64)
    frame_rot = np.zeros((tdim, num_tokens, 3, 3), dtype=np.float32)
    frame_t = np.zeros((tdim, num_tokens, 3), dtype=np.float32)
    cb_coords = np.zeros((tdim, num_tokens, 3), dtype=np.float32)
    ca_coords = np.zeros((tdim, num_tokens, 3), dtype=np.float32)
    frame_mask = np.zeros((tdim, num_tokens), dtype=np.float32)
    cb_mask = np.zeros((tdim, num_tokens), dtype=np.float32)
    template_mask = np.zeros((tdim, num_tokens), dtype=np.float32)

    # Now create features per token
    if data.templates is not None:
        for tok_idx, token in enumerate(data.tokens):
            # Check if chain has templates
            chain_id = int(token["asym_id"])
            if chain_id not in data.templates:
                continue

            # Add per template features
            for temp_idx, template in enumerate(data.templates[chain_id]):
                t_data = template.coordinates
                idx = np.where(t_data["res_idx"] == token["res_idx"])[0]
                if len(idx) == 0:
                    continue

                # Add template features
                res_type[temp_idx, tok_idx] = t_data["res_type"][idx[0]]
                frame_rot[temp_idx, tok_idx] = t_data["frame_rot"][idx[0]].reshape(3, 3)
                frame_t[temp_idx, tok_idx] = t_data["frame_t"][idx[0]]
                cb_coords[temp_idx, tok_idx] = t_data["coords_cb"][idx[0]]
                ca_coords[temp_idx, tok_idx] = t_data["coords_ca"][idx[0]]
                cb_mask[temp_idx, tok_idx] = t_data["mask_cb"][idx[0]]
                frame_mask[temp_idx, tok_idx] = t_data["mask_frame"][idx[0]]
                template_mask[temp_idx, tok_idx] = 1

    # Convert to one-hot
    res_type = torch.from_numpy(res_type)
    res_type = one_hot(res_type, num_classes=const.num_tokens)

    return {
        "template_restype": res_type,
        "template_frame_rot": torch.from_numpy(frame_rot),
        "template_frame_t": torch.from_numpy(frame_t),
        "template_cb": torch.from_numpy(cb_coords),
        "template_ca": torch.from_numpy(ca_coords),
        "template_mask_cb": torch.from_numpy(cb_mask),
        "template_mask_frame": torch.from_numpy(frame_mask),
        "template_mask": torch.from_numpy(template_mask),
    }


def process_symmetry_features(
    cropped: Input,
    symmetries: Dict,
    backbone_only: bool = False,
    atom14: bool = False,
    atom14_geometric: bool = False,
    atom37: bool = False,
) -> Dict[str, Tensor]:
    """Get the symmetry features.

    Parameters
    ----------
    data : Input
        The input to the model.

    Returns
    -------
    Dict[str, Tensor]
        The symmetry features.

    """
    # TODO this does not work with multiple conformers
    features = get_chain_symmetries(
        cropped, backbone_only, atom14, atom14_geometric, atom37
    )
    features.update(get_amino_acids_symmetries(cropped, backbone_only, atom14, atom37))
    features.update(get_ligand_symmetries(cropped, symmetries))

    return features


def process_ensemble_features(
    data: Input,
    random: np.random.Generator,
    num_ensembles: int,
    ensemble_sample_replacement: bool,
    fix_single_ensemble: bool,
) -> Dict[str, Tensor]:
    """Get the ensemble features.

    Parameters
    ----------
    data : Input
        The input to the model.
    random : np.random.Generator
        The random number generator.
    num_ensembles : int
        The maximum number of ensembles to sample.
    ensemble_sample_replacement : bool
        Whether to sample with replacement.

    Returns
    -------
    Dict[str, Tensor]
        The ensemble features.

    """
    assert num_ensembles > 0, "Number of conformers sampled must be greater than 0."

    # Number of available conformers in the structure
    # s_ensemble_num = min(len(cropped.structure.ensemble), 24)  # Limit to 24 conformers DEBUG: TODO: remove !
    s_ensemble_num = len(data.structure.ensemble)

    if fix_single_ensemble:
        # Always take the first conformer for train and validation
        assert num_ensembles == 1, (
            "Number of conformers sampled must be 1 with fix_single_ensemble=True."
        )
        ensemble_ref_idxs = np.array([0])
    else:
        if ensemble_sample_replacement:
            # Used in training
            ensemble_ref_idxs = random.integers(0, s_ensemble_num, (num_ensembles,))
        else:
            # Used in validation
            if s_ensemble_num < num_ensembles:
                # Take all available conformers
                ensemble_ref_idxs = np.arange(0, s_ensemble_num)
            else:
                # Sample without replacement
                ensemble_ref_idxs = random.choice(
                    s_ensemble_num, num_ensembles, replace=False
                )

    ensemble_features = {
        "ensemble_ref_idxs": torch.Tensor(ensemble_ref_idxs).long(),
    }

    return ensemble_features


def repopulate_res_type(
    feat,
    mol_types: List[str] = ["PROTEIN", "DNA", "RNA"],
    tokens: List[str] = ["GLY", "DC", "C"],
    repopulate_non_standards: bool = False,
    design_only: bool = True,
):
    feat = copy.deepcopy(feat)
    if isinstance(mol_types, str):
        mol_types = [mol_types]
    if isinstance(tokens, str):
        tokens = [tokens]

    for mol_type, token in zip(mol_types, tokens):
        if repopulate_non_standards:
            mask = feat["mol_type"] == const.chain_type_ids[mol_type]
        else:
            mask = (feat["mol_type"] == const.chain_type_ids[mol_type]) & feat[
                "is_standard"
            ].bool()

        if design_only:
            mask &= feat["design_mask"].bool()
        name = convert_atom_name(token)
        name = torch.tensor(name).to(feat["res_type"])
        ccd_name = convert_ccd(token)
        ccd_name = torch.tensor(ccd_name).to(feat["res_type"])
        old = feat["res_type"][mask]
        new = torch.zeros_like(old)
        new[..., const.token_ids[token]] = 1
        feat["res_type"][mask] = new
        feat["ccd"][mask] = ccd_name
    return feat


def res_from_atom14(
    feat: Dict[str, Tensor],
    threshold: float = 0.5,
    invalid_token: str = "UNK",
):
    """Returns a new dict of features with residue types inferred from atom14 coords geometry. Always updates res_type, ref_atom_name_chars_full, and ref_element. If num_bins is not None, then ref_atom_name_name_chars are also updated.

    Parameters
    ----------
    feat : Dict[str, Tensor]
        The features.

    feat : float
        The value up to which coordinates are still counted as belonging to a backbone atom and therefore contributing to its count.

    num_bins: int
        If this is not None, then ref_atom_name_chars are also updated.

    Returns
    -------
    Dict[str, Tensor]
        The ensemble features.
    """
    one_hot = one_hot_torch
    feat = copy.deepcopy(feat)

    # Only attempt residue-type inference on designed protein residues
    design_mask = (
        feat["design_mask"].bool()
        & (feat["mol_type"] == const.chain_type_ids["PROTEIN"]).bool()
    )
    if design_mask.sum() == 0:
        return feat

    # Get designed atom coordinates in shape N//14 x 14 x 3
    atom_to_token = torch.argmax(feat["atom_to_token"].int(), dim=-1)
    token_indices = feat["token_index"][design_mask]
    atom_design_mask = torch.isin(atom_to_token, token_indices)
    atom_design_mask = atom_design_mask & feat["atom_pad_mask"].bool()
    design_coords = feat["coords"][atom_design_mask].clone()
    design_coords = design_coords.view(len(design_coords) // 14, 14, 3)

    # For each sidechain atom, compute closest backbone atom and count them
    # while excluding those side chain atoms whose distance is above a threshold
    distances = torch.cdist(design_coords[:, :4], design_coords[:, 4:])
    value, argmin = torch.min(distances, dim=1)
    argmin[value > threshold] = -1
    arange = torch.arange(len(const.ref_atoms["GLY"]), device=argmin.device)
    counts = (argmin[:, :, None] == arange[None, None, :]).sum(1).long()

    # update res_type and name
    # counts has shape N x 4, now we extract which combination of counts corresponds to which restype
    # Invalid counts will be turned into UNK. Note that there also exists a valid count that encodes UNK.
    res_type_letters = [
        const.placement_count_to_token.get(tuple(count.tolist()), invalid_token)
        for count in counts
    ]
    res_type = [const.token_ids[ttype] for ttype in res_type_letters]
    res_type = torch.tensor(res_type).to(feat["res_type"])
    feat["res_type"][design_mask] = one_hot(res_type, len(const.tokens))
    ccds = [convert_ccd(res_name) for res_name in res_type_letters]
    ccds = torch.tensor(ccds).to(feat["res_type"])
    feat["ccd"][design_mask] = ccds

    # update ref_element, and ref_atom_name_chars, ref_charge,
    design_names = feat["ref_atom_name_chars"][atom_design_mask]
    design_elements = feat["ref_element"][atom_design_mask]
    for design_token_index, res_type_letter in enumerate(res_type_letters):
        for atom14_idx, atom_name in enumerate(const.ref_atoms[res_type_letter]):
            design_atom_idx = design_token_index * 14 + atom14_idx

            name = convert_atom_name(atom_name)
            name = torch.tensor(name).to(feat["ref_element"])
            design_names[design_atom_idx] = one_hot(name, num_classes=64)
            design_elements[design_atom_idx] = one_hot(
                torch.tensor(
                    const.element_to_atomic_num[
                        elem_from_name(atom_name, res_type_letter)
                    ]
                ),
                num_classes=const.num_elements,
            ).to(feat["ref_element"])
    feat["ref_atom_name_chars"][atom_design_mask] = design_names
    feat["ref_element"][atom_design_mask] = design_elements
    return feat


def res_from_atom37(
    feat: Dict[str, Tensor],
    threshold: float = 0.5,
    invalid_token: str = "UNK",
):
    """Returns a new dict of features with residue types inferred from atom37 coords geometry. Always updates res_type, ref_atom_name_chars_full, and ref_element. If num_bins is not None, then ref_atom_name_name_chars are also updated.

    Parameters
    ----------
    feat : Dict[str, Tensor]
        The features.

    feat : float
        The value up to which coordinates are still counted as belonging to a backbone atom and therefore contributing to its count.

    num_bins: int
        If this is not None, then ref_atom_name_chars are also updated.

    Returns
    -------
    Dict[str, Tensor]
        The ensemble features.
    """
    one_hot = one_hot_torch
    feat = copy.deepcopy(feat)

    # A residue being True in the design mask implies
    # that the residue is a standard residue and has moltype PROTEIN
    design_mask = feat["design_mask"].bool()
    if design_mask.sum() == 0:
        return feat

    # Get designed atom coordinates in shape N//37 x 37 x 3
    atom_to_token = torch.argmax(feat["atom_to_token"], dim=-1)
    token_indices = feat["token_index"][design_mask]
    atom_design_mask = torch.isin(atom_to_token, token_indices)
    atom_design_mask = atom_design_mask & feat["atom_pad_mask"].bool()
    design_coords = feat["coords"][atom_design_mask].clone()
    design_coords = design_coords.view(len(design_coords) // 37, 37, 3)

    # For each sidechain atom, compute closest backbone atom and count them
    # while excluding those side chain atoms whose distance is above a threshold
    distances = torch.cdist(design_coords[:, :4], design_coords[:, 4:])
    value, argmin = torch.min(distances, dim=1)
    argmin[value > threshold] = -1
    ca_placements = (argmin[:, :] == const.atom_types.index("CA")).long()

    res_names = [res for res in list(const.fake_atom_placements.keys()) if res != "UNK"]
    atom37_mask = torch.stack(
        [
            torch.tensor(
                [
                    1 if name in const.ref_atoms[res_type] else 0
                    for name in const.atom_types[4:]
                ],
                device=ca_placements.device,
            )
            for res_type in res_names
        ]
    )

    B = ca_placements.shape[0]
    atom37_comparison = torch.abs(
        ca_placements.unsqueeze(1).expand(-1, 20, -1)
        - (1 - atom37_mask.unsqueeze(0).expand(B, -1, -1))
    ).sum(axis=-1)
    decoded_idxs, res_idxs = np.where(atom37_comparison.cpu().numpy() == 0)
    _, n_decoded_res = np.unique(decoded_idxs, return_counts=True)
    if len(n_decoded_res) > 0:
        assert np.max(n_decoded_res) <= 1, "Ambiguous residue!"
    res_type_letters = ["UNK"] * B
    for i, res_i in zip(decoded_idxs, res_idxs):
        res_type_letters[i] = res_names[res_i]

    res_type = [const.token_ids[ttype] for ttype in res_type_letters]
    res_type = torch.tensor(res_type).to(feat["res_type"])
    feat["res_type"][design_mask] = one_hot(res_type, len(const.tokens))
    ccds = [convert_ccd(res_name) for res_name in res_type_letters]
    ccds = torch.tensor(ccds).to(feat["res_type"])
    feat["ccd"][design_mask] = ccds

    # update ref_element, and ref_atom_name_chars, ref_charge,
    design_names = feat["ref_atom_name_chars"][atom_design_mask]
    design_elements = feat["ref_element"][atom_design_mask]
    for design_token_index, res_type_letter in enumerate(res_type_letters):
        for atom14_idx, atom_name in enumerate(const.ref_atoms[res_type_letter]):
            atom37_idx = const.atom_types.index(atom_name)
            design_atom_idx = design_token_index * 37 + atom37_idx

            name = convert_atom_name(atom_name)
            name = torch.tensor(name).to(feat["ref_atom_name_chars"])
            design_names[design_atom_idx] = one_hot(name, num_classes=64)
            design_elements[design_atom_idx] = one_hot(
                torch.tensor(
                    const.element_to_atomic_num[
                        elem_from_name(atom_name, res_type_letter)
                    ]
                ),
                num_classes=const.num_elements,
            )
    feat["ref_atom_name_chars"][atom_design_mask] = design_names
    feat["ref_element"][atom_design_mask] = design_elements
    return feat


def find_token_idx_for_atom(data: Input, atom_idx: int) -> int:
    """Find the token_idx that contains the given atom index.

    Parameters
    ----------
    data : Input
        The input data containing tokens and structure information
    atom_idx : int
        The atom index to find the corresponding token for

    Returns
    -------
    int
        The token_idx that contains the given atom index
    """
    # Iterate through tokens to find which one contains this atom
    for token in data.tokens:
        if token["atom_idx"] <= atom_idx < token["atom_idx"] + token["atom_num"]:
            return token["token_idx"]

    raise ValueError(f"No token found containing atom index {atom_idx}")


class AF3Featurizer:
    """AF3-like featurizer for model training."""

    def process(
        self,
        data: Input,
        random: np.random.Generator,
        molecules: Dict[str, Mol],
        training: bool,
        max_seqs: int,
        backbone_only: bool = False,
        atom14: bool = False,
        atom14_geometric: bool = False,
        atom37: bool = False,
        design: bool = False,
        atoms_per_window_queries: int = 32,
        min_dist: float = 2.0,
        max_dist: float = 22.0,
        num_bins: int = 64,
        num_ensembles: int = 1,
        ensemble_sample_replacement: bool = False,
        disto_use_ensemble: Optional[bool] = False,
        fix_single_ensemble: Optional[bool] = True,
        max_tokens: Optional[int] = None,
        max_atoms: Optional[int] = None,
        pad_to_max_seqs: bool = False,
        compute_symmetries: bool = False,
        binder_pocket_conditioned_prop: Optional[float] = 0.0,
        contact_conditioned_prop: Optional[float] = 0.0,
        binder_pocket_cutoff_min: Optional[float] = 4.0,
        binder_pocket_cutoff_max: Optional[float] = 20.0,
        binder_pocket_sampling_geometric_p: Optional[float] = 0.0,
        disulfide_prob: Optional[float] = 1.0,
        disulfide_on: Optional[bool] = False,
        noisy_coords_std: Optional[float] = 0.0,
        only_ligand_binder_pocket: Optional[bool] = False,
        only_pp_contact: Optional[bool] = False,
        maximum_bond_distance: Optional[int] = 0,
        compute_affinity: Optional[bool] = False,
        has_structure_label: bool = True,
        has_affinity_label: bool = False,
        add_affinity_cropping: Optional[bool] = False,
        max_tokens_affinity: int = 384,
        max_tokens_to_atomize_affinity: int = 32,
        max_atoms_to_atomize_affinity: int = 128,
        affinity_only_atoms: bool = False,
        single_sequence_prop: Optional[float] = 0.0,
        msa_sampling: bool = False,
        use_templates: bool = False,
        max_templates: int = 4,
        pad_to_max_templates: bool = False,
        override_bfactor: float = False,
        override_method: Optional[str] = None,
        compute_frames: bool = True,
        override_coords: Optional[Tensor] = None,
        bfactor_md_correction: bool = False,
        use_templates_v2: bool = False,
        charge_info: Optional[np.ndarray] = None,
        inverse_fold: bool = False,
    ) -> Dict[str, Tensor]:
        """Compute features.

        Parameters
        ----------
        data : Input
            The input to the model.
        training : bool
            Whether the model is in training mode.
        max_tokens : int, optional
            The maximum number of tokens.
        max_atoms : int, optional
            The maximum number of atoms
        max_seqs : int, optional
            The maximum number of sequences.

        Returns
        -------
        Dict[str, Tensor]
            The features for model training.

        """
        # Compute random number of sequences
        if training and max_seqs is not None:
            if random.random() > single_sequence_prop:
                max_seqs_batch = random.integers(1, max_seqs + 1)
            else:
                max_seqs_batch = 1
        else:
            max_seqs_batch = max_seqs

        # Compute ensemble features
        ensemble_features = process_ensemble_features(
            data=data,
            random=random,
            num_ensembles=num_ensembles,
            ensemble_sample_replacement=ensemble_sample_replacement,
            fix_single_ensemble=fix_single_ensemble,
        )

        # Compute token features
        token_features = process_token_features(
            data=data,
            random=random,
            max_tokens=max_tokens,
            binder_pocket_conditioned_prop=binder_pocket_conditioned_prop,
            contact_conditioned_prop=contact_conditioned_prop,
            binder_pocket_cutoff_min=binder_pocket_cutoff_min,
            binder_pocket_cutoff_max=binder_pocket_cutoff_max,
            binder_pocket_sampling_geometric_p=binder_pocket_sampling_geometric_p,
            only_ligand_binder_pocket=only_ligand_binder_pocket,
            only_pp_contact=only_pp_contact,
            maximum_bond_distance=maximum_bond_distance,
            override_method=override_method,
            noisy_coords_std=noisy_coords_std,
            disulfide_prob=disulfide_prob,
            disulfide_on=disulfide_on,
        )

        # Compute atom features
        atom_features = process_atom_features(
            data=data,
            random=random,
            molecules=molecules,
            ensemble_features=ensemble_features,
            atoms_per_window_queries=atoms_per_window_queries,
            min_dist=min_dist,
            max_dist=max_dist,
            num_bins=num_bins,
            max_atoms=max_atoms,
            max_tokens=max_tokens,
            disto_use_ensemble=disto_use_ensemble,
            add_affinity_cropping=add_affinity_cropping,
            max_tokens_affinity=max_tokens_affinity,
            max_tokens_to_atomize_affinity=max_tokens_to_atomize_affinity,
            max_atoms_to_atomize_affinity=max_atoms_to_atomize_affinity,
            affinity_only_atoms=affinity_only_atoms,
            override_bfactor=override_bfactor,
            compute_frames=compute_frames,
            override_coords=override_coords,
            bfactor_md_correction=bfactor_md_correction,
            backbone_only=backbone_only,
            atom14=atom14,
            atom14_geometric=atom14_geometric,
            atom37=atom37,
            design=design,
            charge_info=charge_info,
            inverse_fold=inverse_fold,
        )

        atom_structure_groups = token_features["structure_group"][
            torch.argmax(atom_features["atom_to_token"].int(), dim=-1)
        ]
        if (atom_structure_groups ==1 ).int().sum() > 0:
            absolute_coords = center_random_augmentation(atom_features["coords"], (atom_structure_groups == 1)[None]) * (atom_structure_groups == 1)[None,:].unsqueeze(-1)
        else:
            absolute_coords = atom_features["coords"] * 0
        # Compute MSA features
        atom_features.update({"absolute_coords": absolute_coords})

        msa_features = process_msa_features(
            data=data,
            random=random,
            max_seqs_batch=max_seqs_batch,
            max_seqs=max_seqs,
            max_tokens=max_tokens,
            pad_to_max_seqs=pad_to_max_seqs,
            msa_sampling=training and msa_sampling,
        )

        # Compute MSA features
        msa_features_affinity = {}
        if compute_affinity:
            msa_features_affinity = process_msa_features(
                data=data,
                random=random,
                max_seqs_batch=1,
                max_seqs=1,
                max_tokens=max_tokens,
                pad_to_max_seqs=pad_to_max_seqs,
                msa_sampling=training and msa_sampling,
                affinity=True,
            )

        # Compute template features
        if use_templates and (not use_templates_v2):
            template_features = process_template_features(
                data=data,
                max_tokens=max_tokens,
                max_templates=max_templates,
                pad_to_max_templates=pad_to_max_templates,
            )
        else:
            template_features = {}

        # Compute symmetry features
        symmetry_features = {}
        if compute_symmetries:
            symmetries = get_symmetries(molecules)
            symmetry_features = process_symmetry_features(
                data, symmetries, backbone_only, atom14, atom14_geometric, atom37
            )

        # Compute label features
        label_features = {}
        label_features["has_structure"] = torch.tensor(
            [has_structure_label], dtype=torch.float32
        )
        label_features["has_affinity"] = torch.tensor(
            [has_affinity_label], dtype=torch.float32
        )

        # Compute secondary structure features
        t0 = time.time()
        design_mask = token_features["design_mask"].bool()
        ss = torch.zeros(len(design_mask), dtype=torch.long)
        design_ss_mask = token_features["design_ss_mask"].bool()
        if design_ss_mask.sum() > 0:
            atom_design_mask = (
                atom_features["atom_to_token"].float()
                @ design_mask.unsqueeze(-1).float()
            )
            atom_design_mask = atom_design_mask.squeeze().bool()
            bb_design_mask = (
                atom_features["atom_pad_mask"].bool()
                & atom_design_mask
                & atom_features["backbone_mask"].bool()
            )
            if bb_design_mask.sum() > 1:
                bb = atom_features["coords"][0][bb_design_mask]
                bb = bb.reshape(-1, 4, 3)
                assert len(bb) == design_mask.sum()
                try:
                    # 0: loop,  1: alpha-helix,  2: beta-strand
                    ss_out = pydssp.assign(bb, out_type="index")
                    # 1: loop,  2: alpha-helix,  3: beta-strand
                    ss_out += 1
                    ss[design_mask] = ss_out
                    ss *= design_ss_mask
                except:
                    print(
                        "Could not comptue secondary structure annotation. Leaving it unspecified"
                    )
        token_features.update({"ss_type": ss})

        return {
            "structure_bonds": data.structure.bonds,
            **token_features,
            **atom_features,
            **msa_features,
            **template_features,
            **symmetry_features,
            **label_features,
            **ensemble_features,
            **msa_features_affinity,
        }


def update_ligand_atom_types_from_atom_type(
    feat: Dict[str, Tensor], atom_type_logits: Tensor
):
    one_hot = one_hot_torch
    feat = copy.deepcopy(feat)
    design_mask = (
        feat["design_mask"].bool()
        & (feat["mol_type"] == const.chain_type_ids["NONPOLYMER"]).bool()
    )
    if design_mask.sum() == 0:
        return feat

    atom_to_token = torch.argmax(feat["atom_to_token"].int(), dim=-1)
    token_indices = feat["token_index"][design_mask]
    atom_design_mask = torch.isin(atom_to_token, token_indices)
    atom_design_mask = atom_design_mask & feat["atom_pad_mask"].bool()

    atom_type_pred = torch.argmax(atom_type_logits, dim=-1)
    ligand_atom_types = atom_type_pred[atom_design_mask]

    design_elements = feat["ref_element"][atom_design_mask]
    design_names = feat["ref_atom_name_chars"][atom_design_mask]

    from collections import defaultdict

    element_counters = defaultdict(int)
    for atom_idx, pred_type in enumerate(ligand_atom_types.tolist()):
        atomic_num = pred_type
        element_symbol = const.atomic_num_to_element.get(int(atomic_num), "C")

        element_counters[element_symbol] += 1
        design_elements[atom_idx] = one_hot(
            torch.tensor(atomic_num, device=feat["ref_element"].device),
            num_classes=const.num_elements,
        )
        count = element_counters[element_symbol]
        # Always use element symbol followed by count starting at 1 (e.g., "C1", "C2").
        atom_name = f"{element_symbol}{count}"
        atom_name = atom_name[:4]  # ensure 4-char limit
        name_tensor = convert_atom_name(atom_name)
        name_tensor = torch.tensor(name_tensor).to(feat["ref_atom_name_chars"].device)
        design_names[atom_idx] = one_hot(name_tensor, num_classes=64)

    feat["ref_element"][atom_design_mask] = design_elements
    feat["ref_atom_name_chars"][atom_design_mask] = design_names
    return feat
