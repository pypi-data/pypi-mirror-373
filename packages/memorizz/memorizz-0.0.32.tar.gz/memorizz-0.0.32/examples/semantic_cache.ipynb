{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24a4b950",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RichmondAlake/memorizz/blob/main/examples/semantic_cache.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479f199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU memorizz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc645bb",
   "metadata": {},
   "source": [
    "## Semantic Cache with Memorizz and MemAgents\n",
    "\n",
    "A **semantic cache** is an intelligent caching mechanism that stores query-response pairs and retrieves them based on meaning similarity rather than exact text matching. Unlike traditional caches that require exact key matches, semantic caching uses vector embeddings to find previously answered similar questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aad912",
   "metadata": {},
   "source": [
    "| Scenario | Traditional Cache Result | Semantic Cache Result |\n",
    "|----------|-------------------------|----------------------|\n",
    "| User asks: `\"What is ML?\"` after `\"What is machine learning?\"` was cached | ❌ Cache Miss - New LLM call | ✅ Cache Hit - Returns stored answer |\n",
    "| User asks: `\"Explain Python\"` after `\"Tell me about Python programming\"` was cached | ❌ Cache Miss - New LLM call | ✅ Cache Hit - Returns stored answer |\n",
    "| User has typo: `\"Wht is AI?\"` after `\"What is AI?\"` was cached | ❌ Cache Miss - New LLM call | ✅ Cache Hit - Returns stored answer |\n",
    "| Different language: `\"¿Qué es IA?\"` after `\"What is AI?\"` was cached | ❌ Cache Miss - New LLM call | ✅ Cache Hit - Cross-language match |\n",
    "| Rephrased: `\"How does neural network work?\"` after `\"Explain neural networks\"` was cached | ❌ Cache Miss - New LLM call | ✅ Cache Hit - Semantic similarity |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d458cc",
   "metadata": {},
   "source": [
    "| Aspect | Traditional Cache | Semantic Cache |\n",
    "|--------|------------------|----------------|\n",
    "| **Matching Strategy** | Exact string/key matching | Vector similarity matching |\n",
    "| **Query Examples** | `\"What is ML?\"` ≠ `\"What is machine learning?\"` | `\"What is ML?\"` ≈ `\"What is machine learning?\"` |\n",
    "| **Cache Hit Conditions** | Key must match exactly | Similarity score > threshold (e.g., 0.78) |\n",
    "| **Storage Structure** | `{key: value}` pairs | `{query, response, embedding, metadata}` |\n",
    "| **Lookup Complexity** | O(1) hash lookup | O(n) vector search or O(log n) with indexing |\n",
    "| **Memory Requirements** | Low (just key-value) | Higher (stores embeddings ~1536 floats per entry) |\n",
    "| **Setup Complexity** | Simple | Requires embedding provider + vector database |\n",
    "| **Intelligence Level** | Dumb (exact match only) | Smart (understands meaning and context) |\n",
    "| **Language Variations** | `\"color\"` ≠ `\"colour\"` | `\"color\"` ≈ `\"colour\"` |\n",
    "| **Typo Tolerance** | `\"machne learning\"` = Cache Miss | `\"machne learning\"` ≈ `\"machine learning\"` |\n",
    "| **Paraphrasing** | `\"How to learn Python?\"` ≠ `\"Python learning guide\"` | `\"How to learn Python?\"` ≈ `\"Python learning guide\"` |\n",
    "| **Cache Efficiency** | Low (many similar queries stored separately) | High (one entry serves many similar queries) |\n",
    "| **Configuration** | Simple size + TTL | Threshold, scope, embedding provider, TTL, size |\n",
    "| **Cost Implications** | Low storage, no computation | Higher storage, embedding computation cost |\n",
    "| **Use Cases** | API responses, computed values | Q&A systems, chatbots, knowledge retrieval |\n",
    "| **Invalidation Strategy** | Time-based or manual | Time-based + similarity score updates |\n",
    "| **Multi-language Support** | None (each language needs separate cache) | Good (cross-language semantic matching) |\n",
    "| **Context Awareness** | None | Yes (understands question intent) |\n",
    "| **Scalability** | Excellent (O(1) lookup) | Good (with proper vector indexing) |\n",
    "| **Cache Hit Rate** | Lower (exact matches only) | Higher (semantic matches) |\n",
    "| **Implementation Examples** | Redis, Memcached, in-memory dict | Memorizz, vector databases, embedding-based systems |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ea397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# Function to securely get and set environment variables\n",
    "def set_env_securely(var_name, prompt):\n",
    "    value = getpass.getpass(prompt)\n",
    "    os.environ[var_name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_env_securely(\"MONGODB_URI\", \"Enter your MongoDB URI: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59a455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_env_securely(\"OPENAI_API_KEY\", \"Enter your OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128aa7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_env_securely(\"VOYAGE_API_KEY\", \"Enter your VOYAGE AI API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d1ab8c",
   "metadata": {},
   "source": [
    "### Step 1: Initalize a Memory Provider\n",
    "\n",
    "A Memory Provider is a core abstraction layer that manages the persistence, organization, and retrieval of all memory components within an agentic system. It serves as the central nervous system for memory management, providing standardized interfaces between AI agents and underlying storage technologies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c1bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from memorizz.memory_provider.mongodb.provider import MongoDBConfig, MongoDBProvider\n",
    "\n",
    "# Create a mongodb config with voyageai embeddings\n",
    "mongodb_config = MongoDBConfig(\n",
    "    uri=os.environ[\"MONGODB_URI\"],\n",
    "    db_name=\"testing_memorizz\",\n",
    "    embedding_provider=\"voyageai\",\n",
    "    embedding_config={\n",
    "        \"embedding_type\": \"contextualized\",\n",
    "        \"model\": \"voyage-context-3\",\n",
    "        \"output_dimension\": 256,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create a memory provider\n",
    "memory_provider = MongoDBProvider(mongodb_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e75c9a",
   "metadata": {},
   "source": [
    "## Step 2: Create a MemAgent with a Semantic Cache Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020ffd60",
   "metadata": {},
   "source": [
    "### Semantic Cache Configuration - The Smart Settings\n",
    "\n",
    "| Setting | Argument | Description | Purpose | Example Values |\n",
    "|---------|----------|-------------|---------|----------------|\n",
    "| **Similarity Threshold** | `similarity_threshold=0.85` | Controls how \"similar\" two questions need to be for a cache hit (0.0 to 1.0 scale) | Prevents false positives while allowing intelligent matching of rephrased questions | `0.70` (loose), `0.78` (moderate), `0.85` (strict), `0.90` (very strict) |\n",
    "| **Cache Size Limit** | `max_cache_size=100` | Maximum number of cached query-response pairs to store in memory | Manages memory usage and prevents unlimited cache growth | `50` (small), `100` (medium), `500` (large), `1000` (enterprise) |\n",
    "| **Time-To-Live** | `ttl_hours=24.0` | How long cached responses remain valid before expiring (in hours) | Ensures information freshness and prevents stale responses | `1.0` (1 hour), `6.0` (6 hours), `24.0` (1 day), `168.0` (1 week) |\n",
    "| **Memory Provider Sync** | `enable_memory_provider_sync=True` | Whether to store cache entries in persistent database (MongoDB) | Enables cache persistence across agent restarts and sharing between instances | `True` (persistent), `False` (in-memory only) |\n",
    "| **Usage Tracking** | `enable_usage_tracking=True` | Whether to track usage statistics for cached entries | Provides analytics and enables intelligent cache eviction based on popularity | `True` (track stats), `False` (no tracking) |\n",
    "| **Session Scoping** | `enable_session_scoping=False` | Whether to isolate cache entries by user session ID | Controls cache sharing across different conversation sessions | `True` (session-isolated), `False` (cross-session sharing) |\n",
    "| **Cache Scope** | `scope=SemanticCacheScope.LOCAL` | Defines cache visibility boundaries between agents | Controls whether agents share cached responses or maintain separate caches | `SemanticCacheScope.LOCAL` (agent-specific), `SemanticCacheScope.GLOBAL` (shared across agents) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be43cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from memorizz.enums import SemanticCacheScope\n",
    "from memorizz.short_term_memory.semantic_cache import SemanticCacheConfig\n",
    "\n",
    "\n",
    "# Create a semantic cache config\n",
    "semantic_cache_config = SemanticCacheConfig(\n",
    "    similarity_threshold=0.85,\n",
    "    max_cache_size=100,\n",
    "    ttl_hours=24.0,\n",
    "    enable_memory_provider_sync=True,\n",
    "    enable_usage_tracking=True,\n",
    "    enable_session_scoping=False,\n",
    "    scope=SemanticCacheScope.LOCAL # This agent will have access to cached responses made by it's own instance\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ab1e01",
   "metadata": {},
   "source": [
    "### **Configuration Examples by Use Case**\n",
    "\n",
    "| Use Case | Configuration Example | Rationale |\n",
    "|----------|----------------------|-----------|\n",
    "| **High-Accuracy System** | `similarity_threshold=0.90, max_cache_size=200, ttl_hours=6.0` | Strict matching for critical applications, shorter TTL for fresh data |\n",
    "| **Cost-Optimized Chatbot** | `similarity_threshold=0.75, max_cache_size=1000, ttl_hours=48.0` | More aggressive caching to reduce LLM API calls |\n",
    "| **Multi-Agent Knowledge Sharing** | `scope=SemanticCacheScope.GLOBAL, enable_session_scoping=False` | Agents learn from each other's interactions |\n",
    "| **Privacy-Focused Application** | `scope=SemanticCacheScope.LOCAL, enable_session_scoping=True` | Strict isolation between agents and sessions |\n",
    "| **Development/Testing** | `similarity_threshold=0.70, max_cache_size=50, ttl_hours=1.0` | Lower threshold for testing, small cache, quick expiration |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6447d5",
   "metadata": {},
   "source": [
    "This code below creates an intelligent AI agent using the Memorizz library that combines GPT-4's language capabilities with semantic caching for enhanced performance and cost efficiency. \n",
    "\n",
    "The `MemAgent` is initialized with OpenAI's GPT-4 model as its core language processor, connected to a persistent memory provider (likely MongoDB) for storing cached responses, and configured with a specific instruction that defines its personality as a concise, helpful assistant. \n",
    "\n",
    "Most importantly, the agent has semantic caching enabled (`semantic_cache=True`) with custom configuration settings (`semantic_cache_config`), which means it can intelligently recognize when new user queries are semantically similar to previously answered questions and return cached responses instantly instead of making expensive API calls to GPT-4, resulting in faster response times, reduced costs, and consistent answers for similar queries even when phrased differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b8aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from memorizz.memagent import MemAgent\n",
    "from memorizz.llms.openai import OpenAI\n",
    "# Create agent with semantic cache enabled\n",
    "local_scoped_agent = MemAgent(\n",
    "    model=OpenAI(model=\"gpt-4\"),\n",
    "    memory_provider=memory_provider,\n",
    "    instruction=\"You are a helpful assistant that answers questions concisely.\",\n",
    "    semantic_cache=True,  # Enable semantic cache\n",
    "    semantic_cache_config=semantic_cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de0dfc4",
   "metadata": {},
   "source": [
    "Don't forget to save the agent before running the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b412be",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_scoped_agent.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59352475",
   "metadata": {},
   "source": [
    "## Step 3: Testing Semantic Cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52963c12",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### First LLM Call (Cache Miss)\n",
    "\n",
    "This code below demonstrates the semantic cache workflow in action by testing how the agent handles a fresh query that hasn't been cached before. \n",
    "\n",
    "The code uses agent.run() to ask \"What is the capital of United Kingdom?\" for the first time, which will result in a cache miss since no similar question has been asked previously, forcing the agent to make an actual API call to GPT-4 to generate the response, and then automatically store both the query and the GPT-4 response (along with the query's vector embedding) in the semantic cache for future use. \n",
    "\n",
    "The printed output will show the actual response from GPT-4, and behind the scenes, this interaction creates a new cache entry that will enable faster responses for semantically similar questions like \"What's the UK's capital?\" or \"Tell me the capital city of Britain\" in subsequent queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5297d942",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Test the REAL semantic cache workflow\n",
    "print(\"🧪 Testing semantic cache with agent.run():\")\n",
    "\n",
    "# First query - will call LLM and cache response\n",
    "print(\"\\n1. First query (will call LLM):\")\n",
    "response1 = local_scoped_agent.run(\"What is the capital of United Kingdom?\")\n",
    "print(f\"Response: {response1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11bb6e4",
   "metadata": {},
   "source": [
    "### Second LLM Call (Cache Hit)\n",
    "\n",
    "This code below demonstrates the power of semantic caching by asking a semantically similar question to the one previously cached, showing how the agent can intelligently recognize that \"Tell me the capital city of United Kingdom\" is essentially the same question as the earlier \"What is the capital of United Kingdom?\" despite different phrasing. \n",
    "\n",
    "When `agent.run()` processes this query, it will generate a vector embedding for the new question, perform a similarity search against cached entries, find that the similarity score exceeds the configured threshold (likely around 0.85), and return the previously cached GPT-4 response instantly without making another expensive API call. \n",
    "\n",
    "This results in a much faster response time, cost savings, and demonstrates how semantic caching enables intelligent query matching based on meaning rather than exact text, allowing users to ask the same question in multiple ways while still benefiting from cached responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a12cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar query - should hit cache (no LLM call!)\n",
    "print(\"\\n2. Similar query (should hit cache):\")\n",
    "response2 = local_scoped_agent.run(\"Tell me the capital city of United Kingdom\")\n",
    "print(f\"Response: {response2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f44167",
   "metadata": {},
   "source": [
    "This code below now validates the semantic cache functionality by comparing two responses to verify if the cache is working correctly, then provides diagnostic information about the cache's performance. \n",
    "\n",
    "The comparison logic checks if `response1` (from the first query) equals `response2` (from a semantically similar second query) - if they match, it confirms a successful cache hit where the agent retrieved the stored response instead of calling the LLM again, but if they differ, it indicates a cache miss where the agent generated a new response. \n",
    "\n",
    "Additionally, the code retrieves and displays cache statistics using `agent.semantic_cache_instance.get_stats()` to show the total number of cached entries and the cumulative usage count across all cache hits, providing insights into how effectively the semantic cache is being utilized and helping developers monitor the cache's performance and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181c602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if it was a cache hit\n",
    "if response1 == response2:\n",
    "    print(\"✅ CACHE HIT! Same response returned\")\n",
    "else:\n",
    "    print(\"❌ Cache miss - responses differ\")\n",
    "\n",
    "# Check cache stats\n",
    "stats = local_scoped_agent.semantic_cache_instance.get_stats()\n",
    "print(f\"\\nCache stats: {stats['total_entries']} entries, {stats['total_usage_count']} uses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52de63f",
   "metadata": {},
   "source": [
    "# Step 4: Agent With Global Semantic Cache Access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797d4af8",
   "metadata": {},
   "source": [
    "This code below creates a `global_scoped_agent` that demonstrates semantic cache sharing across multiple AI agents by configuring the cache scope to `SemanticCacheScope.GLOBAL`. \n",
    "\n",
    "Unlike a locally-scoped agent that only accesses its own cached responses, this global-scoped agent can retrieve and benefit from cached responses created by any other agent in the system, effectively creating a shared knowledge pool where all agents learn from each other's interactions. The agent is built with GPT-4 as its language model, connected to a persistent memory provider, and configured with a strict similarity threshold of 0.85 to ensure high-quality cache matches, while the `enable_memory_provider_sync=True` setting ensures that this shared cache is stored persistently in the database. \n",
    "\n",
    "This global approach maximizes cache efficiency and cost savings across an entire multi-agent system, as any agent's interaction with a question like \"What is machine learning?\" would create a cache entry that all other agents can instantly access when users ask semantically similar questions like \"Explain ML\" or \"Define machine learning.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c8734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_scoped_agent = MemAgent(\n",
    "    model=OpenAI(model=\"gpt-4\"),\n",
    "    memory_provider=memory_provider,\n",
    "    instruction=\"You are a helpful assistant that answers questions concisely.\",\n",
    "    semantic_cache=True,  # Enable semantic cache\n",
    "    semantic_cache_config=SemanticCacheConfig(\n",
    "        similarity_threshold=0.85,\n",
    "        max_cache_size=100,\n",
    "        ttl_hours=24.0,\n",
    "        enable_memory_provider_sync=True,\n",
    "        enable_usage_tracking=True,\n",
    "        enable_session_scoping=False,\n",
    "        scope=SemanticCacheScope.GLOBAL\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7728bd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_scoped_agent.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555c33c3",
   "metadata": {},
   "source": [
    "This code below executes a query through the `global_scoped_agent` asking \"What is the capital of United Kingdom?\" which will demonstrates how the global semantic cache scope works in practice. \n",
    "\n",
    "When this agent processes the query, it first searches the global cache (shared across all agents in the system) for any semantically similar questions that have been previously answered, and if another agent has already cached a response to a similar question like \"What's the UK's capital?\" or \"Tell me London's status as capital,\" this agent will retrieve and return that cached response instantly without calling GPT-4. \n",
    "\n",
    "However, if no similar question exists in the global cache, the agent will query GPT-4 for a fresh response and then store the new query-response pair in the global cache, making it available for all other agents in the system to benefit from in future interactions, thereby contributing to the collective knowledge pool while potentially saving API costs and response time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdadabd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_scoped_agent.run(\"What is the capital of United Kingdom?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586b5889",
   "metadata": {},
   "source": [
    "This is a new query not in the semantic cache collection in this run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58f63cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_scoped_agent.run(\"Tell me the capital city of Australia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee37735",
   "metadata": {},
   "source": [
    "Local scoped agent can't access previosuly cached responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0a0416",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_scoped_agent.run(\"What is the capital of Australia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56217dff",
   "metadata": {},
   "source": [
    "## Step 5: Cache Clearing Strategies\n",
    "\n",
    "| Method | Scope | Use Case | Example Usage |\n",
    "|--------|-------|----------|---------------|\n",
    "| `agent.semantic_cache_instance.clear()` | **Agent's in-memory cache only** | Quick development reset, testing, debugging | Testing new cache behavior, clearing during development iterations |\n",
    "| `agent.semantic_cache_instance.clear(session_id=\"...\")` | **Specific user session** | User logout cleanup, session isolation, privacy compliance | User ends conversation, switching between different user contexts |\n",
    "| `agent.semantic_cache_instance.clear(memory_id=\"...\")` | **Specific memory context** | Context-specific cleanup, memory boundary management | Switching between different conversation topics or workflows |\n",
    "| `memory_provider.clear_semantic_cache()` | **Database level (all agents globally)** | Complete system reset, production maintenance, emergency cleanup | System maintenance, migrating to new cache version, fixing corrupted cache |\n",
    "| `memory_provider.clear_semantic_cache(agent_id=\"...\")` | **All cache entries for specific agent** | Agent-specific maintenance, individual agent reset | Redeploying specific agent, fixing agent-specific cache issues |\n",
    "| `memory_provider.clear_semantic_cache(memory_id=\"...\")` | **Database entries for specific memory context** | Memory-specific database cleanup across all agents | Cleaning up specific workflow or conversation context globally |\n",
    "| `memory_provider.clear_semantic_cache(agent_id=\"...\", memory_id=\"...\")` | **Specific agent + memory combination** | Precise targeted cleanup, surgical cache removal | Fixing specific agent-memory combination issues, targeted debugging |\n",
    "\n",
    "### **Clearing Strategy Decision Tree**\n",
    "\n",
    "| Scenario | Recommended Method | Rationale |\n",
    "|----------|-------------------|-----------|\n",
    "| **Development/Testing** | `agent.semantic_cache_instance.clear()` | Fast, local, doesn't affect other developers |\n",
    "| **User Privacy/Logout** | `clear(session_id=\"user_session\")` | Removes only user-specific cached responses |\n",
    "| **Production Maintenance** | `memory_provider.clear_semantic_cache()` | Complete system cleanup, affects all agents |\n",
    "| **Agent Redeployment** | `clear_semantic_cache(agent_id=\"agent_123\")` | Removes cached responses for updated agent |\n",
    "| **Emergency Cache Corruption** | `memory_provider.clear_semantic_cache()` | Nuclear option to fix system-wide issues |\n",
    "| **Context Switching** | `clear(memory_id=\"context_456\")` | Clean slate for new conversation context |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eda9c5a",
   "metadata": {},
   "source": [
    "Let's clear the local agent cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e40619",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_scoped_agent.semantic_cache_instance.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe06c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_scoped_agent.semantic_cache_instance.get_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e72e5fb",
   "metadata": {},
   "source": [
    "Let's clear the global agent cache using the memory provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07a975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_provider.clear_semantic_cache(agent_id=global_scoped_agent.agent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fad646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_scoped_agent.semantic_cache_instance.get_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f2ae8d",
   "metadata": {},
   "source": [
    "Let's clear all cache in the memory provider (database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf363af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_provider.clear_semantic_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "memorizz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
