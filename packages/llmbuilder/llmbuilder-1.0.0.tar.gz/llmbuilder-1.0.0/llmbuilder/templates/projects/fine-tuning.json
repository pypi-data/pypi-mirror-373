{
  "name": "fine-tuning",
  "description": "Fine-tuning focused project template with PEFT support and model adaptation",
  "version": "1.0.0",
  "directories": [
    "data/raw",
    "data/cleaned",
    "data/deduped",
    "data/tokens",
    "data/finetune",
    "data/instruction",
    "data/chat",
    "models/base",
    "models/adapters",
    "exports/checkpoints",
    "exports/tokenizer",
    "exports/gguf",
    "exports/merged",
    "logs",
    "scripts",
    "evaluation",
    "configs"
  ],
  "files": {
    "requirements.txt": "llmbuilder>=1.0.0\npeft>=0.4.0\ntransformers>=4.21.0\ndatasets>=2.0.0\nevaluate>=0.2.0\n",
    "finetune_config.yaml": "# Fine-tuning configuration\nproject:\n  name: \"finetuning_project\"\n  description: \"Fine-tuning project with PEFT support\"\n  \nbase_model:\n  name: \"microsoft/DialoGPT-medium\"\n  path: \"models/base\"\n  trust_remote_code: false\n  \nfinetuning:\n  method: \"lora\"  # lora, qlora, full\n  task_type: \"CAUSAL_LM\"\n  \n  # LoRA configuration\n  lora:\n    r: 16\n    lora_alpha: 32\n    lora_dropout: 0.1\n    target_modules: [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n    bias: \"none\"\n    \n  # QLoRA configuration\n  qlora:\n    bits: 4\n    quant_type: \"nf4\"\n    use_double_quant: true\n    compute_dtype: \"float16\"\n    \ntraining:\n  batch_size: 4\n  gradient_accumulation_steps: 4\n  learning_rate: 0.0002\n  num_epochs: 3\n  warmup_steps: 100\n  weight_decay: 0.01\n  max_grad_norm: 1.0\n  \n  # Optimization\n  use_gradient_checkpointing: true\n  dataloader_num_workers: 4\n  \ndata:\n  format: \"instruction\"  # instruction, chat, text\n  max_length: 512\n  train_file: \"data/finetune/train.jsonl\"\n  eval_file: \"data/finetune/eval.jsonl\"\n  \n  # Data formatting\n  instruction_template: \"### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\"\n  \nevaluation:\n  eval_steps: 100\n  eval_strategy: \"steps\"\n  metric_for_best_model: \"eval_loss\"\n  load_best_model_at_end: true\n  \nlogging:\n  logging_steps: 10\n  save_steps: 500\n  save_total_limit: 3\n  report_to: [\"tensorboard\"]\n",
    "scripts/prepare_instruction_data.py": "#!/usr/bin/env python3\n\"\"\"\nPrepare instruction-following dataset for fine-tuning.\n\"\"\"\n\nimport json\nimport argparse\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\ndef format_instruction_data(data: List[Dict[str, Any]], template: str) -> List[str]:\n    \"\"\"\n    Format instruction data using template.\n    \n    Args:\n        data: List of instruction-response pairs\n        template: Template string with {instruction} and {response} placeholders\n        \n    Returns:\n        List of formatted strings\n    \"\"\"\n    formatted = []\n    for item in data:\n        if 'instruction' in item and 'response' in item:\n            formatted_text = template.format(\n                instruction=item['instruction'],\n                response=item['response']\n            )\n            formatted.append(formatted_text)\n    return formatted\n\ndef main():\n    parser = argparse.ArgumentParser(description='Prepare instruction data')\n    parser.add_argument('--input', required=True, help='Input JSON file')\n    parser.add_argument('--output', required=True, help='Output JSONL file')\n    parser.add_argument('--template', help='Instruction template')\n    args = parser.parse_args()\n    \n    # Default template\n    template = args.template or \"### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\"\n    \n    # Load input data\n    with open(args.input, 'r') as f:\n        data = json.load(f)\n    \n    # Format data\n    formatted_data = format_instruction_data(data, template)\n    \n    # Save output\n    output_path = Path(args.output)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    \n    with open(output_path, 'w') as f:\n        for text in formatted_data:\n            json.dump({\"text\": text}, f)\n            f.write('\\n')\n    \n    print(f\"Processed {len(formatted_data)} examples\")\n    print(f\"Output saved to: {output_path}\")\n\nif __name__ == '__main__':\n    main()\n",
    "scripts/merge_adapters.py": "#!/usr/bin/env python3\n\"\"\"\nMerge LoRA adapters with base model.\n\"\"\"\n\nimport argparse\nfrom pathlib import Path\n\ndef merge_adapters(base_model_path: str, adapter_path: str, output_path: str):\n    \"\"\"\n    Merge LoRA adapters with base model.\n    \n    Args:\n        base_model_path: Path to base model\n        adapter_path: Path to LoRA adapters\n        output_path: Path to save merged model\n    \"\"\"\n    try:\n        from peft import PeftModel\n        from transformers import AutoModelForCausalLM, AutoTokenizer\n        \n        print(f\"Loading base model from: {base_model_path}\")\n        base_model = AutoModelForCausalLM.from_pretrained(base_model_path)\n        \n        print(f\"Loading adapters from: {adapter_path}\")\n        model = PeftModel.from_pretrained(base_model, adapter_path)\n        \n        print(\"Merging adapters...\")\n        merged_model = model.merge_and_unload()\n        \n        print(f\"Saving merged model to: {output_path}\")\n        output_path = Path(output_path)\n        output_path.mkdir(parents=True, exist_ok=True)\n        \n        merged_model.save_pretrained(output_path)\n        \n        # Also save tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n        tokenizer.save_pretrained(output_path)\n        \n        print(\"Merge completed successfully!\")\n        \n    except ImportError:\n        print(\"Error: PEFT library not installed. Run: pip install peft\")\n    except Exception as e:\n        print(f\"Error during merge: {e}\")\n\ndef main():\n    parser = argparse.ArgumentParser(description='Merge LoRA adapters')\n    parser.add_argument('--base-model', required=True, help='Base model path')\n    parser.add_argument('--adapter', required=True, help='Adapter path')\n    parser.add_argument('--output', required=True, help='Output path')\n    args = parser.parse_args()\n    \n    merge_adapters(args.base_model, args.adapter, args.output)\n\nif __name__ == '__main__':\n    main()\n"
  },
  "config_overrides": {
    "model": {
      "architecture": "gpt",
      "vocab_size": 50257,
      "embedding_dim": 768,
      "num_layers": 12,
      "num_heads": 12,
      "hidden_dim": 3072,
      "max_seq_length": 1024
    },
    "training": {
      "batch_size": 4,
      "learning_rate": 0.0002,
      "num_epochs": 3,
      "use_mixed_precision": true,
      "gradient_checkpointing": true
    },
    "data": {
      "preprocessing": {
        "max_length": 512,
        "min_length": 10
      }
    }
  },
  "sample_data": {
    "enabled": true,
    "files": [
      {
        "name": "sample_finetune_data.jsonl",
        "content": "{\"text\": \"### Instruction:\\nExplain what fine-tuning is in machine learning.\\n\\n### Response:\\nFine-tuning is a technique where you take a pre-trained model and adapt it to a specific task by training it on task-specific data with a lower learning rate.\"}\n{\"text\": \"### Instruction:\\nWhat is LoRA?\\n\\n### Response:\\nLoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning method that adds trainable low-rank matrices to existing model layers, allowing adaptation with fewer parameters.\"}\n{\"text\": \"### Instruction:\\nHow do you evaluate a fine-tuned model?\\n\\n### Response:\\nYou can evaluate a fine-tuned model using metrics like perplexity, BLEU score, accuracy, and task-specific metrics on a held-out test set.\"}"
      }
    ]
  }
}