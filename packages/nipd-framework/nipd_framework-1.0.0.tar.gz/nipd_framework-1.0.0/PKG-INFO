Metadata-Version: 2.4
Name: nipd-framework
Version: 1.0.0
Summary: Network Iterated Prisoner's Dilemma Framework for Multi-Agent Learning
Home-page: https://github.com/maximusJWL/nipd-framework
Author: maximusjwl
Author-email: max.lams99@gmail.com
License: CC BY-NC 4.0
Project-URL: Bug Reports, https://github.com/maximusJWL/nipd-framework/issues
Project-URL: Source, https://github.com/maximusJWL/nipd-framework
Project-URL: Documentation, https://github.com/maximusJWL/nipd-framework#readme
Keywords: multi-agent,prisoner-dilemma,reinforcement-learning,game-theory,networks,cooperation,defection,social-dilemmas
Classifier: Intended Audience :: Science/Research
Classifier: Intended Audience :: Education
Classifier: License :: Other/Proprietary License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.13
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Scientific/Engineering :: Mathematics
Requires-Python: >=3.13
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy>=1.21.0
Requires-Dist: matplotlib>=3.5.0
Requires-Dist: torch>=1.9.0
Requires-Dist: tqdm>=4.62.0
Requires-Dist: pandas>=1.3.0
Requires-Dist: seaborn>=0.11.0
Requires-Dist: networkx>=2.6.0
Requires-Dist: ipykernel>=6.0.0
Requires-Dist: jupyter>=1.0.0
Requires-Dist: notebook>=6.4.0
Provides-Extra: dev
Requires-Dist: pytest>=6.0; extra == "dev"
Requires-Dist: pytest-cov>=2.0; extra == "dev"
Requires-Dist: black>=21.0; extra == "dev"
Requires-Dist: flake8>=3.8; extra == "dev"
Requires-Dist: mypy>=0.800; extra == "dev"
Provides-Extra: docs
Requires-Dist: sphinx>=4.0; extra == "docs"
Requires-Dist: sphinx-rtd-theme>=1.0; extra == "docs"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: license
Dynamic: license-file
Dynamic: project-url
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# NIPD Framework - Network Iterated Prisoner's Dilemma

A comprehensive Python package for simulating and analyzing multi-agent learning in network-structured environments using the Iterated Prisoner's Dilemma.

## Installation

```bash
# Install from PyPI
pip install nipd-framework

# Or install from source
git clone https://github.com/maximusJWL/nipd-framework
cd nipd-framework
pip install -e .
```

## Package Contents

The package includes:
- **Core simulation engine** (`nipd.agent_simulator`, `nipd.network_environment`, etc.)
- **Pretrained models** (one final model per algorithm type)
- **Training scripts** (for users who want to train their own models)
- **Example usage scripts**

**Note**: Training outputs (curves, reports, intermediate models) are excluded from the package to keep it lightweight. Only the final pretrained models are included.

## Quick Start

```python
import nipd

# Create a simple simulation
agent_config = {
    'titfortat': 5,
    'cooperator': 3,
    'defector': 2
}

network_config = {
    'type': 'small_world',
    'k_neighbors': 3,
    'rewire_prob': 0.1
}

simulation_config = {
    'episode_length': 100,
    'num_episodes': 1,
    'reward_matrix': [[3.0, 0.0], [5.0, 1.0]],
    'use_system_rewards': False,
    'noise': {'enabled': True, 'probability': 0.05}
}

# Run simulation
simulator = nipd.AgentSimulator(agent_config, network_config, simulation_config)
simulator.run_simulation()
simulator.create_visualizations()
```

### Command Line Usage

The package includes a command-line interface:

```bash
# Run the default simulation
nipd-simulate

# Run with custom configuration (modify agent_simulator.py first)
python -m nipd.agent_simulator
```

### Examples

See the `examples/` directory for more detailed usage examples:

```bash
# Basic simulation example
python examples/basic_simulation.py

# Advanced simulation with multiple agent types
python examples/advanced_simulation.py
```

## Overview

The NIPD Framework implements various reinforcement learning algorithms for studying cooperation and defection strategies in multi-agent environments, specifically focusing on the Network Iterated Prisoner's Dilemma game.

## Overview

The project contains implementations of different reinforcement learning agents that can learn to cooperate or defect in a networked environment. Each agent type uses different learning algorithms and can be configured for online learning (adapting during gameplay) or offline learning (using pre-trained models).

## Package Structure

```
nipd-framework/
├── nipd/                           # Main package directory
│   ├── __init__.py                # Package initialization
│   ├── agent_simulator.py         # Main simulation engine
│   ├── network_environment.py     # Network environment implementation
│   ├── network_tops.py            # Network topology utilities
│   ├── round.py                   # Game round implementation
│   └── models/                    # Agent model implementations
│       ├── standard_mappo/        # Standard MAPPO agents
│       ├── cooperative_mappo/     # Cooperative MAPPO agents
│       ├── decentralized_ppo/     # Decentralized PPO agents
│       ├── lola/                  # LOLA agents
│       ├── q_network/             # Q-Network agents
│       ├── simple_q_learning/     # Simple Q-Learning agents
│       ├── online_mappo/          # Online learning MAPPO agents
│       ├── online_decentralized_ppo/ # Online learning DPPO agents
│       ├── online_lola/           # Online learning LOLA agents
│       ├── online_q_network/      # Online learning Q-Network agents
│       └── online_simple_q_learning/ # Online learning Simple Q-Learning agents
├── examples/                       # Example scripts
│   ├── basic_simulation.py        # Basic usage example
│   └── advanced_simulation.py     # Advanced usage example
├── setup.py                       # Package setup configuration
├── requirements.txt               # Python dependencies
├── LICENSE                        # CC BY-NC 4.0 License
└── README.md                      # This file
```

## Agent Types

### Standard Agents (Pre-trained)
- **Standard MAPPO**: Multi-Agent Proximal Policy Optimization with centralized training
- **Cooperative MAPPO**: MAPPO variant optimized for cooperation
- **Decentralized PPO**: Decentralized Proximal Policy Optimization
- **LOLA**: Learning with Opponent-Learning Awareness
- **Q-Network**: Deep Q-Network implementation
- **Simple Q-Learning**: Tabular Q-Learning with state discretization

### Online Learning Agents
- **Online MAPPO**: MAPPO with online learning capabilities
- **Online DPPO**: Decentralized PPO with online learning
- **Online LOLA**: LOLA with online learning and opponent modeling
- **Online Q-Network**: Q-Network with online learning
- **Online Simple Q-Learning**: Simple Q-Learning with online adaptation

## Key Features

### Online Learning
Online learning agents can adapt their strategies during gameplay based on observations of opponent behavior. They update their networks after each round and can switch between cooperation and defection strategies based on the observed opponent's behavior.

### Observation Format
All agents use a standardized 4-dimensional observation format:
- `own_prev`: Agent's previous action (0 = cooperate, 1 = defect)
- `neighbor_prev`: Neighbor's previous action
- `neighbor_coop_rate`: Neighbor's cooperation rate
- `neighbors_norm`: Normalized number of neighbors

### Reward System
The system supports both private rewards (individual agent rewards) and local rewards (collective performance). Agents can be configured to optimize for either individual gain or collective benefit.

### Noise Implementation
The simulation includes configurable noise that can cause agents to execute the opposite action than intended, modeling real-world uncertainty.

## Usage

### Basic Simulation

```python
import nipd

# Agent Configuration
agent_config = {
    'titfortat': 5,      # 5 Tit-for-Tat agents
    'cooperator': 3,     # 3 Always Cooperate agents
    'defector': 2,       # 2 Always Defect agents
    'decentralized_ppo': 0,
    'standard_mappo': 0,
    'cooperative_mappo': 0,
    'lola': 0,
    'q_learner': 0,
    'online_simple_q': 0,
    'online_q_network': 0,
    'online_decentralized_ppo': 0,
    'online_lola': 0,
    'online_mappo': 0
}

# Network Configuration
network_config = {
    'type': 'small_world',
    'k_neighbors': 4,
    'rewire_prob': 0.1
}

# Simulation Configuration
simulation_config = {
    'episode_length': 250,
    'num_episodes': 1,
    'reward_matrix': [[3.0, 0.0], [5.0, 1.0]],
    'use_system_rewards': False,
    'noise': {'enabled': True, 'probability': 0.05}
}

# Create and run simulator
simulator = nipd.AgentSimulator(agent_config, network_config, simulation_config)
simulator.run_simulation()
simulator.create_visualizations()
```

# Simulation Configuration
simulation_config = {
    'episode_length': 200,        # Number of timesteps per episode
    'num_episodes': 1,           # Number of episodes to run
    'reward_matrix': [            # Prisoner's Dilemma reward matrix
        [3.0, 0.0],              # Cooperate vs [Cooperate, Defect]
        [5.0, 1.0]               # Defect vs [Cooperate, Defect]
    ],
    'use_system_rewards': False,      # True: use system-wide rewards, False: use private rewards
    'noise': {
        'enabled': True,              # Enable action noise in simulation
        'probability': 0.05,          # Probability of action flip per agent per timestep (0.0-1.0)
        'description': 'Random chance for agents to execute opposite action than intended'
    }
}

# Create and run simulation
simulator = AgentSimulator(agent_config, network_config, simulation_config)
simulator.run_simulation()
```

### Using Different Agent Types

```python
# Configure agent types with counts
agent_config = {
    'decentralized_ppo': 2,        # 2 decentralized PPO agents
    'standard_mappo': 1,           # 1 standard MAPPO agent
    'cooperative_mappo': 1,        # 1 cooperative MAPPO agent
    'lola': 2,                     # 2 LOLA agents
    'q_learner': 1,                # 1 Q-learning agent
    'titfortat': 1,                # 1 tit-for-tat agent
    'cooperator': 1,               # 1 cooperator agent
    'defector': 1,                 # 1 defector agent
    # Online learning agents (initialized from pretrained models)
    'online_simple_q': 0,
    'online_q_network': 0,
    'online_decentralized_ppo': 0,
    'online_lola': 0,
    'online_mappo': 0
}

simulator = AgentSimulator(agent_config, network_config, simulation_config)
simulator.run_simulation()
```

### System vs Private Optimization

```python
# For system optimization (agents optimize for collective benefit)
simulation_config['use_system_rewards'] = True

# For private optimization (agents optimize for individual benefit)
simulation_config['use_system_rewards'] = False
```

### Adding Noise

```python
# Add 10% noise to actions
simulation_config['noise'] = {
    'enabled': True,
    'probability': 0.1,  # 10% chance of action flip
    'description': 'Random chance for agents to execute opposite action than intended'
}
```

## Configuration Options

### Network Configuration
- `type`: Network topology ('small_world', 'random', 'ring')
- `k_neighbors`: Number of neighbors per agent
- `rewire_prob`: Rewiring probability for small-world networks

### Simulation Configuration
- `episode_length`: Number of timesteps per episode
- `num_episodes`: Number of episodes to run
- `reward_matrix`: Payoff matrix for the game
- `use_system_rewards`: Whether agents optimize for system or private rewards
- `noise`: Noise configuration dictionary with `enabled`, `probability`, and `description`

### Agent Configuration
The `agent_config` dictionary specifies how many agents of each type to include:
- `decentralized_ppo`: Number of decentralized PPO agents
- `standard_mappo`: Number of standard MAPPO agents
- `cooperative_mappo`: Number of cooperative MAPPO agents
- `lola`: Number of LOLA agents
- `q_learner`: Number of Q-learning agents
- `titfortat`: Number of tit-for-tat agents
- `cooperator`: Number of always-cooperate agents
- `defector`: Number of always-defect agents
- `online_*`: Number of online learning agents (initialized from pretrained models)

## Output and Analysis

The simulation generates several outputs:

1. **CSV Files**: Detailed round-by-round data including actions, rewards, and cooperation rates
2. **Visualizations**: 
   - Move history heatmaps
   - Cooperation rate over time
   - Score per round graphs
   - Network topology visualization
3. **Leaderboard**: Final agent rankings based on total scores
4. **Logs**: Detailed simulation logs with agent behavior and learning updates

## Model Loading

All models are stored in the centralized `models/` directory. Each agent type has its own subdirectory containing:
- Trained model files (`.pt` for PyTorch models, `.json` for Q-tables)
- Training metrics and logs
- Model configurations

## Setup and Dependencies

### Virtual Environment Setup
```bash
# Create virtual environment
python -m venv .venv

# Activate virtual environment
# Windows:
.venv\Scripts\activate.bat
# Linux/Mac:
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Install Jupyter kernel
python -m ipykernel install --user --name nipd_env --display-name "NIPD Environment"
```

### Dependencies
- Python 3.13+
- PyTorch
- NumPy
- Matplotlib
- Pandas
- NetworkX
- Jupyter
- ipykernel

## Running Examples

### Quick Start
```bash
python agent_simulator.py
```

### Compare Training Approaches
```bash
python compare_training_approaches.py
```

### Custom Simulation
```python
# Create your own simulation script
from agent_simulator import AgentSimulator

# Define your configuration
agent_config = {...}
network_config = {...}
simulation_config = {...}

# Run simulation
simulator = AgentSimulator(agent_config, network_config, simulation_config)
simulator.run_simulation()
```

## Research Applications

This codebase is designed for research in:
- Multi-agent reinforcement learning
- Cooperation and defection strategies
- Network effects on agent behavior
- Online learning and adaptation
- Social dilemma games
- Emergent cooperation

## Notes

- Online learning agents require pretrained models to initialize from
- The system supports universal model loading for different agent counts
- All agents use the same observation format for fair comparison
- Noise implementation helps model real-world uncertainty
- System optimization can lead to different emergent behaviors than private optimization
