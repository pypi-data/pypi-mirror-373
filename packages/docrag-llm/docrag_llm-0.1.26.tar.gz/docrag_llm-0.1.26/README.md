# ğŸ“„ DocRAG LLM  
**Docling â†’ Chroma â†’ Ollama: Simple Local RAG Pipeline**  

[![PyPI](https://img.shields.io/pypi/v/docrag-llm)](https://pypi.org/project/docrag-llm/)  
[![Python](https://img.shields.io/pypi/pyversions/docrag-llm)](https://pypi.org/project/docrag-llm/)  
[![License](https://img.shields.io/pypi/l/docrag-llm)](https://github.com/one1cat/docrag-llm/blob/main/LICENSE)  


---

### ğŸ” What is DocRAG LLM?  
**DocRAG LLM** is a local-first **Retrieval-Augmented Generation (RAG)** pipeline.  
It connects **[Docling](https://github.com/DS4SD/docling)** for parsing â†’ **[ChromaDB](https://www.trychroma.com/)** for vector storage â†’ **[Ollama](https://ollama.com/)** for local LLM inference.  

No cloud lock-in. No API costs. Just **local docs â†’ local vectors â†’ local LLMs**.  

---

## âœ¨ Features
- ğŸ” Parse documents with **Docling** (`PDF`, `DOCX`, `PPTX`, `HTML`, etc.)  
- ğŸ“‘ **Intelligent chunking** for retrieval  
- ğŸ§  Store embeddings in **ChromaDB**  
- ğŸ¤– Answer questions using **Ollama** (default: `llama3.2:1b`)  
- ğŸ›¡ï¸ **Privacy-first** â†’ all local execution  
- ğŸ–¥ï¸ Use as a **CLI tool** or **Python library**  

---

## ğŸ“¦ Installation
```bash
pip install docrag-llm
```

Requirements:
- Python 3.10+  
- [Ollama](https://ollama.com/) installed & running  
- Local models:  
  ```bash
  ollama pull llama3.2:1b
  ollama pull nomic-embed-text
  ```

---

## ğŸš€ Quickstart

### CLI â€“ Ingest and Ask
```bash
# Ingest a document (default collection: demo)
python -m docrag.cli ingest https://arxiv.org/pdf/2508.20755

# Ask a question (default LLM: llama3.2:1b)
python -m docrag.cli ask "Summarize in 1 paragraph with 5 bullet points"
```

### Python API
```python
from docrag import DocragSettings, RAGPipeline

cfg = DocragSettings(
    persist_path="./.chroma",
    collection="demo",
    embed_model="nomic-embed-text",
    llm_model="llama3.2:1b",
)

pipeline = RAGPipeline(cfg)

# Ingest
n_chunks = pipeline.ingest("https://arxiv.org/pdf/2508.20755")
print(f"Ingested {n_chunks} chunks")

# Ask
answer = pipeline.ask("Give a concise bullet summary of the paper's contributions.")
print(answer)
```

---

## âš™ï¸ Configuration
Both CLI & Python API let you customize:
- `persist_path` â†’ where ChromaDB stores vectors  
- `collection` â†’ logical collection name  
- `embed_model` â†’ embedding model (Ollama tag)  
- `llm_model` â†’ LLM model (default: `llama3.2:1b`)  
- `chunk_chars` / `chunk_overlap` â†’ chunking granularity  

---

## ğŸ“Š Roadmap
- [ ] `model-check` CLI â†’ list installed Ollama models  
- [ ] Support multiple backends (Weaviate, Milvus)  
- [ ] Streaming output for long answers  
- [ ] Expanded test suite (large document regression cases)  
- [ ] Example notebooks & Hugging Face demo  

---

## ğŸ¤ Contributing
PRs and issues welcome!  

```bash
pip install "docrag-llm[dev]"
ruff check .
pytest
```

---

## ğŸ“œ License
MIT License Â© 2025 [Armando Medina](https://github.com/one1cat)

---
