#!/usr/bin/python
# -*- coding: UTF-8 -*-
"""
# @Time    : 2025-08-31 22:35
# @Author  : crawl-coder
# @Desc    : å‘½ä»¤è¡Œå…¥å£ï¼šcrawlo checkï¼Œæ£€æŸ¥æ‰€æœ‰çˆ¬è™«å®šä¹‰æ˜¯å¦åˆè§„ã€‚
"""

import sys
import configparser
from pathlib import Path
from importlib import import_module

from crawlo.crawler import CrawlerProcess
from crawlo.utils.log import get_logger


logger = get_logger(__name__)


def get_project_root():
    """
    ä»å½“å‰ç›®å½•å‘ä¸ŠæŸ¥æ‰¾ crawlo.cfgï¼Œç¡®å®šé¡¹ç›®æ ¹ç›®å½•
    """
    current = Path.cwd()

    for _ in range(10):
        cfg = current / "crawlo.cfg"
        if cfg.exists():
            return current

        if current == current.parent:
            break
        current = current.parent

    return None


def main(args):
    """
    ä¸»å‡½æ•°ï¼šæ£€æŸ¥æ‰€æœ‰çˆ¬è™«å®šä¹‰çš„åˆè§„æ€§
    ç”¨æ³•: crawlo check
    """
    if args:
        print("âŒ Usage: crawlo check")
        return 1

    try:
        # 1. æŸ¥æ‰¾é¡¹ç›®æ ¹ç›®å½•
        project_root = get_project_root()
        if not project_root:
            print("âŒ Error: Cannot find 'crawlo.cfg'. Are you in a crawlo project?")
            print("ğŸ’¡ Tip: Run this command inside your project directory.")
            return 1

        project_root_str = str(project_root)
        if project_root_str not in sys.path:
            sys.path.insert(0, project_root_str)

        # 2. è¯»å– crawlo.cfg
        cfg_file = project_root / "crawlo.cfg"
        if not cfg_file.exists():
            print(f"âŒ Error: Expected config file not found: {cfg_file}")
            return 1

        config = configparser.ConfigParser()
        config.read(cfg_file, encoding="utf-8")

        if not config.has_section("settings") or not config.has_option("settings", "default"):
            print("âŒ Error: Missing [settings] section or 'default' option in crawlo.cfg")
            return 1

        settings_module = config.get("settings", "default")
        project_package = settings_module.split(".")[0]

        # 3. ç¡®ä¿é¡¹ç›®åŒ…å¯å¯¼å…¥
        try:
            import_module(project_package)
        except ImportError as e:
            print(f"âŒ Failed to import project package '{project_package}': {e}")
            return 1

        # 4. åŠ è½½çˆ¬è™«
        spider_modules = [f"{project_package}.spiders"]
        process = CrawlerProcess(spider_modules=spider_modules)
        spider_names = process.get_spider_names()

        if not spider_names:
            print("ğŸ“­ No spiders found.")
            print("ğŸ’¡ Make sure:")
            print("   â€¢ Spiders are defined in the 'spiders' module")
            print("   â€¢ They have a `name` attribute")
            print("   â€¢ Modules are properly imported")
            return 1

        print(f"ğŸ” Checking {len(spider_names)} spider(s)...")
        print("-" * 60)

        issues_found = False

        for name in sorted(spider_names):
            cls = process.get_spider_class(name)
            issues = []

            # æ£€æŸ¥ name å±æ€§
            if not getattr(cls, "name", None):
                issues.append("missing or empty 'name' attribute")
            elif not isinstance(cls.name, str):
                issues.append("'name' is not a string")

            # æ£€æŸ¥ start_requests æ˜¯å¦å¯è°ƒç”¨
            if not callable(getattr(cls, "start_requests", None)):
                issues.append("missing or non-callable 'start_requests' method")

            # æ£€æŸ¥ start_urls ç±»å‹ï¼ˆä¸åº”æ˜¯å­—ç¬¦ä¸²ï¼‰
            if hasattr(cls, "start_urls") and isinstance(cls.start_urls, str):
                issues.append("'start_urls' is a string; should be list or tuple")

            # å®ä¾‹åŒ–å¹¶æ£€æŸ¥ parse æ–¹æ³•ï¼ˆéå¼ºåˆ¶ä½†æ¨èï¼‰
            try:
                spider = cls.create_instance(None)
                if not callable(getattr(spider, "parse", None)):
                    issues.append("no 'parse' method defined (recommended)")
            except Exception as e:
                issues.append(f"failed to instantiate spider: {e}")

            # è¾“å‡ºç»“æœ
            if issues:
                print(f"âŒ {name:<20} {cls.__name__}")
                for issue in issues:
                    print(f"     â€¢ {issue}")
                issues_found = True
            else:
                print(f"âœ… {name:<20} {cls.__name__} (OK)")

        print("-" * 60)

        if issues_found:
            print("âš ï¸  Some spiders have issues. Please fix them.")
            return 1
        else:
            print("ğŸ‰ All spiders are compliant and well-defined!")
            return 0

    except Exception as e:
        print(f"âŒ Unexpected error during check: {e}")
        logger.exception("Exception in 'crawlo check'")
        return 1


if __name__ == "__main__":
    """
    æ”¯æŒç›´æ¥è¿è¡Œï¼š
        python -m crawlo.commands.check
    """
    sys.exit(main(sys.argv[1:]))