#!/usr/bin/python
# -*- coding: UTF-8 -*-
"""
# @Time    : 2025-08-31 22:36
# @Author  : crawl-coder
# @Desc    : å‘½ä»¤è¡Œå…¥å£ï¼šcrawlo statsï¼ŒæŸ¥çœ‹æœ€è¿‘è¿è¡Œçš„çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯ã€‚
"""

import sys
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Any

from crawlo.utils.log import get_logger


logger = get_logger(__name__)

# é»˜è®¤å­˜å‚¨ç›®å½•ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰
STATS_DIR = "logs/stats"


def get_stats_dir() -> Path:
    """
    è·å–ç»Ÿè®¡æ–‡ä»¶å­˜å‚¨ç›®å½•ï¼Œä¼˜å…ˆä½¿ç”¨é¡¹ç›®æ ¹ä¸‹çš„ logs/stats/
    å¦‚æœä¸åœ¨é¡¹ç›®ä¸­ï¼Œå›é€€åˆ°å½“å‰ç›®å½•
    """
    # å°è¯•æŸ¥æ‰¾é¡¹ç›®æ ¹ç›®å½•ï¼ˆé€šè¿‡ crawlo.cfgï¼‰
    current = Path.cwd()
    for _ in range(10):
        if (current / "crawlo.cfg").exists():
            return current / STATS_DIR
        if current == current.parent:
            break
        current = current.parent

    # å›é€€ï¼šä½¿ç”¨å½“å‰ç›®å½•ä¸‹çš„ logs/stats
    return Path.cwd() / STATS_DIR


def record_stats(crawler):
    """
    ã€ä¾›çˆ¬è™«è¿è¡Œæ—¶è°ƒç”¨ã€‘è®°å½•çˆ¬è™«ç»“æŸåçš„ç»Ÿè®¡ä¿¡æ¯åˆ° JSON æ–‡ä»¶
    éœ€åœ¨ Crawler çš„ closed å›è°ƒä¸­è°ƒç”¨
    """
    spider_name = getattr(crawler.spider, "name", "unknown")
    stats = crawler.stats.get_stats() if crawler.stats else {}

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    stats_dir = Path(get_stats_dir())
    stats_dir.mkdir(parents=True, exist_ok=True)

    filename = stats_dir / f"{spider_name}_{timestamp}.json"
    try:
        with open(filename, "w", encoding="utf-8") as f:
            json.dump({
                "spider": spider_name,
                "timestamp": datetime.now().isoformat(),
                "stats": stats
            }, f, ensure_ascii=False, indent=2, default=str)
        logger.info(f"ğŸ“Š Stats saved for spider '{spider_name}' â†’ {filename}")
    except Exception as e:
        logger.error(f"Failed to save stats for '{spider_name}': {e}")


def load_all_stats() -> Dict[str, list]:
    """
    åŠ è½½æ‰€æœ‰å·²ä¿å­˜çš„ç»Ÿè®¡æ–‡ä»¶ï¼ŒæŒ‰ spider name åˆ†ç»„
    è¿”å›: {spider_name: [stats_record, ...]}
    """
    stats_dir = get_stats_dir()
    if not stats_dir.exists():
        return {}

    result = {}
    json_files = sorted(stats_dir.glob("*.json"), key=lambda x: x.stat().st_mtime, reverse=True)

    for file in json_files:
        try:
            with open(file, "r", encoding="utf-8") as f:
                data = json.load(f)
            spider_name = data.get("spider", "unknown")
            result.setdefault(spider_name, []).append(data)
        except Exception as e:
            logger.warning(f"Failed to load stats file {file}: {e}")
    return result


def format_value(v: Any) -> str:
    """æ ¼å¼åŒ–å€¼ï¼Œé˜²æ­¢å¤ªé•¿æˆ–ä¸å¯æ‰“å°"""
    if isinstance(v, float):
        return f"{v:.4f}"
    return str(v)


def main(args):
    """
    ä¸»å‡½æ•°ï¼šæŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
    ç”¨æ³•:
        crawlo stats                 â†’ æ˜¾ç¤ºæ‰€æœ‰çˆ¬è™«æœ€è¿‘ä¸€æ¬¡è¿è¡Œ
        crawlo stats myspider        â†’ æ˜¾ç¤ºæŒ‡å®šçˆ¬è™«æ‰€æœ‰å†å²è®°å½•
        crawlo stats myspider --all  â†’ æ˜¾ç¤ºæ‰€æœ‰å†å²ï¼ˆåŒä¸Šï¼‰
    """
    if len(args) > 2:
        print("Usage: crawlo stats [spider_name] [--all]")
        return 1

    spider_name = None
    show_all = False

    if args:
        spider_name = args[0]
        show_all = "--all" in args or "-a" in args

    # åŠ è½½æ‰€æœ‰ stats
    all_stats = load_all_stats()
    if not all_stats:
        print("ğŸ“Š No stats found. Run a spider first.")
        print(f"ğŸ’¡ Stats are saved in: {get_stats_dir()}")
        return 0

    if not spider_name:
        # æ˜¾ç¤ºæ¯ä¸ªçˆ¬è™«æœ€è¿‘ä¸€æ¬¡è¿è¡Œ
        print("ğŸ“Š Recent Spider Statistics (last run):")
        print("-" * 60)
        for name, runs in all_stats.items():
            latest = runs[0]
            print(f"ğŸ•·ï¸  {name} ({latest['timestamp'][:19]})")
            stats = latest["stats"]
            for k in sorted(stats.keys()):
                print(f"    {k:<30} {format_value(stats[k])}")
            print()
        return 0

    else:
        # æŸ¥çœ‹æŒ‡å®šçˆ¬è™«
        if spider_name not in all_stats:
            print(f"ğŸ“Š No stats found for spider '{spider_name}'")
            available = ', '.join(all_stats.keys())
            if available:
                print(f"ğŸ’¡ Available spiders: {available}")
            return 1

        runs = all_stats[spider_name]
        if show_all:
            print(f"ğŸ“Š All runs for '{spider_name}' ({len(runs)} runs):")
        else:
            runs = runs[:1]
            print(f"ğŸ“Š Last run for '{spider_name}':")

        print("-" * 60)
        for run in runs:
            print(f"â±ï¸  Timestamp: {run['timestamp']}")
            stats = run["stats"]
            for k in sorted(stats.keys()):
                print(f"    {k:<30} {format_value(stats[k])}")
            print("â”€" * 60)
        return 0


if __name__ == "__main__":
    """
    æ”¯æŒç›´æ¥è¿è¡Œï¼š
        python -m crawlo.commands.stats
    """
    sys.exit(main(sys.argv[1:]))