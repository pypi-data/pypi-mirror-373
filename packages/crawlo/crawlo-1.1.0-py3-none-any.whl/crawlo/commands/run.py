#!/usr/bin/python
# -*- coding: UTF-8 -*-
"""
# @Time    : 2025-08-31 22:36
# @Author  : crawl-coder
# @Desc    : å‘½ä»¤è¡Œå…¥å£ï¼šcrawlo run <spider_name>|allï¼Œç”¨äºè¿è¡ŒæŒ‡å®šçˆ¬è™«ã€‚
"""
import sys
import asyncio
import configparser
from pathlib import Path
from importlib import import_module

from crawlo.crawler import CrawlerProcess
from crawlo.utils.log import get_logger
from crawlo.utils.project import get_settings
from crawlo.commands.stats import record_stats  # è‡ªåŠ¨è®°å½• stats

logger = get_logger(__name__)


def get_project_root():
    """
    å‘ä¸ŠæŸ¥æ‰¾ crawlo.cfg æ¥ç¡®å®šé¡¹ç›®æ ¹ç›®å½•
    """
    current = Path.cwd()

    for _ in range(10):
        cfg = current / "crawlo.cfg"
        if cfg.exists():
            return current

        if current == current.parent:
            break
        current = current.parent

    return None


def main(args):
    """
    ä¸»å‡½æ•°ï¼šè¿è¡ŒæŒ‡å®šçˆ¬è™«
    ç”¨æ³•:
        crawlo run <spider_name>
        crawlo run all
    """
    if len(args) < 1:
        print("âŒ Usage: crawlo run <spider_name>|all")
        print("ğŸ’¡ Examples:")
        print("   crawlo run baidu")
        print("   crawlo run all")
        return 1

    spider_arg = args[0]

    try:
        # 1. æŸ¥æ‰¾é¡¹ç›®æ ¹ç›®å½•
        project_root = get_project_root()
        if not project_root:
            print("âŒ Error: Cannot find 'crawlo.cfg'. Are you in a crawlo project?")
            print("ğŸ’¡ Tip: Run this command inside your project directory.")
            return 1

        project_root_str = str(project_root)
        if project_root_str not in sys.path:
            sys.path.insert(0, project_root_str)

        # 2. è¯»å– crawlo.cfg è·å– settings æ¨¡å—
        cfg_file = project_root / "crawlo.cfg"
        if not cfg_file.exists():
            print(f"âŒ Error: crawlo.cfg not found in {project_root}")
            return 1

        config = configparser.ConfigParser()
        config.read(cfg_file, encoding="utf-8")

        if not config.has_section("settings") or not config.has_option("settings", "default"):
            print("âŒ Error: Missing [settings] section or 'default' option in crawlo.cfg")
            return 1

        settings_module = config.get("settings", "default")
        project_package = settings_module.split(".")[0]

        # 3. ç¡®ä¿é¡¹ç›®åŒ…å¯å¯¼å…¥
        try:
            import_module(project_package)
        except ImportError as e:
            print(f"âŒ Failed to import project package '{project_package}': {e}")
            return 1

        # 4. åŠ è½½ settings å’Œçˆ¬è™«æ¨¡å—
        settings = get_settings()  # æ­¤æ—¶å·²å®‰å…¨
        spider_modules = [f"{project_package}.spiders"]
        process = CrawlerProcess(settings=settings, spider_modules=spider_modules)

        # === æƒ…å†µ1ï¼šè¿è¡Œæ‰€æœ‰çˆ¬è™« ===
        if spider_arg.lower() == "all":
            spider_names = process.get_spider_names()
            if not spider_names:
                print("âŒ No spiders found.")
                print("ğŸ’¡ Make sure:")
                print("   â€¢ Spiders are defined in 'spiders/'")
                print("   â€¢ They have a `name` attribute")
                print("   â€¢ Modules are imported (e.g. via __init__.py)")
                return 1

            print(f"ğŸš€ Starting ALL {len(spider_names)} spider(s):")
            print("-" * 60)
            for name in sorted(spider_names):
                cls = process.get_spider_class(name)
                print(f"ğŸ•·ï¸  {name:<20} {cls.__name__}")
            print("-" * 60)

            # æ³¨å†Œ stats è®°å½•ï¼ˆæ¯ä¸ªçˆ¬è™«ç»“æŸæ—¶ä¿å­˜ï¼‰
            for crawler in process.crawlers:
                crawler.signals.connect(record_stats, signal="spider_closed")

            # å¹¶è¡Œè¿è¡Œæ‰€æœ‰çˆ¬è™«ï¼ˆå¯æ”¹ä¸ºä¸²è¡Œï¼šfor name in ... await process.crawl(name)ï¼‰
            asyncio.run(process.crawl(spider_names))
            print("âœ… All spiders completed.")
            return 0

        # === æƒ…å†µ2ï¼šè¿è¡Œå•ä¸ªçˆ¬è™« ===
        spider_name = spider_arg
        if not process.is_spider_registered(spider_name):
            print(f"âŒ Spider '{spider_name}' not found.")
            available = process.get_spider_names()
            if available:
                print("ğŸ’¡ Available spiders:")
                for name in sorted(available):
                    cls = process.get_spider_class(name)
                    print(f"   â€¢ {name} ({cls.__name__})")
            else:
                print("ğŸ’¡ No spiders found. Check your spiders module.")
            return 1

        spider_class = process.get_spider_class(spider_name)

        # æ‰“å°å¯åŠ¨ä¿¡æ¯
        print(f"ğŸš€ Starting spider: {spider_name}")
        print(f"ğŸ“¦ Project: {project_package}")
        print(f"CppClass: {spider_class.__name__}")
        print(f"ğŸ“„ Module: {spider_class.__module__}")
        print("-" * 50)

        # æ³¨å†Œ stats è®°å½•
        for crawler in process.crawlers:
            crawler.signals.connect(record_stats, signal="spider_closed")

        # è¿è¡Œçˆ¬è™«
        asyncio.run(process.crawl(spider_name))

        print("-" * 50)
        print("âœ… Spider completed successfully!")
        return 0

    except KeyboardInterrupt:
        print("\nâš ï¸  Spider interrupted by user.")
        return 1
    except Exception as e:
        print(f"âŒ Unexpected error: {e}")
        logger.exception("Exception during 'crawlo run'")
        return 1


if __name__ == "__main__":
    """
    æ”¯æŒç›´æ¥è¿è¡Œï¼š
        python -m crawlo.commands.run spider_name
    """
    sys.exit(main(sys.argv[1:]))