"""
å‘½ä»¤è¡Œå…¥å£ï¼šcrawlo run <spider_name>
ç”¨äºè¿è¡ŒæŒ‡å®šåç§°çš„çˆ¬è™«ã€‚
"""

import asyncio
from pathlib import Path
import configparser

from crawlo.crawler import CrawlerProcess
from crawlo.utils.project import get_settings
from crawlo.utils.log import get_logger

logger = get_logger(__name__)


def main(args):
    """
    è¿è¡ŒæŒ‡å®šçˆ¬è™«çš„ä¸»å‡½æ•°
    ç”¨æ³•:
        crawlo run <spider_name>
        crawlo run all
    """
    if len(args) < 1:
        print("Usage: crawlo run <spider_name>|all")
        print("Examples:")
        print("  crawlo run baidu")
        print("  crawlo run all")
        return 1

    spider_arg = args[0]

    try:
        # 1. è·å–é¡¹ç›®æ ¹ç›®å½•
        project_root = get_settings().get('PROJECT_ROOT')
        if not project_root:
            print("âŒ Error: Cannot determine project root.")
            return 1

        if str(project_root) not in sys.path:
            sys.path.insert(0, str(project_root))

        # 2. è¯»å– crawlo.cfg è·å–é¡¹ç›®åŒ…å
        cfg_file = project_root / 'crawlo.cfg'
        if not cfg_file.exists():
            print(f"âŒ Error: crawlo.cfg not found in {project_root}")
            return 1

        config = configparser.ConfigParser()
        config.read(cfg_file, encoding='utf-8')

        if not config.has_section('settings') or not config.has_option('settings', 'default'):
            print("âŒ Error: Missing [settings] section or 'default' option in crawlo.cfg")
            return 1

        settings_module = config.get('settings', 'default')
        project_package = settings_module.split('.')[0]

        # 3. åˆ›å»º CrawlerProcess å¹¶è‡ªåŠ¨å‘ç°çˆ¬è™«æ¨¡å—
        spider_modules = [f"{project_package}.spiders"]
        settings = get_settings()
        process = CrawlerProcess(settings=settings, spider_modules=spider_modules)

        # === æ–°å¢ï¼šæ”¯æŒ 'all' ===
        if spider_arg.lower() == "all":
            spider_names = process.get_spider_names()
            if not spider_names:
                print("âŒ No spiders found. Make sure spiders are defined and imported.")
                return 1

            print(f"ğŸš€ Starting ALL {len(spider_names)} spiders:")
            for name in sorted(spider_names):
                cls = process.get_spider_class(name)
                print(f"   ğŸ•·ï¸  {name} ({cls.__name__})")
            print("-" * 50)

            # å¯åŠ¨æ‰€æœ‰çˆ¬è™«
            asyncio.run(process.crawl(spider_names))
            return 0

        # === åŸæœ‰ï¼šå¯åŠ¨å•ä¸ªçˆ¬è™« ===
        spider_name = spider_arg
        if not process.is_spider_registered(spider_name):
            print(f"âŒ Error: Spider with name '{spider_name}' not found.")
            available_names = process.get_spider_names()
            if available_names:
                print("ğŸ’¡ Available spiders:")
                for name in sorted(available_names):
                    cls = process.get_spider_class(name)
                    print(f"   - {name} (class: {cls.__name__})")
            else:
                print("ğŸ’¡ No spiders found. Make sure your spider classes are defined and imported.")
            return 1

        spider_class = process.get_spider_class(spider_name)

        # æ‰“å°å¯åŠ¨ä¿¡æ¯
        print(f"ğŸš€ Starting spider: {spider_name}")
        print(f"ğŸ“ Project: {project_package}")
        print(f"ğŸ•·ï¸  Class: {spider_class.__name__}")
        print("-" * 50)

        # å¯åŠ¨çˆ¬è™«
        asyncio.run(process.crawl(spider_name))

        print("-" * 50)
        print("âœ… Spider completed successfully!")
        return 0

    except KeyboardInterrupt:
        print("\nâš ï¸  Spider interrupted by user.")
        return 1
    except Exception as e:
        print(f"âŒ Error running spider: {e}")
        import traceback
        traceback.print_exc()
        return 1


def list_available_spiders(project_package: str):
    """
    åˆ—å‡ºæŒ‡å®šé¡¹ç›®åŒ…ä¸­æ‰€æœ‰å¯ç”¨çš„çˆ¬è™«ï¼ˆç”¨äºè°ƒè¯•æˆ–å‘½ä»¤è¡Œæ‰©å±•ï¼‰
    """
    try:
        # ä¸´æ—¶åˆ›å»ºä¸€ä¸ª CrawlerProcess æ¥å‘ç°çˆ¬è™«
        process = CrawlerProcess(spider_modules=[f"{project_package}.spiders"])
        available_names = process.get_spider_names()

        if not available_names:
            print("   No spiders found. Make sure:")
            print("   - spiders/ ç›®å½•å­˜åœ¨")
            print("   - çˆ¬è™«ç±»ç»§æ‰¿ Spider ä¸”å®šä¹‰äº† name")
            print("   - æ¨¡å—è¢«å¯¼å…¥ï¼ˆå¯é€šè¿‡ __init__.py è§¦å‘ï¼‰")
            return

        print(f"Found {len(available_names)} spider(s):")
        for name in sorted(available_names):
            cls = process.get_spider_class(name)
            module = cls.__module__.replace(project_package + ".", "")
            print(f"   - {name} ({cls.__name__} @ {module})")
    except Exception as e:
        print(f"âŒ Failed to list spiders: {e}")
        import traceback
        traceback.print_exc()


def run_spider_by_name(spider_name: str, project_package: str = None):
    """
    åœ¨ä»£ç ä¸­ç›´æ¥è¿è¡ŒæŸä¸ªçˆ¬è™«ï¼ˆéœ€æä¾› project_packageï¼‰
    """
    if project_package is None:
        # å°è¯•ä»é…ç½®è¯»å–
        cfg_file = Path('crawlo.cfg')
        if cfg_file.exists():
            config = configparser.ConfigParser()
            config.read(cfg_file, encoding='utf-8')
            if config.has_option('settings', 'default'):
                project_package = config.get('settings', 'default').split('.')[0]

    if not project_package:
        print("âŒ Error: project_package is required.")
        return 1

    # æ·»åŠ é¡¹ç›®è·¯å¾„
    project_root = get_settings().get('PROJECT_ROOT')
    if project_root and str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))

    # å¤ç”¨ main å‡½æ•°é€»è¾‘
    args = [spider_name]
    return main(args)


if __name__ == '__main__':
    """
    å…è®¸ç›´æ¥è¿è¡Œï¼š
        python -m crawlo.commands.run <spider_name>
    """
    import sys

    sys.exit(main(sys.argv[1:]))
