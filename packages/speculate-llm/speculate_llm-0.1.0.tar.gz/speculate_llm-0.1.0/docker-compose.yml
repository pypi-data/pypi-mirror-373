version: "3.9"

services:
  llm_tests:
    build: .
    image: llm-tests:latest
    container_name: llm_tests
    working_dir: /app

    # Live-mount your repo so you can edit tests/code without rebuilding
    volumes:
      - ./:/app

    environment:
      # Point to your hostâ€™s Ollama
      # On Mac/Windows this works out of the box.
      # On Linux, see extra_hosts or switch to network_mode: host
      OLLAMA_BASE_URL: "http://host.docker.internal:11434"
      OLLAMA_API_STYLE: "generate"
      LLM_DEFAULT_SEED: "1234"           # provider default seed (unless overridden)
      LLM_RUNS_PER_TEST: "3"             # suite-wide runs-per-test
      LLM_ACCURACY_THRESHOLD: "0.9"      # suite-wide required accuracy

    # Reliable invocation (avoids the __main__ confusion)
    entrypoint: ["python3", "-m", "speculate.cli", "./scenarios"]

    # For Linux, ensure the container can resolve host.docker.internal
    extra_hosts:
      - "host.docker.internal:host-gateway"

    # Alternatively on Linux you can use:
    # network_mode: "host"
