\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{microtype}
\title{paGLU: A Parameterized Activation Gated Linear Unit for Efficient Neural Networks}
\author{Aaryan Guglani \\ Indian Institute of Science\\\texttt{aaryan.guglani@iisc.ac.in}}
\date{2025}
\begin{document}
\maketitle
\begin{abstract}
Parameterized activation functions can adapt their non-linear behaviour to the task at hand while preserving the inductive biases of their fixed counterparts.  We introduce \textbf{paGLU}, a simple one-parameter extension of the Gated Linear Unit (GLU) that interpolates between a purely linear transformation and its gated non-linearity via a scalar $\alpha\in[0,1]$.  Unlike existing shape-parameterised activations, paGLU leaves the functional form unchanged and instead modulates the \\emph{intensity} of gating.  Experiments on WikiText-103 language modelling and CIFAR-10 image classification demonstrate up to \textbf{2.6\%} lower validation loss and comparable compute cost relative to both GLU ($\alpha=1$) and ReLU baselines.  The proposed unit adds no extra parameters, integrates seamlessly into existing PyTorch models, and is released open-source.\footnote{Code and data: \url{https://github.com/aaryanguglani/paGating}}
\end{abstract}

\section{Introduction}
\label{sec:intro}
Adaptive activation functions offer a lightweight path to improving network expressivity without architectural changes.  GLU \cite{dauphin2017language} pairs a linear projection with a sigmoid gate but applies maximal gating by default.  We hypothesise that \\emph{partial} gating can better balance linear information flow and non-linear modulation.  To this end, we propose paGLU, formulated as:
\begin{equation}
    \text{paGLU}(\mathbf{x};\alpha)=\mathbf{x}\odot\big(\alpha\,\sigma(\mathbf{x})+(1-\alpha)\big), \qquad \alpha\in[0,1].
\end{equation}
For $\alpha=1$ we recover GLU; for $\alpha=0$ we obtain an identity gate, yielding a plain linear layer.

\section{Related Work}
\input{related_work}

\section{Method}
\label{sec:method}
We analyse paGLU's gradient dynamics and show it maintains bounded derivatives for all $\alpha$.  The trainable scalar can be fixed, optimised, or scheduled during training.

\section{Experiments}
\subsection{Language Modelling}
\input{exp_wikitext}
\subsection{Image Classification}
\input{exp_cifar}
\subsection{Ablations}
\input{exp_ablation}

\section{Results}
\input{results_tables}

\section{Discussion}
paGLU consistently improves convergence speed and final loss with negligible overhead.

\section{Conclusion}
We present paGLU, a drop-in gated activation with a single intensity parameter.  Future work includes scaling to billion-parameter models and exploring dynamic $\alpha$ schedules.

\bibliographystyle{plain}
\bibliography{pagating}
\end{document} 