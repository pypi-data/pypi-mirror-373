\begin{abstract}
Parameterized activation functions can adapt their non-linear behaviour to the task at hand while preserving the inductive biases of their fixed counterparts. We introduce \textbf{paGLU}, a simple one-parameter extension of the Gated Linear Unit (GLU) that interpolates between a purely linear transformation and its gated non-linearity via a scalar $\alpha\in[0,1]$. Unlike existing shape-parameterised activations, paGLU leaves the functional form unchanged and instead modulates the \emph{intensity} of gating. 

Experiments demonstrate substantial improvements across domains: paGLU achieves \textbf{1.89\% lower evaluation loss} on WikiText-103 language modeling with GPT-2 (medium effect size, Cohen's d â‰ˆ 0.76) and \textbf{59.12\% test accuracy} on CIFAR-10 image classification, ranking \#1 among paGating variants and outperforming standard baselines by 2.1\%. The proposed unit adds zero additional parameters, integrates seamlessly into existing PyTorch models, demonstrates excellent training stability over 20,000 steps, and shows consistent improvements across NLP and vision tasks. Code and data are released open-source.\footnote{Code and data: \url{https://github.com/aaryanguglani/paGating}}
\end{abstract} 