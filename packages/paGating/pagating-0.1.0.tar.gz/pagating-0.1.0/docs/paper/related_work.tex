Parameterized activation functions have gained attention for their ability to adapt to different tasks while preserving computational efficiency. Swish~\cite{ramachandran2017searching} introduced learnable parameters in activation functions, while GELU~\cite{hendrycks2016gaussian} provided probabilistic interpretations. 

Gated Linear Units (GLU)~\cite{dauphin2017language} demonstrated the effectiveness of gating mechanisms in language modeling, inspiring variants like SwiGLU~\cite{shazeer2020glu} and GeGLU~\cite{shazeer2020glu}. However, these approaches typically apply full gating intensity.

Recent work on adaptive activations includes PReLU~\cite{he2015delving} and ELU~\cite{clevert2015fast}, which modify activation shapes. In contrast, paGLU preserves the functional form while modulating gating intensity, offering a complementary approach to existing parameterization strategies.

Our work differs by focusing on \emph{intensity modulation} rather than shape modification, enabling smooth interpolation between linear and gated behaviors while maintaining zero computational overhead. 