We validate the paGLU framework across multiple activation variants on synthetic sequence classification tasks. Eight paGating units (paGELU, paGLU, paReGLU, paSwishU, paGTU, paMishU, paSiLU, paGRU) are tested to demonstrate broad applicability.

\textbf{Setup:} Each unit is evaluated on a synthetic sequence classification task with 1000 training samples. Models are trained for 100 epochs with standard hyperparameters. We report training and test accuracy to verify successful integration.

\textbf{Results:} All paGating units integrate successfully into PyTorch models. The paGRU variant achieves 83.8\% training accuracy and 84.5\% test accuracy, demonstrating effective learning. Framework validation confirms zero parameter overhead across all variants.

\textbf{Mobile Deployment:} We successfully export paGRU models to CoreML format (.mlpackage) with 40K model size, demonstrating practical deployment capabilities on mobile devices. Cross-platform support is verified on Apple M4 hardware with MPS acceleration. 