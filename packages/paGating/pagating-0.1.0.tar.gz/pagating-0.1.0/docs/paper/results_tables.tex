\begin{table}[ht]
\centering
\caption{Language modeling results on WikiText-103 with GPT-2 Small. paGLU achieves baseline equivalence with zero overhead.}
\label{tab:verified_nlp_results}
\begin{tabular}{lcccc}
\toprule
Configuration & $\alpha$ & Steps & Train Loss & Eval Loss \\
\midrule
Baseline GPT-2 & 0.0 & 16,000 & 1.625 & 1.781 \\
paGLU $\alpha$=0.0 & 0.0 & 20,000 & 1.627 & 1.776 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
\centering
\caption{Framework validation results demonstrating successful integration and deployment capabilities.}
\label{tab:framework_validation}
\begin{tabular}{lcc}
\toprule
Component & Result & Status \\
\midrule
Baseline Equivalence & $\alpha$=0.0 matches standard implementation & Verified \\
Parameter Overhead & 0 additional parameters & Verified \\
FLOP Overhead & 0\% computational overhead & Verified \\
Transformer Integration & All paGating units successful & Verified \\
Mobile Deployment & CoreML export (40K model) & Verified \\
Cross-Platform Support & Apple M4 MPS acceleration & Verified \\
\bottomrule
\end{tabular}
\end{table} 